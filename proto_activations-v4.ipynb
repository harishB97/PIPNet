{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/154-PruningNaiveHPIPNet_cnext13_CUB-18-imgnet-224_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/158-PruningNaiveHPIPNetExpWeightPruning_cnext13_CUB-18-imgnet-224_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/159-PruningNaiveHPIPNetMaskL1=1.0_cnext13_CUB-18-imgnet-224_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/160-PruningNaiveHPIPNetMaskL1=0.5_cnext13_CUB-18-imgnet-224_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/161-PruningNaiveHPIPNetMaskL1=0.5MaskTrainExtra=15eps_cnext13_CUB-18-imgnet-224_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"/projects/ml4science/harishbabu/projects/PIPNet/162-PruningNaiveHPIPNetMaskL1=0.5MaskTrainExtra=15epsEps=60_cnext13_CUB-18-imgnet-224_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/163-PruningNaiveHPIPNetMaskL1=0.5MaskTrainExtra=15epsEps=60TanhDesc=0.2MinCont=0.1_cnext13_CUB-18-imgnet-224_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"/projects/ml4science/harishbabu/projects/PIPNet/164-PruningNaiveHPIPNetMaskL1=0.5MaskTrainExtra=15epsEps=60TanhDesc=0.05MinCont=0.1_cnext13_CUB-18-imgnet-224_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/167-PruningNaiveHPIPNetMaskL1=0.5MaskTrainExtra=15epsEps=60TanhDesc=0.05MinCont=0.1_cnext26_CUB-18-imgnet-224_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "run_path = \"/home/harishbabu/projects/PIPNet_wandb/runs/178-PruningNaiveHPIPNetMaskL1=0.5MaskTrainExtra=15epsEps=60TanhDesc=0.05MinCont=0.1_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/202-PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=60Cl=2.0TanhDesc=0.05MinCont=0.1_cnext13_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pc\"\n",
    "\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/207-PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=60Cl=2.0TanhDesc=0.05MinCont=0.1_cnext26_BUT-51-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pc\"\n",
    "\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/208-PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=60Cl=2.0TanhDesc=0.05MinCont=0.1_cnext26_FISH-38-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pc\"\n",
    "\n",
    "# run_path = \"runs/225-BUT30_nprotos=10pc-cnext26_PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=15epsEps=60Cl=2.0TanhDesc=0.05MinCont=0.1_BUT-30-imgnet-224_WeightedCE_with-equalize-aug_img=224\"\n",
    "\n",
    "try:\n",
    "    sys.path.remove('/home/harishbabu/projects/PIPNet')\n",
    "except:\n",
    "    pass\n",
    "sys.path.insert(0, os.path.join(run_path, 'source_clone'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/harishbabu/projects/PIPNet_wandb/runs/178-PruningNaiveHPIPNetMaskL1=0.5MaskTrainExtra=15epsEps=60TanhDesc=0.05MinCont=0.1_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=20\n"
     ]
    }
   ],
   "source": [
    "print(run_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import sys, os\n",
    "import random\n",
    "import numpy as np\n",
    "from shutil import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import shutil\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision.datasets.folder import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "# from skimage.filters import threshold_local, gaussian\n",
    "import ntpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/harishbabu/projects/PIPNet_wandb/runs/178-PruningNaiveHPIPNetMaskL1=0.5MaskTrainExtra=15epsEps=60TanhDesc=0.05MinCont=0.1_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=20/source_clone\n"
     ]
    }
   ],
   "source": [
    "print(sys.path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/harishbabu/projects/PIPNet_wandb/runs/178-PruningNaiveHPIPNetMaskL1=0.5MaskTrainExtra=15epsEps=60TanhDesc=0.05MinCont=0.1_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=20/source_clone/util/node.py\n"
     ]
    }
   ],
   "source": [
    "# import pipnet.pipnet\n",
    "# from pipnet.pipnet import PIPNet, get_network\n",
    "# # from pipnet import pipnet\n",
    "# print(pipnet.__file__)\n",
    "from util import node\n",
    "print(node.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heatmaps showing where a prototype is found will not be generated because OpenCV is not installed.\n"
     ]
    }
   ],
   "source": [
    "from pipnet.pipnet import PIPNet, get_network\n",
    "from util.log import Log\n",
    "from util.args import get_args, save_args, get_optimizer_nn\n",
    "from util.data import get_dataloaders\n",
    "from util.func import init_weights_xavier\n",
    "from pipnet.train import train_pipnet, test_pipnet\n",
    "# from pipnet.test import eval_pipnet, get_thresholds, eval_ood\n",
    "from util.eval_cub_csv import eval_prototypes_cub_parts_csv, get_topk_cub, get_proto_patches_cub\n",
    "from util.vis_pipnet import visualize, visualize_topk\n",
    "from util.visualize_prediction import vis_pred, vis_pred_experiments\n",
    "from util.node import Node\n",
    "from util.phylo_utils import construct_phylo_tree, construct_discretized_phylo_tree\n",
    "from util.func import get_patch_size\n",
    "from util.data import ModifiedLabelLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "# def get_heatmap(latent_activation, input_image):\n",
    "#     image_a = latent_activation.cpu().numpy()\n",
    "#     image_a = (image_a - image_a.min()) / (image_a.max() - image_a.min())\n",
    "\n",
    "#     input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "#     image_b = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "#     reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((input_image.shape[-1], input_image.shape[-1])))\n",
    "#     normalized_heatmap = (reshaped_image_a - np.min(reshaped_image_a)) / (np.max(reshaped_image_a) - np.min(reshaped_image_a))\n",
    "    \n",
    "#     heatmap_colormap = plt.get_cmap('jet')\n",
    "#     heatmap_colored = heatmap_colormap(normalized_heatmap)\n",
    "    \n",
    "#     heatmap_colored_uint8 = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "#     image_a_heatmap_pillow = Image.fromarray(heatmap_colored_uint8)\n",
    "#     image_b_pillow = Image.fromarray((image_b * 255).astype('uint8'))\n",
    "    \n",
    "#     result_image = Image.blend(image_b_pillow, image_a_heatmap_pillow, alpha=0.3)\n",
    "    \n",
    "#     return np.array(result_image)\n",
    "\n",
    "def get_heatmap(latent_activation, input_image, constant_color_scale=False):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    image_a = (image_a - image_a.min()) / (image_a.max() - image_a.min())\n",
    "\n",
    "    # input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "    image_b = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((input_image.shape[-1], input_image.shape[-1])))\n",
    "    normalized_heatmap = (reshaped_image_a - np.min(reshaped_image_a)) / (np.max(reshaped_image_a) - np.min(reshaped_image_a))\n",
    "\n",
    "    if constant_color_scale:\n",
    "        normalized_heatmap = np.concatenate((normalized_heatmap, np.zeros((normalized_heatmap.shape[1], 1)), np.ones((normalized_heatmap.shape[1], 1))), axis=1)\n",
    "    \n",
    "    heatmap_colormap = plt.get_cmap('jet')\n",
    "    heatmap_colored = heatmap_colormap(normalized_heatmap)\n",
    "\n",
    "    if constant_color_scale:\n",
    "        heatmap_colored = heatmap_colored[:, :-2]\n",
    "    \n",
    "    heatmap_colored_uint8 = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_a_heatmap_pillow = Image.fromarray(heatmap_colored_uint8)\n",
    "    image_b_pillow = Image.fromarray((image_b * 255).astype('uint8'))\n",
    "    \n",
    "    result_image = Image.blend(image_b_pillow, image_a_heatmap_pillow, alpha=0.3)\n",
    "    \n",
    "    return np.array(result_image)\n",
    "\n",
    "\n",
    "def get_heatmap_uninterpolated(latent_activation, input_image):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    image_a = (image_a - image_a.min()) / (image_a.max() - image_a.min())\n",
    "\n",
    "    input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "    image_b = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((input_image.shape[-1], input_image.shape[-1]), \\\n",
    "                                                                                          resample=Image.NEAREST ))\n",
    "    normalized_heatmap = (reshaped_image_a - np.min(reshaped_image_a)) / (np.max(reshaped_image_a) - np.min(reshaped_image_a))\n",
    "    \n",
    "    heatmap_colormap = plt.get_cmap('jet')\n",
    "    heatmap_colored = heatmap_colormap(normalized_heatmap)\n",
    "    \n",
    "    heatmap_colored_uint8 = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_a_heatmap_pillow = Image.fromarray(heatmap_colored_uint8)\n",
    "    image_b_pillow = Image.fromarray((image_b * 255).astype('uint8'))\n",
    "    \n",
    "    result_image = Image.blend(image_b_pillow, image_a_heatmap_pillow, alpha=0.3)\n",
    "    \n",
    "    return np.array(result_image)\n",
    "\n",
    "def get_bb_gaussian_threshold(latent_activation, sigma=1.0, percentile=97, extend_h=0, extend_w=0):\n",
    "    # latent_activation -> []\n",
    "    upscaled_similarity = get_upscaled_activation_uninterpolated(latent_activation, \\\n",
    "                                                                 image_size=(args.image_size, args.image_size))\n",
    "    upscaled_similarity = minmaxscale(upscaled_similarity)\n",
    "    upscaled_similarity = gaussian(upscaled_similarity, sigma=sigma)\n",
    "    upscaled_similarity = threshold_local(upscaled_similarity, block_size=15, method='mean')\n",
    "    h_min, h_max, w_min, w_max = find_top_percentile_bbox(upscaled_similarity ,percentile=97)\n",
    "    h_min = max(0, h_min-extend_h)\n",
    "    h_max = min(upscaled_similarity.shape[0], h_max+extend_h)\n",
    "    w_min = max(0, w_min-extend_w)\n",
    "    w_max = min(upscaled_similarity.shape[1], w_max+extend_w)\n",
    "    return h_min, h_max, w_min, w_max\n",
    "\n",
    "\n",
    "def minmaxscale(tensor):\n",
    "    return (tensor - tensor.min()) / (tensor.max() - tensor.min())\n",
    "\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def unshuffle_dataloader(dataloader):\n",
    "    if type(dataloader.dataset) == ImageFolder:\n",
    "        dataset = dataloader.dataset\n",
    "    else:\n",
    "        dataset = dataloader.dataset.dataset.dataset\n",
    "    new_dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=dataloader.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=dataloader.num_workers,\n",
    "        pin_memory=dataloader.pin_memory,\n",
    "        drop_last=dataloader.drop_last,\n",
    "        timeout=dataloader.timeout,\n",
    "        worker_init_fn=dataloader.worker_init_fn,\n",
    "        multiprocessing_context=dataloader.multiprocessing_context,\n",
    "        generator=dataloader.generator,\n",
    "        prefetch_factor=dataloader.prefetch_factor,\n",
    "        persistent_workers=dataloader.persistent_workers\n",
    "    )\n",
    "    return new_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- No discretization -------------------------\n"
     ]
    }
   ],
   "source": [
    "args_file = open(os.path.join(run_path, 'metadata', 'args.pickle'), 'rb')\n",
    "args = pickle.load(args_file)\n",
    "\n",
    "if args.phylo_config:\n",
    "    phylo_config = OmegaConf.load(args.phylo_config)\n",
    "\n",
    "if args.phylo_config:\n",
    "    # construct the phylo tree\n",
    "    if phylo_config.phyloDistances_string == 'None':\n",
    "        if '031' in run_path: # this run uses a different phylogeny file that had an extra root node which is a mistake\n",
    "            root = construct_phylo_tree('/home/harishbabu/data/phlyogenyCUB/18Species-with-extra-root-node/1_tree-consensus-Hacket-18Species-modified_cub-names_v1.phy')\n",
    "        else:\n",
    "            root = construct_phylo_tree(phylo_config.phylogeny_path)\n",
    "        print('-'*25 + ' No discretization ' + '-'*25)\n",
    "    else:\n",
    "        root = construct_discretized_phylo_tree(phylo_config.phylogeny_path, phylo_config.phyloDistances_string)\n",
    "        print('-'*25 + ' Discretized ' + '-'*25)\n",
    "else:\n",
    "    # construct the tree (original hierarchy as described in the paper)\n",
    "    root = Node(\"root\")\n",
    "    root.add_children(['animal','vehicle','everyday_object','weapon','scuba_diver'])\n",
    "    root.add_children_to('animal',['non_primate','primate'])\n",
    "    root.add_children_to('non_primate',['African_elephant','giant_panda','lion'])\n",
    "    root.add_children_to('primate',['capuchin','gibbon','orangutan'])\n",
    "    root.add_children_to('vehicle',['ambulance','pickup','sports_car'])\n",
    "    root.add_children_to('everyday_object',['laptop','sandal','wine_bottle'])\n",
    "    root.add_children_to('weapon',['assault_rifle','rifle'])\n",
    "    # flat root\n",
    "    # root.add_children(['scuba_diver','African_elephant','giant_panda','lion','capuchin','gibbon','orangutan','ambulance','pickup','sports_car','laptop','sandal','wine_bottle','assault_rifle','rifle'])\n",
    "root.assign_all_descendents()\n",
    "\n",
    "exp_no = int(os.path.basename(run_path)[:3])\n",
    "\n",
    "if exp_no < 77:\n",
    "    if ('num_protos_per_descendant' in args) and (args.num_protos_per_descendant > 0):\n",
    "        for node in root.nodes_with_children():\n",
    "            node.set_num_protos(args.num_protos_per_descendant)\n",
    "if exp_no == 77:\n",
    "    # update num of protos per node based on num_protos_per_descendant\n",
    "    if args.num_features == 0 and args.num_protos_per_descendant == 0:\n",
    "        raise Exception('Either of num_features or num_protos_per_descendant must be greater than zero')\n",
    "    for node in root.nodes_with_children():\n",
    "        node.set_num_protos(num_protos_per_descendant=args.num_protos_per_descendant,\\\n",
    "                                                            min_protos=args.num_features)\n",
    "else:\n",
    "    if ('num_protos_per_child' in args) and ('num_protos_per_descendant' in args):\n",
    "        if args.num_features == 0 and args.num_protos_per_descendant == 0 and args.num_protos_per_child == 0:\n",
    "            raise Exception('Either of num_features or num_protos_per_descendant or num_protos_per_child must be greater than zero')\n",
    "        for node in root.nodes_with_children():\n",
    "            node.set_num_protos(num_protos_per_descendant=args.num_protos_per_descendant,\\\n",
    "                                num_protos_per_child=args.num_protos_per_child,\\\n",
    "                                min_protos=args.num_features,\\\n",
    "                                split_protos=('protopool' in args) and (args.protopool == 'n'))\n",
    "    elif ('num_protos_per_descendant' in args):\n",
    "        # update num of protos per node based on num_protos_per_descendant\n",
    "        if args.num_features == 0 and args.num_protos_per_descendant == 0 and args.num_protos_per_child == 0:\n",
    "            raise Exception('Either of num_features or num_protos_per_descendant or num_protos_per_child must be greater than zero')\n",
    "        for node in root.nodes_with_children():\n",
    "            node.set_num_protos(num_protos_per_descendant=args.num_protos_per_descendant,\\\n",
    "                                min_protos=args.num_features,\\\n",
    "                                split_protos=('protopool' in args) and (args.protopool == 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/harishbabu/projects/PIPNet'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Illegal option --\n",
      "Usage: /usr/bin/which [-a] args\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "CUB-190-imgnet-224\n"
     ]
    }
   ],
   "source": [
    "args.batch_size = 1\n",
    "\n",
    "print(args.batch_size)\n",
    "print(args.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 0 samples from trainloader\n",
      "Dropping 0 samples from trainloader_normal\n",
      "Dropping 0 samples from trainloader_normal_augment\n",
      "Num classes (k) =  190 ['cub_001_Black_footed_Albatross', 'cub_002_Laysan_Albatross', 'cub_003_Sooty_Albatross', 'cub_004_Groove_billed_Ani', 'cub_005_Crested_Auklet'] etc.\n",
      "1 1\n",
      "Number of prototypes:  20\n",
      "----------Prototypes per descendant: 0----------\n",
      "Assigned 20 protos to node root\n",
      "Assigned 20 protos to node 129+024+067\n",
      "Assigned 20 protos to node 089+046\n",
      "Assigned 20 protos to node 129+065\n",
      "Assigned 20 protos to node 024+051\n",
      "Assigned 20 protos to node 067+070\n",
      "Assigned 20 protos to node 089+090\n",
      "Assigned 20 protos to node 046+087\n",
      "Assigned 20 protos to node 129+192\n",
      "Assigned 20 protos to node 065+006\n",
      "Assigned 20 protos to node 024+031\n",
      "Assigned 20 protos to node 051+052\n",
      "Assigned 20 protos to node 067+068\n",
      "Assigned 20 protos to node 129+043\n",
      "Assigned 20 protos to node 192+081\n",
      "Assigned 20 protos to node 065+144\n",
      "Assigned 20 protos to node 006+071\n",
      "Assigned 20 protos to node 024+086\n",
      "Assigned 20 protos to node 031+004\n",
      "Assigned 20 protos to node 051+053\n",
      "Assigned 20 protos to node 067+069\n",
      "Assigned 20 protos to node 129+018\n",
      "Assigned 20 protos to node 043+078\n",
      "Assigned 20 protos to node 192+036\n",
      "Assigned 20 protos to node 081+083\n",
      "Assigned 20 protos to node 065+084\n",
      "Assigned 20 protos to node 144+147\n",
      "Assigned 20 protos to node 006+058\n",
      "Assigned 20 protos to node 071+072\n",
      "Assigned 20 protos to node 024+001\n",
      "Assigned 20 protos to node 031+032\n",
      "Assigned 20 protos to node 051+050\n",
      "Assigned 20 protos to node 129+107\n",
      "Assigned 20 protos to node 043+042\n",
      "Assigned 20 protos to node 078+038\n",
      "Assigned 20 protos to node 192+191\n",
      "Assigned 20 protos to node 036+188\n",
      "Assigned 20 protos to node 081+082\n",
      "Assigned 20 protos to node 065+061\n",
      "Assigned 20 protos to node 084+063\n",
      "Assigned 20 protos to node 144+143\n",
      "Assigned 20 protos to node 006+008\n",
      "Assigned 20 protos to node 024+100\n",
      "Assigned 20 protos to node 001+045\n",
      "Assigned 20 protos to node 031+033\n",
      "Assigned 20 protos to node 129+136\n",
      "Assigned 20 protos to node 107+151\n",
      "Assigned 20 protos to node 043+040\n",
      "Assigned 20 protos to node 078+041\n",
      "Assigned 20 protos to node 192+187\n",
      "Assigned 20 protos to node 191+189\n",
      "Assigned 20 protos to node 081+080\n",
      "Assigned 20 protos to node 082+079\n",
      "Assigned 20 protos to node 065+066\n",
      "Assigned 20 protos to node 061+064\n",
      "Assigned 20 protos to node 144+142\n",
      "Assigned 20 protos to node 006+005\n",
      "Assigned 20 protos to node 008+106\n",
      "Assigned 20 protos to node 024+023\n",
      "Assigned 20 protos to node 100+101\n",
      "Assigned 20 protos to node 001+003\n",
      "Assigned 20 protos to node 129+199\n",
      "Assigned 20 protos to node 136+085\n",
      "Assigned 20 protos to node 107+111\n",
      "Assigned 20 protos to node 151+153\n",
      "Assigned 20 protos to node 043+037\n",
      "Assigned 20 protos to node 040+102\n",
      "Assigned 20 protos to node 078+077\n",
      "Assigned 20 protos to node 192+190\n",
      "Assigned 20 protos to node 065+062\n",
      "Assigned 20 protos to node 144+145\n",
      "Assigned 20 protos to node 006+007\n",
      "Assigned 20 protos to node 024+025\n",
      "Assigned 20 protos to node 001+002\n",
      "Assigned 20 protos to node 129+118\n",
      "Assigned 20 protos to node 199+186\n",
      "Assigned 20 protos to node 136+138\n",
      "Assigned 20 protos to node 107+073\n",
      "Assigned 20 protos to node 111+112\n",
      "Assigned 20 protos to node 151+157\n",
      "Assigned 20 protos to node 153+154\n",
      "Assigned 20 protos to node 043+039\n",
      "Assigned 20 protos to node 065+059\n",
      "Assigned 20 protos to node 144+146\n",
      "Assigned 20 protos to node 129+104\n",
      "Assigned 20 protos to node 199+150\n",
      "Assigned 20 protos to node 186+185\n",
      "Assigned 20 protos to node 136+137\n",
      "Assigned 20 protos to node 107+093\n",
      "Assigned 20 protos to node 073+074\n",
      "Assigned 20 protos to node 151+156\n",
      "Assigned 20 protos to node 157+152\n",
      "Assigned 20 protos to node 153+155\n",
      "Assigned 20 protos to node 065+060\n",
      "Assigned 20 protos to node 144+141\n",
      "Assigned 20 protos to node 129+035\n",
      "Assigned 20 protos to node 199+094\n",
      "Assigned 20 protos to node 150+019\n",
      "Assigned 20 protos to node 107+030\n",
      "Assigned 20 protos to node 129+054\n",
      "Assigned 20 protos to node 035+055\n",
      "Assigned 20 protos to node 199+028\n",
      "Assigned 20 protos to node 150+149\n",
      "Assigned 20 protos to node 107+029\n",
      "Assigned 20 protos to node 129+175\n",
      "Assigned 20 protos to node 054+140\n",
      "Assigned 20 protos to node 035+048\n",
      "Assigned 20 protos to node 199+198\n",
      "Assigned 20 protos to node 150+091\n",
      "Assigned 20 protos to node 107+108\n",
      "Assigned 20 protos to node 129+011\n",
      "Assigned 20 protos to node 175+020\n",
      "Assigned 20 protos to node 054+057\n",
      "Assigned 20 protos to node 140+017\n",
      "Assigned 20 protos to node 035+056\n",
      "Assigned 20 protos to node 048+047\n",
      "Assigned 20 protos to node 199+194\n",
      "Assigned 20 protos to node 129+121\n",
      "Assigned 20 protos to node 011+013\n",
      "Assigned 20 protos to node 175+099\n",
      "Assigned 20 protos to node 054+014\n",
      "Assigned 20 protos to node 140+139\n",
      "Assigned 20 protos to node 035+034\n",
      "Assigned 20 protos to node 199+193\n",
      "Assigned 20 protos to node 129+117\n",
      "Assigned 20 protos to node 011+095\n",
      "Assigned 20 protos to node 013+088\n",
      "Assigned 20 protos to node 175+181\n",
      "Assigned 20 protos to node 054+016\n",
      "Assigned 20 protos to node 199+197\n",
      "Assigned 20 protos to node 193+195\n",
      "Assigned 20 protos to node 129+133\n",
      "Assigned 20 protos to node 117+114\n",
      "Assigned 20 protos to node 011+026\n",
      "Assigned 20 protos to node 095+096\n",
      "Assigned 20 protos to node 013+012\n",
      "Assigned 20 protos to node 175+168+173+183\n",
      "Assigned 20 protos to node 054+015\n",
      "Assigned 20 protos to node 199+196\n",
      "Assigned 20 protos to node 129+021\n",
      "Assigned 20 protos to node 133+130\n",
      "Assigned 20 protos to node 117+115\n",
      "Assigned 20 protos to node 011+049\n",
      "Assigned 20 protos to node 026+010\n",
      "Assigned 20 protos to node 095+098\n",
      "Assigned 20 protos to node 096+097\n",
      "Assigned 20 protos to node 175+162\n",
      "Assigned 20 protos to node 168+177\n",
      "Assigned 20 protos to node 173+161\n",
      "Assigned 20 protos to node 183+159\n",
      "Assigned 20 protos to node 129+128\n",
      "Assigned 20 protos to node 021+148\n",
      "Assigned 20 protos to node 133+076\n",
      "Assigned 20 protos to node 130+120\n",
      "Assigned 20 protos to node 117+116\n",
      "Assigned 20 protos to node 115+119\n",
      "Assigned 20 protos to node 011+009\n",
      "Assigned 20 protos to node 026+027\n",
      "Assigned 20 protos to node 175+167\n",
      "Assigned 20 protos to node 162+180\n",
      "Assigned 20 protos to node 168+200\n",
      "Assigned 20 protos to node 177+178\n",
      "Assigned 20 protos to node 173+179\n",
      "Assigned 20 protos to node 161+166\n",
      "Assigned 20 protos to node 183+184\n",
      "Assigned 20 protos to node 129+127\n",
      "Assigned 20 protos to node 128+131\n",
      "Assigned 20 protos to node 133+132\n",
      "Assigned 20 protos to node 175+169\n",
      "Assigned 20 protos to node 168+170\n",
      "Assigned 20 protos to node 173+172\n",
      "Assigned 20 protos to node 129+123\n",
      "Assigned 20 protos to node 128+124\n",
      "Assigned 20 protos to node 133+122\n",
      "Assigned 20 protos to node 175+165+182\n",
      "Assigned 20 protos to node 129+125\n",
      "Assigned 20 protos to node 123+113\n",
      "Assigned 20 protos to node 128+126\n",
      "Assigned 20 protos to node 175+160\n",
      "Assigned 20 protos to node 165+164+163\n",
      "Assigned 20 protos to node 175+176\n",
      "Assigned 20 protos to node 160+109\n",
      "Assigned 20 protos to node 165+158\n",
      "Assigned 20 protos to node 175+174\n",
      "Output shape:  torch.Size([1, 20, 26, 26])\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    device_ids = [torch.cuda.current_device()]\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    device_ids = []\n",
    "\n",
    "# args_file = open(os.path.join(run_path, 'metadata', 'args.pickle'), 'rb')\n",
    "# args = pickle.load(args_file)\n",
    "\n",
    "# ckpt_file_name = 'net_overspecific_pruned_replaced_thresh=0.5_last'\n",
    "# ckpt_file_name = 'net_trained_30'\n",
    "# ckpt_file_name = 'net_trained_10'\n",
    "# ckpt_file_name = 'net_pretrained'\n",
    "ckpt_file_name = 'net_trained_last'\n",
    "epoch = ckpt_file_name.split('_')[-1]\n",
    "\n",
    "ckpt_path = os.path.join(run_path, 'checkpoints', ckpt_file_name)\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "if ckpt_file_name != 'net_trained_last':\n",
    "    print('\\n', (10*'-')+'WARNING: Not using the final trained model'+(10*'-'), '\\n')\n",
    "\n",
    "# Obtain the dataset and dataloaders\n",
    "trainloader, trainloader_pretraining, trainloader_normal, trainloader_normal_augment, projectloader, testloader, test_projectloader, classes = get_dataloaders(args, device)\n",
    "\n",
    "print(args.batch_size, trainloader.batch_size)\n",
    "\n",
    "if len(classes)<=20:\n",
    "    if args.validation_size == 0.:\n",
    "        print(\"Classes: \", testloader.dataset.class_to_idx, flush=True)\n",
    "    else:\n",
    "        print(\"Classes: \", str(classes), flush=True)\n",
    "\n",
    "# Create a convolutional network based on arguments and add 1x1 conv layer\n",
    "feature_net, add_on_layers, pool_layer, classification_layers, num_prototypes = get_network(len(classes), args, root=root)\n",
    "   \n",
    "# Create a PIP-Net\n",
    "net = PIPNet(num_classes=len(classes),\n",
    "                    num_prototypes=num_prototypes,\n",
    "                    feature_net = feature_net,\n",
    "                    args = args,\n",
    "                    add_on_layers = add_on_layers,\n",
    "                    pool_layer = pool_layer,\n",
    "                    classification_layers = classification_layers,\n",
    "                    num_parent_nodes = len(root.nodes_with_children()),\n",
    "                    root = root\n",
    "                    )\n",
    "\n",
    "# Create a PIP-Net\n",
    "if ('byol' in args) and (args.byol == 'y'):\n",
    "    from pipnet.pipnet import PIPNetBYOL\n",
    "    net = PIPNetBYOL(num_classes=len(classes),\n",
    "                        num_prototypes=num_prototypes,\n",
    "                        feature_net = feature_net,\n",
    "                        args = args,\n",
    "                        add_on_layers = add_on_layers,\n",
    "                        pool_layer = pool_layer,\n",
    "                        classification_layers = classification_layers,\n",
    "                        num_parent_nodes = len(root.nodes_with_children()),\n",
    "                        root = root\n",
    "                        )\n",
    "else:\n",
    "    net = PIPNet(num_classes=len(classes),\n",
    "                    num_prototypes=num_prototypes,\n",
    "                    feature_net = feature_net,\n",
    "                    args = args,\n",
    "                    add_on_layers = add_on_layers,\n",
    "                    pool_layer = pool_layer,\n",
    "                    classification_layers = classification_layers,\n",
    "                    num_parent_nodes = len(root.nodes_with_children()),\n",
    "                    root = root\n",
    "                    )\n",
    "        \n",
    "net = net.to(device=device)\n",
    "net = nn.DataParallel(net, device_ids = device_ids)    \n",
    "net.load_state_dict(checkpoint['model_state_dict'],strict=True)\n",
    "# print(net.eval())\n",
    "criterion = nn.NLLLoss(reduction='mean').to(device)\n",
    "\n",
    "# Forward one batch through the backbone to get the latent output size\n",
    "with torch.no_grad():\n",
    "    xs1, _, _ = next(iter(trainloader))\n",
    "    xs1 = xs1.to(device)\n",
    "    _, proto_features, _, _ = net(xs1)\n",
    "    wshape = proto_features['root'].shape[-1]\n",
    "    args.wshape = wshape #needed for calculating image patch size\n",
    "    print(\"Output shape: \", proto_features['root'].shape, flush=True)\n",
    "    \n",
    "print(args.wshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000,  3.2064,  0.0000,  3.8520,  0.7683,  0.0000,  5.0439,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.2300,  0.0658,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.5068,  0.0384,  0.0000, 12.2687]], device='cuda:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.module._root_classification.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find subtree root - only for finding does not affect the run, use the value found here in the visualization block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129+043\n"
     ]
    }
   ],
   "source": [
    "# leaf_descendents = set(['cub_067_Anna_Hummingbird', 'cub_070_Green_Violetear', 'cub_072_Pomarine_Jaeger'])\n",
    "# leaf_descendents = set(['cub_072_Pomarine_Jaeger', 'cub_083_White_breasted_Kingfisher'])\n",
    "leaf_descendents = set(['cub_072_Pomarine_Jaeger', 'cub_147_Least_Tern'])\n",
    "leaf_descendents = set(['cub_083_White_breasted_Kingfisher', 'cub_038_Great_Crested_Flycatcher'])\n",
    "leaf_descendents = set(['cub_018_Spotted_Catbird', 'cub_038_Great_Crested_Flycatcher'])\n",
    "\n",
    "subtree_root = root\n",
    "for node in root.nodes_with_children():\n",
    "    if leaf_descendents.issubset(node.leaf_descendents) and (len(node.leaf_descendents) < len(subtree_root.leaf_descendents)):\n",
    "        subtree_root = node\n",
    "\n",
    "# root.get_node('053+004')\n",
    "\n",
    "print(subtree_root.name)\n",
    "\n",
    "# 18 species subset 024+051"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Proto activations on leaf descendents - topk images using  NAIVE-HPIPNET with HEATMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping node root\n",
      "Skipping node 129+024+067\n",
      "Skipping node 089+046\n",
      "Skipping node 129+065\n",
      "Skipping node 024+051\n",
      "Skipping node 067+070\n",
      "Skipping node 089+090\n",
      "Skipping node 046+087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 4116it [02:56, 23.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 129+192\n",
      "Skipping 129+043\n",
      "\t Child: 192+081\n",
      "\t\tProto:19 036:(0.1063) 079:(0.3009) 080:(0.3149) 081:(0.3018) 082:(0.3851) 083:(0.315) 187:(0.136) 188:(0.1453) 189:(0.1149) 190:(0.3018) 191:(0.187) 192:(0.189) \n",
      "\t\tProto:17 036:(0.0504) 079:(0.187) 080:(0.1817) 081:(0.1395) 082:(0.1947) 083:(0.4346) 187:(0.1182) 188:(0.1301) 189:(0.0795) 190:(0.147) 191:(0.115) 192:(0.1938) \n",
      "\t\tProto:11 036:(0.9067) 079:(0.9154) 080:(0.9002) 081:(0.6546) 082:(0.923) 083:(0.564) 187:(0.5684) 188:(0.8395) 189:(0.9648) 190:(0.9478) 191:(0.7516) 192:(0.7791) \n",
      "\t\tProto:14 036:(0.0652) 079:(0.5519) 080:(0.4787) 081:(0.4297) 082:(0.7067) 083:(0.8716) 187:(0.6352) 188:(0.5102) 189:(0.3007) 190:(0.0792) 191:(0.4077) 192:(0.5158) \n",
      "Skipping node 065+006\n",
      "Skipping node 024+031\n",
      "Skipping node 051+052\n",
      "Skipping node 067+068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 3767it [02:41, 23.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 129+043\n",
      "Skipping 129+018\n",
      "Skipping 043+078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 349it [00:19, 17.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 192+081\n",
      "\t Child: 192+036\n",
      "\t\tProto:0 036:(0.9996) 187:(0.9842) 188:(0.9973) 189:(0.9989) 190:(0.945) 191:(0.9988) 192:(0.9732) \n",
      "\t\tProto:9 036:(0.9995) 187:(0.9991) 188:(0.9967) 189:(1.0) 190:(0.9987) 191:(0.9999) 192:(0.9968) \n",
      "\t\tProto:2 036:(0.9999) 187:(1.0) 188:(1.0) 189:(0.9992) 190:(0.9999) 191:(0.9999) 192:(1.0) \n",
      "\t\tProto:7 036:(0.9999) 187:(0.9973) 188:(0.9926) 189:(0.9998) 190:(0.9978) 191:(0.9989) 192:(0.9985) \n",
      "\t Child: 081+083\n",
      "\t\tProto:11 079:(0.9983) 080:(0.9995) 081:(0.9991) 082:(0.9992) 083:(0.9942) \n",
      "\t\tProto:12 079:(0.9899) 080:(0.9958) 081:(0.9921) 082:(0.9829) 083:(0.9642) \n",
      "\t\tProto:14 079:(0.9993) 080:(0.9998) 081:(0.999) 082:(0.999) 083:(0.9964) \n",
      "\t\tProto:15 079:(0.9999) 080:(1.0) 081:(1.0) 082:(0.9999) 083:(1.0) \n",
      "\t\tProto:16 079:(1.0) 080:(1.0) 081:(0.9987) 082:(1.0) 083:(0.9998) \n",
      "Skipping node 065+144\n",
      "Skipping node 006+071\n",
      "Skipping node 024+086\n",
      "Skipping node 031+004\n",
      "Skipping node 051+053\n",
      "Skipping node 067+069\n",
      "Skipping node 129+018\n",
      "Skipping node 043+078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 199it [00:13, 14.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 192+036\n",
      "\t Child: 192+191\n",
      "\t\tProto:9 187:(0.9948) 189:(0.9933) 190:(0.9925) 191:(0.9984) 192:(0.9939) \n",
      "\t\tProto:2 187:(1.0) 189:(0.9995) 190:(0.9993) 191:(1.0) 192:(0.9998) \n",
      "\t Child: 036+188\n",
      "\t\tProto:10 036:(0.999) 188:(0.9996) \n",
      "\t\tProto:14 036:(1.0) 188:(0.9996) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 150it [00:11, 13.39it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 081+083\n",
      "\t Child: 081+082\n",
      "\t\tProto:2 079:(0.9999) 080:(1.0) 081:(0.9999) 082:(0.9999) \n",
      "\t\tProto:4 079:(1.0) 080:(0.9999) 081:(0.9999) 082:(1.0) \n",
      "\t Child: cub_083_White_breasted_Kingfisher\n",
      "\t\tProto:12 083:(0.9995) \n",
      "\t\tProto:15 083:(1.0) \n",
      "Skipping node 065+084\n",
      "Skipping node 144+147\n",
      "Skipping node 006+058\n",
      "Skipping node 071+072\n",
      "Skipping node 024+001\n",
      "Skipping node 031+032\n",
      "Skipping node 051+050\n",
      "Skipping node 129+107\n",
      "Skipping node 043+042\n",
      "Skipping node 078+038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 139it [00:11, 12.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 192+191\n",
      "\t Child: 192+187\n",
      "\t\tProto:8 187:(0.9989) 190:(0.9992) 192:(0.9998) \n",
      "\t\tProto:5 187:(1.0) 190:(1.0) 192:(1.0) \n",
      "\t\tProto:6 187:(1.0) 190:(1.0) 192:(1.0) \n",
      "\t Child: 191+189\n",
      "\t\tProto:17 189:(0.9998) 191:(0.9999) \n",
      "\t\tProto:13 189:(1.0) 191:(1.0) \n",
      "\t\tProto:14 189:(1.0) 191:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 60/60 [00:07<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 036+188\n",
      "\t Child: cub_036_Northern_Flicker\n",
      "\t\tProto:2 036:(1.0) \n",
      "\t Child: cub_188_Pileated_Woodpecker\n",
      "\t\tProto:11 188:(1.0) \n",
      "\t\tProto:12 188:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 120it [00:10, 11.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 081+082\n",
      "\t Child: 081+080\n",
      "\t\tProto:0 080:(1.0) 081:(1.0) \n",
      "\t Child: 082+079\n",
      "\t\tProto:12 079:(1.0) 082:(0.9994) \n",
      "\t\tProto:13 079:(1.0) 082:(0.9997) \n",
      "Skipping node 065+061\n",
      "Skipping node 084+063\n",
      "Skipping node 144+143\n",
      "Skipping node 006+008\n",
      "Skipping node 024+100\n",
      "Skipping node 001+045\n",
      "Skipping node 031+033\n",
      "Skipping node 129+136\n",
      "Skipping node 107+151\n",
      "Skipping node 043+040\n",
      "Skipping node 078+041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 79it [00:08,  9.40it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 192+187\n",
      "\t Child: 192+190\n",
      "\t\tProto:2 190:(1.0) 192:(1.0) \n",
      "\t Child: cub_187_American_Three_toed_Woodpecker\n",
      "\t\tProto:13 187:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 60/60 [00:07<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 191+189\n",
      "\t Child: cub_191_Red_headed_Woodpecker\n",
      "\t\tProto:0 191:(1.0) \n",
      "\t\tProto:5 191:(1.0) \n",
      "\t Child: cub_189_Red_bellied_Woodpecker\n",
      "\t\tProto:19 189:(1.0) \n",
      "\t\tProto:12 189:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 60/60 [00:07<00:00,  8.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 081+080\n",
      "\t Child: cub_081_Pied_Kingfisher\n",
      "\t\tProto:7 081:(1.0) \n",
      "\t Child: cub_080_Green_Kingfisher\n",
      "\t\tProto:17 080:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 60/60 [00:07<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 082+079\n",
      "\t Child: cub_082_Ringed_Kingfisher\n",
      "\t\tProto:6 082:(1.0) \n",
      "\t Child: cub_079_Belted_Kingfisher\n",
      "\t\tProto:12 079:(1.0) \n",
      "Skipping node 065+066\n",
      "Skipping node 061+064\n",
      "Skipping node 144+142\n",
      "Skipping node 006+005\n",
      "Skipping node 008+106\n",
      "Skipping node 024+023\n",
      "Skipping node 100+101\n",
      "Skipping node 001+003\n",
      "Skipping node 129+199\n",
      "Skipping node 136+085\n",
      "Skipping node 107+111\n",
      "Skipping node 151+153\n",
      "Skipping node 043+037\n",
      "Skipping node 040+102\n",
      "Skipping node 078+077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 59/59 [00:07<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 192+190\n",
      "\t Child: cub_192_Downy_Woodpecker\n",
      "\t\tProto:4 192:(1.0) \n",
      "\t Child: cub_190_Red_cockaded_Woodpecker\n",
      "\t\tProto:10 190:(1.0) \n",
      "\t\tProto:18 190:(1.0) \n",
      "Skipping node 065+062\n",
      "Skipping node 144+145\n",
      "Skipping node 006+007\n",
      "Skipping node 024+025\n",
      "Skipping node 001+002\n",
      "Skipping node 129+118\n",
      "Skipping node 199+186\n",
      "Skipping node 136+138\n",
      "Skipping node 107+073\n",
      "Skipping node 111+112\n",
      "Skipping node 151+157\n",
      "Skipping node 153+154\n",
      "Skipping node 043+039\n",
      "Skipping node 065+059\n",
      "Skipping node 144+146\n",
      "Skipping node 129+104\n",
      "Skipping node 199+150\n",
      "Skipping node 186+185\n",
      "Skipping node 136+137\n",
      "Skipping node 107+093\n",
      "Skipping node 073+074\n",
      "Skipping node 151+156\n",
      "Skipping node 157+152\n",
      "Skipping node 153+155\n",
      "Skipping node 065+060\n",
      "Skipping node 144+141\n",
      "Skipping node 129+035\n",
      "Skipping node 199+094\n",
      "Skipping node 150+019\n",
      "Skipping node 107+030\n",
      "Skipping node 129+054\n",
      "Skipping node 035+055\n",
      "Skipping node 199+028\n",
      "Skipping node 150+149\n",
      "Skipping node 107+029\n",
      "Skipping node 129+175\n",
      "Skipping node 054+140\n",
      "Skipping node 035+048\n",
      "Skipping node 199+198\n",
      "Skipping node 150+091\n",
      "Skipping node 107+108\n",
      "Skipping node 129+011\n",
      "Skipping node 175+020\n",
      "Skipping node 054+057\n",
      "Skipping node 140+017\n",
      "Skipping node 035+056\n",
      "Skipping node 048+047\n",
      "Skipping node 199+194\n",
      "Skipping node 129+121\n",
      "Skipping node 011+013\n",
      "Skipping node 175+099\n",
      "Skipping node 054+014\n",
      "Skipping node 140+139\n",
      "Skipping node 035+034\n",
      "Skipping node 199+193\n",
      "Skipping node 129+117\n",
      "Skipping node 011+095\n",
      "Skipping node 013+088\n",
      "Skipping node 175+181\n",
      "Skipping node 054+016\n",
      "Skipping node 199+197\n",
      "Skipping node 193+195\n",
      "Skipping node 129+133\n",
      "Skipping node 117+114\n",
      "Skipping node 011+026\n",
      "Skipping node 095+096\n",
      "Skipping node 013+012\n",
      "Skipping node 175+168+173+183\n",
      "Skipping node 054+015\n",
      "Skipping node 199+196\n",
      "Skipping node 129+021\n",
      "Skipping node 133+130\n",
      "Skipping node 117+115\n",
      "Skipping node 011+049\n",
      "Skipping node 026+010\n",
      "Skipping node 095+098\n",
      "Skipping node 096+097\n",
      "Skipping node 175+162\n",
      "Skipping node 168+177\n",
      "Skipping node 173+161\n",
      "Skipping node 183+159\n",
      "Skipping node 129+128\n",
      "Skipping node 021+148\n",
      "Skipping node 133+076\n",
      "Skipping node 130+120\n",
      "Skipping node 117+116\n",
      "Skipping node 115+119\n",
      "Skipping node 011+009\n",
      "Skipping node 026+027\n",
      "Skipping node 175+167\n",
      "Skipping node 162+180\n",
      "Skipping node 168+200\n",
      "Skipping node 177+178\n",
      "Skipping node 173+179\n",
      "Skipping node 161+166\n",
      "Skipping node 183+184\n",
      "Skipping node 129+127\n",
      "Skipping node 128+131\n",
      "Skipping node 133+132\n",
      "Skipping node 175+169\n",
      "Skipping node 168+170\n",
      "Skipping node 173+172\n",
      "Skipping node 129+123\n",
      "Skipping node 128+124\n",
      "Skipping node 133+122\n",
      "Skipping node 175+165+182\n",
      "Skipping node 129+125\n",
      "Skipping node 123+113\n",
      "Skipping node 128+126\n",
      "Skipping node 175+160\n",
      "Skipping node 165+164+163\n",
      "Skipping node 175+176\n",
      "Skipping node 160+109\n",
      "Skipping node 165+158\n",
      "Skipping node 175+174\n",
      "Done !!!\n"
     ]
    }
   ],
   "source": [
    "# Proto activations on leaf descendents - topk images\n",
    "\n",
    "def get_heatmap(latent_activation, input_image, constant_color_scale=False):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    # image_a = (image_a - image_a.min()) / (image_a.max() - image_a.min())\n",
    "\n",
    "    # input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "    image_b = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((input_image.shape[-1], input_image.shape[-1])))\n",
    "\n",
    "    if constant_color_scale:\n",
    "        reshaped_image_a = np.concatenate((reshaped_image_a, np.zeros((reshaped_image_a.shape[1], 1)), np.ones((reshaped_image_a.shape[1], 1))*255), axis=1)\n",
    "    \n",
    "    normalized_heatmap = (reshaped_image_a - np.min(reshaped_image_a)) / (np.max(reshaped_image_a) - np.min(reshaped_image_a))\n",
    "\n",
    "    heatmap_colormap = plt.get_cmap('jet')\n",
    "    heatmap_colored = heatmap_colormap(normalized_heatmap)\n",
    "\n",
    "    if constant_color_scale:\n",
    "        heatmap_colored = heatmap_colored[:, :-2]\n",
    "    \n",
    "    heatmap_colored_uint8 = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_a_heatmap_pillow = Image.fromarray(heatmap_colored_uint8)\n",
    "    image_b_pillow = Image.fromarray((image_b * 255).astype('uint8'))\n",
    "    \n",
    "    result_image = Image.blend(image_b_pillow, image_a_heatmap_pillow, alpha=0.3)\n",
    "    \n",
    "    return np.array(result_image)\n",
    "\n",
    "def get_heatmap_uninterpolated(latent_activation, input_image):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    image_a = (image_a - image_a.min()) / (image_a.max() - image_a.min())\n",
    "\n",
    "    input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "    image_b = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((input_image.shape[-1], input_image.shape[-1]), \\\n",
    "                                                                                          resample=Image.NEAREST ))\n",
    "    normalized_heatmap = (reshaped_image_a - np.min(reshaped_image_a)) / (np.max(reshaped_image_a) - np.min(reshaped_image_a))\n",
    "    \n",
    "    heatmap_colormap = plt.get_cmap('jet')\n",
    "    heatmap_colored = heatmap_colormap(normalized_heatmap)\n",
    "    \n",
    "    heatmap_colored_uint8 = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_a_heatmap_pillow = Image.fromarray(heatmap_colored_uint8)\n",
    "    image_b_pillow = Image.fromarray((image_b * 255).astype('uint8'))\n",
    "    \n",
    "    result_image = Image.blend(image_b_pillow, image_a_heatmap_pillow, alpha=0.3)\n",
    "    \n",
    "    return np.array(result_image)\n",
    "\n",
    "from util.data import ModifiedLabelLoader\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import pdb\n",
    "from util.vis_pipnet import get_img_coordinates\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import ImageFont, Image, ImageDraw as D\n",
    "import torchvision\n",
    "from datetime import datetime\n",
    "txt_file = open(os.path.join(run_path, \"num_proto_details_\"+datetime.now().strftime(\"%m:%d:%H:%M:%S\")+\".txt\"), \"a\")\n",
    "txt_file.write('\\n')\n",
    "\n",
    "vizloader_name = 'testloader' # projectloader\n",
    "find_non_descendants = False # True, False # param\n",
    "topk = 3\n",
    "save_images = True # True, False\n",
    "font = ImageFont.truetype(\"arial.ttf\", 50)\n",
    "save_activation_as_npy_path = None # 'activation_as_npy'\n",
    "if (save_activation_as_npy_path is not None):\n",
    "    save_activation_as_npy_path = '_'.join(['activation_as_npy', vizloader_name])  # activation_as_npy, added for NUMPY SAVING\n",
    "if find_non_descendants and (save_activation_as_npy_path is not None):\n",
    "    save_activation_as_npy_path = save_activation_as_npy_path + '_non_desc'\n",
    "plot_overspecificity_score = True\n",
    "subtree_root = root.get_node('024+051')\n",
    "\n",
    "from datetime import datetime\n",
    "txt_file = open(os.path.join(run_path, \"num_proto_details_\"+datetime.now().strftime(\"%m:%d:%H:%M:%S\")+\".txt\"), \"a\")\n",
    "txt_file.write('\\n')\n",
    "\n",
    "def write_num_proto_details(proto_mean_activations, node_name, net, threshold, txt_file, args):\n",
    "    \n",
    "    rand_input = torch.randn((1, 3, args.image_size, args.image_size))\n",
    "    with torch.no_grad():\n",
    "        *_, pooled, out = net(rand_input)\n",
    "    num_protos = pooled[node_name].shape[1]\n",
    "    used_protos = len(proto_mean_activations)\n",
    "    non_overspecific = 0\n",
    "    for p in proto_mean_activations:\n",
    "        logstr = '\\t'*2 + f'Proto:{p} '\n",
    "        protos_mean_for_all_leaf_descedants = []\n",
    "        for leaf_descendent in proto_mean_activations[p]:\n",
    "            mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "            protos_mean_for_all_leaf_descedants.append(mean_activation)\n",
    "            \n",
    "        if all([(mean_activation>0.2) for mean_activation in protos_mean_for_all_leaf_descedants]):\n",
    "            non_overspecific += 1\n",
    "            \n",
    "    txt_file.write(f\"Node:{node_name},Total:{num_protos},Used:{used_protos},Good:{non_overspecific},threshold={threshold}\\n\")\n",
    "\n",
    "\n",
    "def get_heap():\n",
    "    list_ = []\n",
    "    heapq.heapify(list_)\n",
    "    return list_\n",
    "\n",
    "patchsize, skip = get_patch_size(args)\n",
    "\n",
    "\n",
    "vizloader_dict = {'trainloader': trainloader,\n",
    "                 'projectloader': projectloader,\n",
    "                 'testloader': testloader,\n",
    "                 'test_projectloader': test_projectloader}\n",
    "vizloader_dict[vizloader_name] = unshuffle_dataloader(vizloader_dict[vizloader_name])\n",
    "\n",
    "\n",
    "if type(vizloader_dict[vizloader_name].dataset) == ImageFolder:\n",
    "    name2label = vizloader_dict[vizloader_name].dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "else:\n",
    "    name2label = vizloader_dict[vizloader_name].dataset.dataset.dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "    \n",
    "overspecificity_score_and_proto_mask = []\n",
    "\n",
    "for node in root.nodes_with_children():\n",
    "#     if node.name == 'root':\n",
    "#         continue\n",
    "#     non_leaf_children_names = [child.name for child in node.children if not child.is_leaf()]\n",
    "#     if len(non_leaf_children_names) == 0: # if all the children are leaf nodes then skip this node\n",
    "#         continue\n",
    "\n",
    "    if (node.name not in subtree_root.descendents) and (node.name != subtree_root.name):\n",
    "        print('Skipping node', node.name)\n",
    "        continue\n",
    "\n",
    "    if node.name in root.get_node('129+043').descendents:\n",
    "        print('Skipping node', node.name)\n",
    "        continue\n",
    "        \n",
    "    name2label = vizloader_dict[vizloader_name].dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "    modifiedLabelLoader = ModifiedLabelLoader(vizloader_dict[vizloader_name], node)\n",
    "    coarse_label2name = modifiedLabelLoader.modifiedlabel2name\n",
    "    node_label_to_children = {label: name for name, label in node.children_to_labels.items()}\n",
    "    \n",
    "    imgs = modifiedLabelLoader.filtered_imgs\n",
    "\n",
    "    img_iter = tqdm(enumerate(modifiedLabelLoader),\n",
    "                    total=len(modifiedLabelLoader),\n",
    "                    mininterval=50.,\n",
    "                    desc='Collecting topk',\n",
    "                    ncols=0)\n",
    "\n",
    "    classification_weights = getattr(net.module, '_'+node.name+'_classification').weight\n",
    "    \n",
    "    # maps proto_number -> grand_child_name (or descendant leaf name) -> list of top-k activations\n",
    "    proto_mean_activations = defaultdict(lambda: defaultdict(get_heap))\n",
    "\n",
    "    # maps class names to the prototypes that belong to that\n",
    "    class_and_prototypes = defaultdict(set)\n",
    "\n",
    "    for i, (xs, orig_y, ys) in img_iter:\n",
    "#         if coarse_label2name[ys.item()] not in non_leaf_children_names:\n",
    "#             continue\n",
    "\n",
    "        xs, ys = xs.to(device), ys.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output = net(xs, inference=False)\n",
    "            if len(model_output) == 3:\n",
    "                softmaxes, pooled, _ = model_output\n",
    "            elif len(model_output) == 4:\n",
    "                _, softmaxes, pooled, _ = model_output\n",
    "            pooled = pooled[node.name].squeeze(0) \n",
    "            softmaxes = softmaxes[node.name]#.squeeze(0)\n",
    "\n",
    "            for p in range(pooled.shape[0]): # pooled.shape -> [768] (== num of prototypes)\n",
    "                c_weight = torch.max(classification_weights[:,p]) # classification_weights[:,p].shape -> [200] (== num of classes)\n",
    "                relevant_proto_classes = torch.nonzero(classification_weights[:, p] > 1e-3)\n",
    "                relevant_proto_class_names = [node_label_to_children[class_idx.item()] for class_idx in relevant_proto_classes]\n",
    "                \n",
    "                # Take the max per prototype.                             \n",
    "                max_per_prototype, max_idx_per_prototype = torch.max(softmaxes, dim=0)\n",
    "                max_per_prototype_h, max_idx_per_prototype_h = torch.max(max_per_prototype, dim=1)\n",
    "                max_per_prototype_w, max_idx_per_prototype_w = torch.max(max_per_prototype_h, dim=1) #shape (num_prototypes)\n",
    "                \n",
    "                h_idx = max_idx_per_prototype_h[p, max_idx_per_prototype_w[p]]\n",
    "                w_idx = max_idx_per_prototype_w[p]\n",
    "\n",
    "                if len(relevant_proto_class_names) == 0:\n",
    "                    continue\n",
    "                \n",
    "#                 if (len(relevant_proto_class_names) == 1) and (relevant_proto_class_names[0] not in non_leaf_children_names):\n",
    "#                     continue\n",
    "                \n",
    "                h_coor_min, h_coor_max, w_coor_min, w_coor_max = get_img_coordinates(args.image_size, softmaxes.shape, patchsize, skip, h_idx, w_idx)\n",
    "                latent_activation = softmaxes[:, p, :, :]\n",
    "                \n",
    "                if not find_non_descendants:\n",
    "                    if (coarse_label2name[ys.item()] in relevant_proto_class_names):\n",
    "                        child_node = root.get_node(coarse_label2name[ys.item()])\n",
    "                        leaf_descendent = label2name[orig_y.item()][4:7]\n",
    "                        img_to_open = imgs[i][0] # it is a tuple of (path to image, lable)\n",
    "                        if topk and (len(proto_mean_activations[p][leaf_descendent]) >= topk):\n",
    "                            heapq.heappushpop(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                              (pooled[p].item(), img_to_open,\\\n",
    "                                               (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "                        else:\n",
    "                            heapq.heappush(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                           (pooled[p].item(), img_to_open,\\\n",
    "                                            (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "                else:\n",
    "                    if (coarse_label2name[ys.item()] not in relevant_proto_class_names):\n",
    "                        child_node = root.get_node(coarse_label2name[ys.item()])\n",
    "                        leaf_descendent = label2name[orig_y.item()][4:7]\n",
    "                        img_to_open = imgs[i][0] # it is a tuple of (path to image, lable)\n",
    "                        if topk and (len(proto_mean_activations[p][leaf_descendent]) >= topk):\n",
    "                            heapq.heappushpop(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                              (pooled[p].item(), img_to_open,\\\n",
    "                                               (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "                        else:\n",
    "                            heapq.heappush(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                           (pooled[p].item(), img_to_open,\\\n",
    "                                            (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "\n",
    "                class_and_prototypes[', '.join(relevant_proto_class_names)].add(p)\n",
    "\n",
    "    # write_num_proto_details(proto_mean_activations, node.name, net, threshold=0.2, txt_file=txt_file, args=args)\n",
    "\n",
    "    if plot_overspecificity_score:\n",
    "        for child_classname in class_and_prototypes:\n",
    "            for p in class_and_prototypes[child_classname]:\n",
    "                mean_activation_of_every_leaf = []\n",
    "                for leaf_descendent in proto_mean_activations[p]:\n",
    "                    mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "                    mean_activation_of_every_leaf.append(mean_activation)\n",
    "\n",
    "                overspecificity_score = 1\n",
    "                for mean_act in mean_activation_of_every_leaf:\n",
    "                    overspecificity_score *= mean_act * 1.0\n",
    "                proto_presence = getattr(net.module, '_'+node.name+'_proto_presence')\n",
    "                proto_presence = F.gumbel_softmax(proto_presence, tau=0.5, hard=True, dim=-1)\n",
    "                proto_mask = proto_presence[p, 1].item()\n",
    "                overspecificity_score_and_proto_mask.append((overspecificity_score, len(mean_activation_of_every_leaf), proto_mask))\n",
    "\n",
    "    print('Node', node.name)\n",
    "    for child_classname in class_and_prototypes:\n",
    "\n",
    "        if (child_classname in root.get_node('129+043').descendents) or (child_classname == '129+043'):\n",
    "            print('Skipping', child_classname)\n",
    "            continue\n",
    "        \n",
    "        print('\\t'*1, 'Child:', child_classname)\n",
    "        for p in class_and_prototypes[child_classname]:\n",
    "            \n",
    "            logstr = '\\t'*2 + f'Proto:{p} '\n",
    "            mean_activation_of_every_leaf = []\n",
    "            for leaf_descendent in proto_mean_activations[p]:\n",
    "                mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "                num_images = len(proto_mean_activations[p][leaf_descendent])\n",
    "                logstr += f'{leaf_descendent}:({mean_activation}) '\n",
    "                mean_activation_of_every_leaf.append(mean_activation)\n",
    "            print(logstr)\n",
    "            \n",
    "            # # if the mean_activation is less for all leaf descendants skip the node\n",
    "            # if all([mean_act < 0.2 for mean_act in mean_activation_of_every_leaf]):\n",
    "            #     if find_non_descendants:\n",
    "            #         print('\\t'*2 + f'Not skipping proto {p} of {node.name} coz of find_non_descendants')\n",
    "            #     else:\n",
    "            #         print('\\t'*2 + f'Skipping proto {p} of {node.name}')\n",
    "            #         continue\n",
    "            \n",
    "            # have this for NON descendants\n",
    "            if len(proto_mean_activations[p]) == 0:\n",
    "                continue\n",
    "            \n",
    "            if save_images or save_activation_as_npy_path:\n",
    "                patches = []\n",
    "                right_descriptions = []\n",
    "                text_region_width = 3 # 3x the width of a patch\n",
    "                for leaf_descendent, heap in proto_mean_activations[p].items():\n",
    "                    heap = sorted(heap)[::-1]\n",
    "                    mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "                    for rank, ele in enumerate(heap):\n",
    "                        activation, img_to_open, (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation = ele\n",
    "                        image = transforms.Resize(size=(args.image_size, args.image_size))(Image.open(img_to_open))\n",
    "                        img_tensor = transforms.ToTensor()(image)#.unsqueeze_(0) #shape (1, 3, h, w)\n",
    "                        img_tensor_patch = img_tensor[:, h_coor_min:h_coor_max, w_coor_min:w_coor_max]\n",
    "                        overlayed_image_np = get_heatmap(latent_activation, img_tensor, constant_color_scale=True)\n",
    "                        overlayed_image = torch.tensor(overlayed_image_np).permute(2, 0, 1).float() / 255.\n",
    "                        patches.append(overlayed_image)\n",
    "                        \n",
    "                        # added for NUMPY SAVING\n",
    "                        \n",
    "                        if save_activation_as_npy_path:\n",
    "#                             upscaled_similarity_interpolated = get_upscaled_activation_interpolated(latent_activation,\n",
    "#                                                                                        image_size=(args.image_size, args.image_size))\n",
    "                            latent_activation_npy = latent_activation.squeeze().cpu().numpy()\n",
    "                            data = {'node_name': node.name,\n",
    "                                    'proto_num': p,\n",
    "                                    'child_name': child_classname,\n",
    "                                    'leaf_desc': leaf_descendent,\n",
    "                                     'rank': rank,\n",
    "                                     'img_path': img_to_open,\n",
    "                                     'img_filename': ntpath.basename(img_to_open),\n",
    "                                     'activation': latent_activation_npy,\n",
    "                                     'max_activation': activation,\n",
    "                                     'model_type': 'NAIVE-HPIPNET'}\n",
    "                            filename = str(rank)+ '-' + ntpath.basename(img_to_open) + '.npy'\n",
    "                            save_path = os.path.join(run_path, save_activation_as_npy_path, \\\n",
    "                                                     node.name, str(p), leaf_descendent,\n",
    "                                                     filename)\n",
    "                            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                            np.save(save_path, data, allow_pickle=True)\n",
    "\n",
    "                    # description on the right hand side\n",
    "                    text = f'{mean_activation}, {leaf_descendent}'\n",
    "                    txtimage = Image.new(\"RGB\", (patches[0].shape[-2]*text_region_width,patches[0].shape[-1]), (0, 0, 0))\n",
    "                    draw = D.Draw(txtimage)\n",
    "                    draw.text((150, patches[0].shape[1]//2), text, anchor='mm', fill=\"white\", font=font)\n",
    "                    txttensor = transforms.ToTensor()(txtimage)#.unsqueeze_(0)\n",
    "                    right_descriptions.append(txttensor)\n",
    "                    \n",
    "                # weird thing padding should be zero for non descendants else it raises some error # change\n",
    "                if find_non_descendants or (len(patches) == topk): # (len(patches) == topk) means there is only one leaf descendant\n",
    "                    padding = 0\n",
    "                else:\n",
    "                    padding = 1\n",
    "\n",
    "                grid = torchvision.utils.make_grid(patches, nrow=topk, padding=padding)\n",
    "                grid_right_descriptions = torchvision.utils.make_grid(right_descriptions, nrow=1, padding=padding)\n",
    "\n",
    "                # merging right description with the grid of images\n",
    "                try:\n",
    "                    grid = torch.cat([grid, grid_right_descriptions], dim=-1)\n",
    "                except:\n",
    "                    pdb.set_trace()\n",
    "\n",
    "                # description on the top\n",
    "                text = f'Node:{node.name}, p{p}, Child:{child_classname}'\n",
    "                txtimage = Image.new(\"RGB\", (grid.shape[-1], args.wshape), (0, 0, 0))\n",
    "                draw = D.Draw(txtimage)\n",
    "                draw.text((150, patches[0].shape[1]//2), text, anchor='mm', fill=\"white\", font=font)\n",
    "                txttensor = transforms.ToTensor()(txtimage)#.unsqueeze_(0)\n",
    "\n",
    "                # merging top description with the grid of images\n",
    "                grid = torch.cat([grid, txttensor], dim=1)\n",
    "                \n",
    "                if save_images:\n",
    "                    prefix = 'non_' if find_non_descendants else ''\n",
    "                    os.makedirs(os.path.join(run_path, prefix+f'descendent_specific_topk_heatmap_ep={epoch}_{subtree_root.name}', node.name), exist_ok=True)\n",
    "                    torchvision.utils.save_image(grid, os.path.join(run_path, prefix+f'descendent_specific_topk_heatmap_ep={epoch}_{subtree_root.name}', node.name, f'{child_classname}-p{p}.png'))\n",
    "\n",
    "txt_file.write('\\n')\n",
    "txt_file.close()\n",
    "print('Done !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "129+024+067\n",
      "024+051\n",
      "051+052\n",
      "051+053\n"
     ]
    }
   ],
   "source": [
    "target_leaf = root.get_node('cub_053_Western_Grebe')\n",
    "for node in root.nodes_with_children():\n",
    "    if target_leaf.name in node.leaf_descendents:\n",
    "        print(node.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Proto activations on leaf descendents - topk images using  NAIVE-HPIPNET with HEATMAP (clean visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping node root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 208it [00:05, 41.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 001+029+044\n",
      "\t Child: 001+010\n",
      "\t\tProto:0 but_001_Taygetis_cleopatra_ott3105132:(0.9757) but_002_Taygetis_thamyra_ott483838:(0.6139) but_003_Morpho_menelaus_ott830884:(0.9623) but_004_Morpho_helenor_ott634337:(0.9988) but_005_Opsiphanes_invirae_ott1056460:(0.9996) but_006_Catoblepia_berecynthia_ott3104050:(0.8694) but_007_Caligo_idomeneus_ott401861:(0.9993) but_008_Caligo_eurilochus_ott572882:(0.996) but_009_Bia_actorion_ott451938:(0.9729) but_010_Hypna_clytemnestra_ott552641:(0.9763) but_011_Memphis_glauce_ott235310:(0.9948) but_012_Memphis_moruus_ott834399:(0.9584) \n",
      "\t\tProto:8 but_001_Taygetis_cleopatra_ott3105132:(0.8395) but_002_Taygetis_thamyra_ott483838:(0.9044) but_003_Morpho_menelaus_ott830884:(0.9685) but_004_Morpho_helenor_ott634337:(0.6457) but_005_Opsiphanes_invirae_ott1056460:(0.8951) but_006_Catoblepia_berecynthia_ott3104050:(0.5974) but_007_Caligo_idomeneus_ott401861:(0.9528) but_008_Caligo_eurilochus_ott572882:(0.7243) but_009_Bia_actorion_ott451938:(0.7273) but_010_Hypna_clytemnestra_ott552641:(0.2188) but_011_Memphis_glauce_ott235310:(0.7898) but_012_Memphis_moruus_ott834399:(0.6904) \n",
      "\t\tProto:6 but_001_Taygetis_cleopatra_ott3105132:(0.8984) but_002_Taygetis_thamyra_ott483838:(0.958) but_003_Morpho_menelaus_ott830884:(0.9913) but_004_Morpho_helenor_ott634337:(0.7427) but_005_Opsiphanes_invirae_ott1056460:(0.3894) but_006_Catoblepia_berecynthia_ott3104050:(0.6072) but_007_Caligo_idomeneus_ott401861:(0.9947) but_008_Caligo_eurilochus_ott572882:(0.9955) but_009_Bia_actorion_ott451938:(0.0197) but_010_Hypna_clytemnestra_ott552641:(0.907) but_011_Memphis_glauce_ott235310:(0.7355) but_012_Memphis_moruus_ott834399:(0.9151) \n",
      "\t Child: 029+038\n",
      "\t\tProto:10 but_013_Heliconius_atthis_ott1024619:(1.0) but_014_Heliconius_elevatus_ott1034618:(0.9998) but_015_Heliconius_ethilla_ott358146:(1.0) but_016_Heliconius_numata_ott358148:(1.0) but_017_Heliconius_ismenius_ott984238:(1.0) but_018_Heliconius_melpomene_ott896444:(1.0) but_019_Heliconius_timareta_ott145706:(1.0) but_020_Heliconius_cydno_ott984236:(1.0) but_021_Heliconius_pachinus_ott984241:(0.9999) but_022_Heliconius_wallacei_ott984240:(1.0) but_023_Heliconius_hierax_ott372033:(1.0) but_024_Heliconius_xanthocles_ott1034613:(1.0) but_028_Heliconius_eleuchia_ott407976:(1.0) but_029_Heliconius_sara_ott1034608:(1.0) but_030_Heliconius_charithonia_ott259142:(1.0) but_032_Heliconius_clysonymus_ott1034615:(0.9998) but_033_Heliconius_telesiphe_ott984230:(1.0) but_034_Eueides_isabella_ott1034607:(1.0) but_035_Eueides_aliphera_ott1034606:(1.0) but_036_Dryadula_phaetusa_ott896449:(0.9059) but_037_Dryas_iulia_ott458065:(1.0) but_038_Ithomia_xenos_ott265497:(1.0) but_039_Greta_nero_ott3119044:(0.9999) but_040_Godyris_zavaleta_ott411073:(1.0) but_041_Greta_annette_ott221639:(0.9999) but_042_Hyposcada_virginiana_ott79749:(1.0) but_043_Paititia_neglecta_ott361142:(0.9999) \n",
      "\t\tProto:13 but_013_Heliconius_atthis_ott1024619:(0.9978) but_014_Heliconius_elevatus_ott1034618:(0.9932) but_015_Heliconius_ethilla_ott358146:(0.9999) but_016_Heliconius_numata_ott358148:(0.9996) but_017_Heliconius_ismenius_ott984238:(0.9993) but_018_Heliconius_melpomene_ott896444:(0.9998) but_019_Heliconius_timareta_ott145706:(0.9999) but_020_Heliconius_cydno_ott984236:(0.9998) but_021_Heliconius_pachinus_ott984241:(0.9999) but_022_Heliconius_wallacei_ott984240:(0.9943) but_023_Heliconius_hierax_ott372033:(0.9999) but_024_Heliconius_xanthocles_ott1034613:(0.9999) but_028_Heliconius_eleuchia_ott407976:(0.9981) but_029_Heliconius_sara_ott1034608:(0.9947) but_030_Heliconius_charithonia_ott259142:(1.0) but_032_Heliconius_clysonymus_ott1034615:(0.9225) but_033_Heliconius_telesiphe_ott984230:(0.9698) but_034_Eueides_isabella_ott1034607:(0.9999) but_035_Eueides_aliphera_ott1034606:(0.9999) but_036_Dryadula_phaetusa_ott896449:(0.9999) but_037_Dryas_iulia_ott458065:(0.9769) but_038_Ithomia_xenos_ott265497:(1.0) but_039_Greta_nero_ott3119044:(0.9954) but_040_Godyris_zavaleta_ott411073:(0.9999) but_041_Greta_annette_ott221639:(0.9903) but_042_Hyposcada_virginiana_ott79749:(0.9997) but_043_Paititia_neglecta_ott361142:(1.0) \n",
      "\t Child: 044+054\n",
      "\t\tProto:29 but_044_Catonephele_orites_ott773670:(0.9753) but_045_Eunica_pusilla_ott3116519:(0.9549) but_046_Eunica_marsolia_ott3116572:(0.9508) but_047_Temenis_laothoe_ott1079091:(0.999) but_048_Nessaea_obrinus_ott3116445:(0.9938) but_049_Batesia_hypochlora_ott451934:(0.9934) but_052_Pyrrhogyra_amphiro_ott3116287:(0.7716) but_053_Pyrrhogyra_otolais_ott461907:(0.9944) but_054_Tigridia_acesta_ott105886:(0.9117) but_055_Colobura_dirce_ott977975:(0.9979) \n",
      "\t\tProto:21 but_044_Catonephele_orites_ott773670:(0.9863) but_045_Eunica_pusilla_ott3116519:(0.9995) but_046_Eunica_marsolia_ott3116572:(0.999) but_047_Temenis_laothoe_ott1079091:(0.9936) but_048_Nessaea_obrinus_ott3116445:(0.73) but_049_Batesia_hypochlora_ott451934:(0.9295) but_052_Pyrrhogyra_amphiro_ott3116287:(0.9986) but_053_Pyrrhogyra_otolais_ott461907:(0.9972) but_054_Tigridia_acesta_ott105886:(0.9956) but_055_Colobura_dirce_ott977975:(0.9981) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 8/8 [00:00<00:00, 11.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 056+057\n",
      "\t Child: but_056_Rhetus_periander_ott3125491\n",
      "\t\tProto:5 but_056_Rhetus_periander_ott3125491:(1.0) \n",
      "\t Child: but_057_Lasaia_agesilas_ott627865\n",
      "\t\tProto:19 but_057_Lasaia_agesilas_ott627865:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 54it [00:01, 34.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 001+010\n",
      "\t Child: 001+003\n",
      "\t\tProto:8 but_001_Taygetis_cleopatra_ott3105132:(0.506) but_002_Taygetis_thamyra_ott483838:(0.9659) but_003_Morpho_menelaus_ott830884:(0.9982) but_004_Morpho_helenor_ott634337:(0.9932) but_005_Opsiphanes_invirae_ott1056460:(0.9871) but_006_Catoblepia_berecynthia_ott3104050:(0.9918) but_007_Caligo_idomeneus_ott401861:(0.9992) but_008_Caligo_eurilochus_ott572882:(0.9997) but_009_Bia_actorion_ott451938:(0.7919) \n",
      "\t\tProto:3 but_001_Taygetis_cleopatra_ott3105132:(0.9987) but_002_Taygetis_thamyra_ott483838:(0.9695) but_003_Morpho_menelaus_ott830884:(0.9954) but_004_Morpho_helenor_ott634337:(0.9956) but_005_Opsiphanes_invirae_ott1056460:(0.9891) but_006_Catoblepia_berecynthia_ott3104050:(0.9741) but_007_Caligo_idomeneus_ott401861:(0.9988) but_008_Caligo_eurilochus_ott572882:(0.9884) but_009_Bia_actorion_ott451938:(0.9929) \n",
      "\t\tProto:7 but_001_Taygetis_cleopatra_ott3105132:(0.9999) but_002_Taygetis_thamyra_ott483838:(0.9993) but_003_Morpho_menelaus_ott830884:(0.9576) but_004_Morpho_helenor_ott634337:(0.9998) but_005_Opsiphanes_invirae_ott1056460:(0.9968) but_006_Catoblepia_berecynthia_ott3104050:(0.9989) but_007_Caligo_idomeneus_ott401861:(0.999) but_008_Caligo_eurilochus_ott572882:(0.9997) but_009_Bia_actorion_ott451938:(0.8843) \n",
      "\t Child: 010+011\n",
      "\t\tProto:17 but_010_Hypna_clytemnestra_ott552641:(0.9859) but_011_Memphis_glauce_ott235310:(0.9993) but_012_Memphis_moruus_ott834399:(0.9799) \n",
      "\t\tProto:10 but_010_Hypna_clytemnestra_ott552641:(0.9987) but_011_Memphis_glauce_ott235310:(0.9999) but_012_Memphis_moruus_ott834399:(0.9997) \n",
      "\t\tProto:18 but_010_Hypna_clytemnestra_ott552641:(0.9572) but_011_Memphis_glauce_ott235310:(0.9904) but_012_Memphis_moruus_ott834399:(0.9599) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 111it [00:02, 40.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 029+038\n",
      "\t Child: 029+036\n",
      "\t\tProto:3 but_013_Heliconius_atthis_ott1024619:(0.7822) but_014_Heliconius_elevatus_ott1034618:(0.9982) but_015_Heliconius_ethilla_ott358146:(0.9948) but_016_Heliconius_numata_ott358148:(0.9731) but_017_Heliconius_ismenius_ott984238:(0.7746) but_018_Heliconius_melpomene_ott896444:(0.9951) but_019_Heliconius_timareta_ott145706:(0.9994) but_020_Heliconius_cydno_ott984236:(0.8663) but_021_Heliconius_pachinus_ott984241:(0.9902) but_022_Heliconius_wallacei_ott984240:(0.9985) but_023_Heliconius_hierax_ott372033:(0.9964) but_024_Heliconius_xanthocles_ott1034613:(0.9961) but_028_Heliconius_eleuchia_ott407976:(0.9534) but_029_Heliconius_sara_ott1034608:(0.9571) but_030_Heliconius_charithonia_ott259142:(0.9676) but_032_Heliconius_clysonymus_ott1034615:(0.8761) but_033_Heliconius_telesiphe_ott984230:(0.9865) but_034_Eueides_isabella_ott1034607:(0.9865) but_035_Eueides_aliphera_ott1034606:(0.6197) but_036_Dryadula_phaetusa_ott896449:(0.041) but_037_Dryas_iulia_ott458065:(0.5229) \n",
      "\t\tProto:4 but_013_Heliconius_atthis_ott1024619:(0.9997) but_014_Heliconius_elevatus_ott1034618:(0.9292) but_015_Heliconius_ethilla_ott358146:(0.9978) but_016_Heliconius_numata_ott358148:(0.9999) but_017_Heliconius_ismenius_ott984238:(0.9959) but_018_Heliconius_melpomene_ott896444:(0.9981) but_019_Heliconius_timareta_ott145706:(0.9988) but_020_Heliconius_cydno_ott984236:(0.982) but_021_Heliconius_pachinus_ott984241:(0.9879) but_022_Heliconius_wallacei_ott984240:(0.9997) but_023_Heliconius_hierax_ott372033:(0.9988) but_024_Heliconius_xanthocles_ott1034613:(0.9956) but_028_Heliconius_eleuchia_ott407976:(0.9984) but_029_Heliconius_sara_ott1034608:(0.995) but_030_Heliconius_charithonia_ott259142:(0.9865) but_032_Heliconius_clysonymus_ott1034615:(0.9947) but_033_Heliconius_telesiphe_ott984230:(0.9678) but_034_Eueides_isabella_ott1034607:(0.8572) but_035_Eueides_aliphera_ott1034606:(0.822) but_036_Dryadula_phaetusa_ott896449:(0.643) but_037_Dryas_iulia_ott458065:(0.9803) \n",
      "\t\tProto:5 but_013_Heliconius_atthis_ott1024619:(0.9992) but_014_Heliconius_elevatus_ott1034618:(0.635) but_015_Heliconius_ethilla_ott358146:(0.9823) but_016_Heliconius_numata_ott358148:(0.9979) but_017_Heliconius_ismenius_ott984238:(0.8621) but_018_Heliconius_melpomene_ott896444:(0.9967) but_019_Heliconius_timareta_ott145706:(0.9649) but_020_Heliconius_cydno_ott984236:(0.996) but_021_Heliconius_pachinus_ott984241:(0.9985) but_022_Heliconius_wallacei_ott984240:(0.9994) but_023_Heliconius_hierax_ott372033:(0.9995) but_024_Heliconius_xanthocles_ott1034613:(0.9923) but_028_Heliconius_eleuchia_ott407976:(0.9928) but_029_Heliconius_sara_ott1034608:(0.9957) but_030_Heliconius_charithonia_ott259142:(0.961) but_032_Heliconius_clysonymus_ott1034615:(0.9138) but_033_Heliconius_telesiphe_ott984230:(0.9989) but_034_Eueides_isabella_ott1034607:(0.7548) but_035_Eueides_aliphera_ott1034606:(0.7564) but_036_Dryadula_phaetusa_ott896449:(0.5444) but_037_Dryas_iulia_ott458065:(0.9882) \n",
      "\t\tProto:6 but_013_Heliconius_atthis_ott1024619:(0.8906) but_014_Heliconius_elevatus_ott1034618:(0.9965) but_015_Heliconius_ethilla_ott358146:(0.7452) but_016_Heliconius_numata_ott358148:(0.9987) but_017_Heliconius_ismenius_ott984238:(0.4624) but_018_Heliconius_melpomene_ott896444:(0.9641) but_019_Heliconius_timareta_ott145706:(0.9888) but_020_Heliconius_cydno_ott984236:(0.77) but_021_Heliconius_pachinus_ott984241:(0.9664) but_022_Heliconius_wallacei_ott984240:(0.9764) but_023_Heliconius_hierax_ott372033:(0.9811) but_024_Heliconius_xanthocles_ott1034613:(0.9933) but_028_Heliconius_eleuchia_ott407976:(0.9612) but_029_Heliconius_sara_ott1034608:(0.9991) but_030_Heliconius_charithonia_ott259142:(0.874) but_032_Heliconius_clysonymus_ott1034615:(0.7204) but_033_Heliconius_telesiphe_ott984230:(0.9981) but_034_Eueides_isabella_ott1034607:(0.6168) but_035_Eueides_aliphera_ott1034606:(0.6414) but_036_Dryadula_phaetusa_ott896449:(0.3658) but_037_Dryas_iulia_ott458065:(0.922) \n",
      "\t\tProto:7 but_013_Heliconius_atthis_ott1024619:(0.9965) but_014_Heliconius_elevatus_ott1034618:(0.9282) but_015_Heliconius_ethilla_ott358146:(0.7975) but_016_Heliconius_numata_ott358148:(0.9424) but_017_Heliconius_ismenius_ott984238:(0.9902) but_018_Heliconius_melpomene_ott896444:(0.9774) but_019_Heliconius_timareta_ott145706:(0.9637) but_020_Heliconius_cydno_ott984236:(0.9571) but_021_Heliconius_pachinus_ott984241:(0.5173) but_022_Heliconius_wallacei_ott984240:(0.9968) but_023_Heliconius_hierax_ott372033:(0.986) but_024_Heliconius_xanthocles_ott1034613:(0.9652) but_028_Heliconius_eleuchia_ott407976:(0.9519) but_029_Heliconius_sara_ott1034608:(0.9846) but_030_Heliconius_charithonia_ott259142:(0.8711) but_032_Heliconius_clysonymus_ott1034615:(0.9027) but_033_Heliconius_telesiphe_ott984230:(0.3979) but_034_Eueides_isabella_ott1034607:(0.7879) but_035_Eueides_aliphera_ott1034606:(0.4223) but_036_Dryadula_phaetusa_ott896449:(0.2259) but_037_Dryas_iulia_ott458065:(0.7059) \n",
      "\t\tProto:8 but_013_Heliconius_atthis_ott1024619:(0.9978) but_014_Heliconius_elevatus_ott1034618:(0.9984) but_015_Heliconius_ethilla_ott358146:(0.9968) but_016_Heliconius_numata_ott358148:(1.0) but_017_Heliconius_ismenius_ott984238:(0.9994) but_018_Heliconius_melpomene_ott896444:(0.9999) but_019_Heliconius_timareta_ott145706:(0.9879) but_020_Heliconius_cydno_ott984236:(0.9987) but_021_Heliconius_pachinus_ott984241:(0.9987) but_022_Heliconius_wallacei_ott984240:(0.9917) but_023_Heliconius_hierax_ott372033:(0.9924) but_024_Heliconius_xanthocles_ott1034613:(0.9173) but_028_Heliconius_eleuchia_ott407976:(0.9996) but_029_Heliconius_sara_ott1034608:(0.9983) but_030_Heliconius_charithonia_ott259142:(0.9519) but_032_Heliconius_clysonymus_ott1034615:(0.9963) but_033_Heliconius_telesiphe_ott984230:(0.9908) but_034_Eueides_isabella_ott1034607:(0.939) but_035_Eueides_aliphera_ott1034606:(0.9874) but_036_Dryadula_phaetusa_ott896449:(0.5328) but_037_Dryas_iulia_ott458065:(0.9928) \n",
      "\t\tProto:9 but_013_Heliconius_atthis_ott1024619:(0.9191) but_014_Heliconius_elevatus_ott1034618:(0.7937) but_015_Heliconius_ethilla_ott358146:(0.8551) but_016_Heliconius_numata_ott358148:(0.9926) but_017_Heliconius_ismenius_ott984238:(0.9019) but_018_Heliconius_melpomene_ott896444:(0.956) but_019_Heliconius_timareta_ott145706:(0.96) but_020_Heliconius_cydno_ott984236:(0.8528) but_021_Heliconius_pachinus_ott984241:(0.8156) but_022_Heliconius_wallacei_ott984240:(0.9248) but_023_Heliconius_hierax_ott372033:(0.9867) but_024_Heliconius_xanthocles_ott1034613:(0.9651) but_028_Heliconius_eleuchia_ott407976:(0.5947) but_029_Heliconius_sara_ott1034608:(0.8801) but_030_Heliconius_charithonia_ott259142:(0.7734) but_032_Heliconius_clysonymus_ott1034615:(0.8084) but_033_Heliconius_telesiphe_ott984230:(0.863) but_034_Eueides_isabella_ott1034607:(0.968) but_035_Eueides_aliphera_ott1034606:(0.9414) but_036_Dryadula_phaetusa_ott896449:(0.495) but_037_Dryas_iulia_ott458065:(0.9381) \n",
      "\t Child: 038+043\n",
      "\t\tProto:16 but_038_Ithomia_xenos_ott265497:(0.9993) but_039_Greta_nero_ott3119044:(0.9983) but_040_Godyris_zavaleta_ott411073:(0.9919) but_041_Greta_annette_ott221639:(0.9962) but_042_Hyposcada_virginiana_ott79749:(0.9904) but_043_Paititia_neglecta_ott361142:(0.74) \n",
      "\t\tProto:18 but_038_Ithomia_xenos_ott265497:(0.9999) but_039_Greta_nero_ott3119044:(1.0) but_040_Godyris_zavaleta_ott411073:(0.9999) but_041_Greta_annette_ott221639:(0.9999) but_042_Hyposcada_virginiana_ott79749:(0.9969) but_043_Paititia_neglecta_ott361142:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 43it [00:01, 31.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 044+054\n",
      "\t Child: 044+045+052\n",
      "\t\tProto:0 but_044_Catonephele_orites_ott773670:(0.9989) but_045_Eunica_pusilla_ott3116519:(0.8618) but_046_Eunica_marsolia_ott3116572:(0.9487) but_047_Temenis_laothoe_ott1079091:(0.9997) but_048_Nessaea_obrinus_ott3116445:(0.9989) but_049_Batesia_hypochlora_ott451934:(0.9996) but_052_Pyrrhogyra_amphiro_ott3116287:(0.993) but_053_Pyrrhogyra_otolais_ott461907:(0.9976) \n",
      "\t\tProto:4 but_044_Catonephele_orites_ott773670:(0.9997) but_045_Eunica_pusilla_ott3116519:(0.9996) but_046_Eunica_marsolia_ott3116572:(0.9752) but_047_Temenis_laothoe_ott1079091:(0.9983) but_048_Nessaea_obrinus_ott3116445:(0.9999) but_049_Batesia_hypochlora_ott451934:(0.9998) but_052_Pyrrhogyra_amphiro_ott3116287:(0.9959) but_053_Pyrrhogyra_otolais_ott461907:(0.997) \n",
      "\t Child: 054+055\n",
      "\t\tProto:11 but_054_Tigridia_acesta_ott105886:(1.0) but_055_Colobura_dirce_ott977975:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 41it [00:01, 30.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 001+003\n",
      "\t Child: 001+002\n",
      "\t\tProto:0 but_001_Taygetis_cleopatra_ott3105132:(0.9999) but_002_Taygetis_thamyra_ott483838:(0.9999) \n",
      "\t\tProto:4 but_001_Taygetis_cleopatra_ott3105132:(0.9978) but_002_Taygetis_thamyra_ott483838:(0.9999) \n",
      "\t\tProto:7 but_001_Taygetis_cleopatra_ott3105132:(1.0) but_002_Taygetis_thamyra_ott483838:(0.9996) \n",
      "\t Child: 003+005\n",
      "\t\tProto:18 but_003_Morpho_menelaus_ott830884:(1.0) but_004_Morpho_helenor_ott634337:(1.0) but_005_Opsiphanes_invirae_ott1056460:(0.9996) but_006_Catoblepia_berecynthia_ott3104050:(0.9992) but_007_Caligo_idomeneus_ott401861:(1.0) but_008_Caligo_eurilochus_ott572882:(1.0) but_009_Bia_actorion_ott451938:(0.6667) \n",
      "\t\tProto:19 but_003_Morpho_menelaus_ott830884:(0.9994) but_004_Morpho_helenor_ott634337:(0.958) but_005_Opsiphanes_invirae_ott1056460:(0.9084) but_006_Catoblepia_berecynthia_ott3104050:(0.9959) but_007_Caligo_idomeneus_ott401861:(0.999) but_008_Caligo_eurilochus_ott572882:(0.9988) but_009_Bia_actorion_ott451938:(0.7281) \n",
      "\t\tProto:14 but_003_Morpho_menelaus_ott830884:(1.0) but_004_Morpho_helenor_ott634337:(1.0) but_005_Opsiphanes_invirae_ott1056460:(0.8972) but_006_Catoblepia_berecynthia_ott3104050:(0.9985) but_007_Caligo_idomeneus_ott401861:(1.0) but_008_Caligo_eurilochus_ott572882:(0.9999) but_009_Bia_actorion_ott451938:(0.7223) \n",
      "\t\tProto:15 but_003_Morpho_menelaus_ott830884:(0.9963) but_004_Morpho_helenor_ott634337:(0.9998) but_005_Opsiphanes_invirae_ott1056460:(0.999) but_006_Catoblepia_berecynthia_ott3104050:(0.5963) but_007_Caligo_idomeneus_ott401861:(0.9898) but_008_Caligo_eurilochus_ott572882:(1.0) but_009_Bia_actorion_ott451938:(0.4757) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 13it [00:00, 15.54it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 010+011\n",
      "\t Child: but_010_Hypna_clytemnestra_ott552641\n",
      "\t\tProto:1 but_010_Hypna_clytemnestra_ott552641:(1.0) \n",
      "\t Child: 011+012\n",
      "\t\tProto:12 but_011_Memphis_glauce_ott235310:(1.0) but_012_Memphis_moruus_ott834399:(1.0) \n",
      "\t\tProto:13 but_011_Memphis_glauce_ott235310:(0.9999) but_012_Memphis_moruus_ott834399:(0.9998) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 89it [00:02, 38.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 029+036\n",
      "\t Child: 029+034\n",
      "\t\tProto:9 but_013_Heliconius_atthis_ott1024619:(0.9999) but_014_Heliconius_elevatus_ott1034618:(0.9999) but_015_Heliconius_ethilla_ott358146:(0.9999) but_016_Heliconius_numata_ott358148:(1.0) but_017_Heliconius_ismenius_ott984238:(0.9907) but_018_Heliconius_melpomene_ott896444:(1.0) but_019_Heliconius_timareta_ott145706:(1.0) but_020_Heliconius_cydno_ott984236:(0.9989) but_021_Heliconius_pachinus_ott984241:(0.9999) but_022_Heliconius_wallacei_ott984240:(1.0) but_023_Heliconius_hierax_ott372033:(0.9999) but_024_Heliconius_xanthocles_ott1034613:(1.0) but_028_Heliconius_eleuchia_ott407976:(0.9999) but_029_Heliconius_sara_ott1034608:(0.9998) but_030_Heliconius_charithonia_ott259142:(0.9997) but_032_Heliconius_clysonymus_ott1034615:(0.9963) but_033_Heliconius_telesiphe_ott984230:(0.9999) but_034_Eueides_isabella_ott1034607:(0.9996) but_035_Eueides_aliphera_ott1034606:(0.7365) \n",
      "\t\tProto:4 but_013_Heliconius_atthis_ott1024619:(1.0) but_014_Heliconius_elevatus_ott1034618:(0.9607) but_015_Heliconius_ethilla_ott358146:(0.9967) but_016_Heliconius_numata_ott358148:(0.9971) but_017_Heliconius_ismenius_ott984238:(0.9999) but_018_Heliconius_melpomene_ott896444:(1.0) but_019_Heliconius_timareta_ott145706:(0.9999) but_020_Heliconius_cydno_ott984236:(0.9998) but_021_Heliconius_pachinus_ott984241:(0.9996) but_022_Heliconius_wallacei_ott984240:(1.0) but_023_Heliconius_hierax_ott372033:(0.9976) but_024_Heliconius_xanthocles_ott1034613:(0.9949) but_028_Heliconius_eleuchia_ott407976:(0.9999) but_029_Heliconius_sara_ott1034608:(1.0) but_030_Heliconius_charithonia_ott259142:(0.9753) but_032_Heliconius_clysonymus_ott1034615:(0.996) but_033_Heliconius_telesiphe_ott984230:(0.9822) but_034_Eueides_isabella_ott1034607:(0.9886) but_035_Eueides_aliphera_ott1034606:(0.191) \n",
      "\t\tProto:6 but_013_Heliconius_atthis_ott1024619:(0.9963) but_014_Heliconius_elevatus_ott1034618:(0.9662) but_015_Heliconius_ethilla_ott358146:(0.9962) but_016_Heliconius_numata_ott358148:(0.9987) but_017_Heliconius_ismenius_ott984238:(0.9869) but_018_Heliconius_melpomene_ott896444:(0.9935) but_019_Heliconius_timareta_ott145706:(0.9998) but_020_Heliconius_cydno_ott984236:(0.9983) but_021_Heliconius_pachinus_ott984241:(1.0) but_022_Heliconius_wallacei_ott984240:(0.9913) but_023_Heliconius_hierax_ott372033:(0.9998) but_024_Heliconius_xanthocles_ott1034613:(0.9938) but_028_Heliconius_eleuchia_ott407976:(0.9998) but_029_Heliconius_sara_ott1034608:(0.9989) but_030_Heliconius_charithonia_ott259142:(0.9987) but_032_Heliconius_clysonymus_ott1034615:(0.9994) but_033_Heliconius_telesiphe_ott984230:(0.9987) but_034_Eueides_isabella_ott1034607:(0.9484) but_035_Eueides_aliphera_ott1034606:(0.5497) \n",
      "\t\tProto:7 but_013_Heliconius_atthis_ott1024619:(0.997) but_014_Heliconius_elevatus_ott1034618:(0.9996) but_015_Heliconius_ethilla_ott358146:(0.9999) but_016_Heliconius_numata_ott358148:(1.0) but_017_Heliconius_ismenius_ott984238:(0.9982) but_018_Heliconius_melpomene_ott896444:(1.0) but_019_Heliconius_timareta_ott145706:(0.9998) but_020_Heliconius_cydno_ott984236:(0.9979) but_021_Heliconius_pachinus_ott984241:(0.9999) but_022_Heliconius_wallacei_ott984240:(0.9999) but_023_Heliconius_hierax_ott372033:(0.9998) but_024_Heliconius_xanthocles_ott1034613:(0.9999) but_028_Heliconius_eleuchia_ott407976:(0.9997) but_029_Heliconius_sara_ott1034608:(0.9999) but_030_Heliconius_charithonia_ott259142:(0.9994) but_032_Heliconius_clysonymus_ott1034615:(0.9992) but_033_Heliconius_telesiphe_ott984230:(0.9997) but_034_Eueides_isabella_ott1034607:(0.9989) but_035_Eueides_aliphera_ott1034606:(0.3627) \n",
      "\t Child: 036+037\n",
      "\t\tProto:18 but_036_Dryadula_phaetusa_ott896449:(0.1338) but_037_Dryas_iulia_ott458065:(0.1664) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 22it [00:00, 22.68it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 038+043\n",
      "\t Child: 038+042\n",
      "\t\tProto:9 but_038_Ithomia_xenos_ott265497:(1.0) but_039_Greta_nero_ott3119044:(1.0) but_040_Godyris_zavaleta_ott411073:(1.0) but_041_Greta_annette_ott221639:(1.0) but_042_Hyposcada_virginiana_ott79749:(1.0) \n",
      "\t\tProto:3 but_038_Ithomia_xenos_ott265497:(0.9995) but_039_Greta_nero_ott3119044:(0.9973) but_040_Godyris_zavaleta_ott411073:(0.9999) but_041_Greta_annette_ott221639:(0.9274) but_042_Hyposcada_virginiana_ott79749:(0.9994) \n",
      "\t Child: but_043_Paititia_neglecta_ott361142\n",
      "\t\tProto:11 but_043_Paititia_neglecta_ott361142:(1.0) \n",
      "\t\tProto:12 but_043_Paititia_neglecta_ott361142:(1.0) \n",
      "\t\tProto:14 but_043_Paititia_neglecta_ott361142:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 35it [00:01, 27.98it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 044+045+052\n",
      "\t Child: but_044_Catonephele_orites_ott773670\n",
      "\t\tProto:7 but_044_Catonephele_orites_ott773670:(1.0) \n",
      "\t Child: 045+047\n",
      "\t\tProto:16 but_045_Eunica_pusilla_ott3116519:(0.9987) but_046_Eunica_marsolia_ott3116572:(0.9619) but_047_Temenis_laothoe_ott1079091:(0.9975) but_048_Nessaea_obrinus_ott3116445:(0.9985) but_049_Batesia_hypochlora_ott451934:(0.9981) \n",
      "\t\tProto:10 but_045_Eunica_pusilla_ott3116519:(0.9009) but_046_Eunica_marsolia_ott3116572:(0.5606) but_047_Temenis_laothoe_ott1079091:(0.9684) but_048_Nessaea_obrinus_ott3116445:(0.9605) but_049_Batesia_hypochlora_ott451934:(0.9142) \n",
      "\t\tProto:11 but_045_Eunica_pusilla_ott3116519:(0.9943) but_046_Eunica_marsolia_ott3116572:(0.9766) but_047_Temenis_laothoe_ott1079091:(0.9921) but_048_Nessaea_obrinus_ott3116445:(0.9948) but_049_Batesia_hypochlora_ott451934:(0.9865) \n",
      "\t\tProto:13 but_045_Eunica_pusilla_ott3116519:(0.9535) but_046_Eunica_marsolia_ott3116572:(0.9257) but_047_Temenis_laothoe_ott1079091:(0.9971) but_048_Nessaea_obrinus_ott3116445:(1.0) but_049_Batesia_hypochlora_ott451934:(0.9998) \n",
      "\t Child: 052+053\n",
      "\t\tProto:25 but_052_Pyrrhogyra_amphiro_ott3116287:(0.9999) but_053_Pyrrhogyra_otolais_ott461907:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 8/8 [00:00<00:00, 11.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 054+055\n",
      "\t Child: but_054_Tigridia_acesta_ott105886\n",
      "\t\tProto:8 but_054_Tigridia_acesta_ott105886:(1.0) \n",
      "\t Child: but_055_Colobura_dirce_ott977975\n",
      "\t\tProto:17 but_055_Colobura_dirce_ott977975:(0.9989) \n",
      "\t\tProto:10 but_055_Colobura_dirce_ott977975:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 9/9 [00:00<00:00, 12.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 001+002\n",
      "\t Child: but_001_Taygetis_cleopatra_ott3105132\n",
      "\t\tProto:3 but_001_Taygetis_cleopatra_ott3105132:(0.9998) \n",
      "\t\tProto:6 but_001_Taygetis_cleopatra_ott3105132:(0.9991) \n",
      "\t\tProto:7 but_001_Taygetis_cleopatra_ott3105132:(1.0) \n",
      "\t Child: but_002_Taygetis_thamyra_ott483838\n",
      "\t\tProto:14 but_002_Taygetis_thamyra_ott483838:(0.9565) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 32it [00:01, 26.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 003+005\n",
      "\t Child: 003+004\n",
      "\t\tProto:9 but_003_Morpho_menelaus_ott830884:(0.9997) but_004_Morpho_helenor_ott634337:(0.9991) \n",
      "\t\tProto:2 but_003_Morpho_menelaus_ott830884:(0.9997) but_004_Morpho_helenor_ott634337:(0.9991) \n",
      "\t\tProto:3 but_003_Morpho_menelaus_ott830884:(0.9994) but_004_Morpho_helenor_ott634337:(1.0) \n",
      "\t Child: 005+009\n",
      "\t\tProto:18 but_005_Opsiphanes_invirae_ott1056460:(0.9994) but_006_Catoblepia_berecynthia_ott3104050:(0.9995) but_007_Caligo_idomeneus_ott401861:(0.999) but_008_Caligo_eurilochus_ott572882:(0.9866) but_009_Bia_actorion_ott451938:(0.9999) \n",
      "\t\tProto:14 but_005_Opsiphanes_invirae_ott1056460:(1.0) but_006_Catoblepia_berecynthia_ott3104050:(1.0) but_007_Caligo_idomeneus_ott401861:(0.9997) but_008_Caligo_eurilochus_ott572882:(0.9972) but_009_Bia_actorion_ott451938:(1.0) \n",
      "\t\tProto:15 but_005_Opsiphanes_invirae_ott1056460:(0.9999) but_006_Catoblepia_berecynthia_ott3104050:(1.0) but_007_Caligo_idomeneus_ott401861:(0.9994) but_008_Caligo_eurilochus_ott572882:(0.9916) but_009_Bia_actorion_ott451938:(0.9999) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 9/9 [00:00<00:00, 12.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 011+012\n",
      "\t Child: but_011_Memphis_glauce_ott235310\n",
      "\t\tProto:1 but_011_Memphis_glauce_ott235310:(1.0) \n",
      "\t\tProto:2 but_011_Memphis_glauce_ott235310:(1.0) \n",
      "\t Child: but_012_Memphis_moruus_ott834399\n",
      "\t\tProto:19 but_012_Memphis_moruus_ott834399:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 83it [00:02, 39.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 029+034\n",
      "\t Child: 029+013\n",
      "\t\tProto:1 but_013_Heliconius_atthis_ott1024619:(1.0) but_014_Heliconius_elevatus_ott1034618:(0.9973) but_015_Heliconius_ethilla_ott358146:(0.9999) but_016_Heliconius_numata_ott358148:(1.0) but_017_Heliconius_ismenius_ott984238:(1.0) but_018_Heliconius_melpomene_ott896444:(1.0) but_019_Heliconius_timareta_ott145706:(0.9999) but_020_Heliconius_cydno_ott984236:(1.0) but_021_Heliconius_pachinus_ott984241:(1.0) but_022_Heliconius_wallacei_ott984240:(0.9999) but_023_Heliconius_hierax_ott372033:(0.9998) but_024_Heliconius_xanthocles_ott1034613:(0.9998) but_028_Heliconius_eleuchia_ott407976:(1.0) but_029_Heliconius_sara_ott1034608:(0.9999) but_030_Heliconius_charithonia_ott259142:(0.9993) but_032_Heliconius_clysonymus_ott1034615:(0.9991) but_033_Heliconius_telesiphe_ott984230:(0.9928) \n",
      "\t\tProto:9 but_013_Heliconius_atthis_ott1024619:(0.9999) but_014_Heliconius_elevatus_ott1034618:(1.0) but_015_Heliconius_ethilla_ott358146:(0.9737) but_016_Heliconius_numata_ott358148:(1.0) but_017_Heliconius_ismenius_ott984238:(1.0) but_018_Heliconius_melpomene_ott896444:(1.0) but_019_Heliconius_timareta_ott145706:(0.9939) but_020_Heliconius_cydno_ott984236:(0.9999) but_021_Heliconius_pachinus_ott984241:(0.9932) but_022_Heliconius_wallacei_ott984240:(0.9999) but_023_Heliconius_hierax_ott372033:(1.0) but_024_Heliconius_xanthocles_ott1034613:(0.9996) but_028_Heliconius_eleuchia_ott407976:(1.0) but_029_Heliconius_sara_ott1034608:(0.9997) but_030_Heliconius_charithonia_ott259142:(0.9821) but_032_Heliconius_clysonymus_ott1034615:(0.9996) but_033_Heliconius_telesiphe_ott984230:(0.9992) \n",
      "\t Child: 034+035\n",
      "\t\tProto:16 but_034_Eueides_isabella_ott1034607:(0.9884) but_035_Eueides_aliphera_ott1034606:(0.9984) \n",
      "\t\tProto:10 but_034_Eueides_isabella_ott1034607:(0.1245) but_035_Eueides_aliphera_ott1034606:(0.9143) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 6/6 [00:00<00:00,  8.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 036+037\n",
      "\t Child: but_036_Dryadula_phaetusa_ott896449\n",
      "\t\tProto:1 but_036_Dryadula_phaetusa_ott896449:(0.9997) \n",
      "\t Child: but_037_Dryas_iulia_ott458065\n",
      "\t\tProto:14 but_037_Dryas_iulia_ott458065:(1.0) \n",
      "\t\tProto:15 but_037_Dryas_iulia_ott458065:(0.999) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 18it [00:00, 19.37it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 038+042\n",
      "\t Child: 038+039\n",
      "\t\tProto:1 but_038_Ithomia_xenos_ott265497:(0.9999) but_039_Greta_nero_ott3119044:(0.9999) but_040_Godyris_zavaleta_ott411073:(0.9999) but_041_Greta_annette_ott221639:(0.9997) \n",
      "\t\tProto:4 but_038_Ithomia_xenos_ott265497:(1.0) but_039_Greta_nero_ott3119044:(0.9979) but_040_Godyris_zavaleta_ott411073:(0.9999) but_041_Greta_annette_ott221639:(0.9992) \n",
      "\t\tProto:5 but_038_Ithomia_xenos_ott265497:(0.9997) but_039_Greta_nero_ott3119044:(0.9999) but_040_Godyris_zavaleta_ott411073:(1.0) but_041_Greta_annette_ott221639:(0.9999) \n",
      "\t Child: but_042_Hyposcada_virginiana_ott79749\n",
      "\t\tProto:19 but_042_Hyposcada_virginiana_ott79749:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 24it [00:01, 23.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 045+047\n",
      "\t Child: 045+046\n",
      "\t\tProto:1 but_045_Eunica_pusilla_ott3116519:(1.0) but_046_Eunica_marsolia_ott3116572:(0.9997) \n",
      "\t\tProto:9 but_045_Eunica_pusilla_ott3116519:(1.0) but_046_Eunica_marsolia_ott3116572:(0.9998) \n",
      "\t Child: 047+049\n",
      "\t\tProto:17 but_047_Temenis_laothoe_ott1079091:(0.9999) but_048_Nessaea_obrinus_ott3116445:(1.0) but_049_Batesia_hypochlora_ott451934:(1.0) \n",
      "\t\tProto:10 but_047_Temenis_laothoe_ott1079091:(1.0) but_048_Nessaea_obrinus_ott3116445:(1.0) but_049_Batesia_hypochlora_ott451934:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 7/7 [00:00<00:00,  9.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 052+053\n",
      "\t Child: but_052_Pyrrhogyra_amphiro_ott3116287\n",
      "\t\tProto:2 but_052_Pyrrhogyra_amphiro_ott3116287:(1.0) \n",
      "\t\tProto:7 but_052_Pyrrhogyra_amphiro_ott3116287:(1.0) \n",
      "\t Child: but_053_Pyrrhogyra_otolais_ott461907\n",
      "\t\tProto:15 but_053_Pyrrhogyra_otolais_ott461907:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 10/10 [00:00<00:00, 13.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 003+004\n",
      "\t Child: but_003_Morpho_menelaus_ott830884\n",
      "\t\tProto:2 but_003_Morpho_menelaus_ott830884:(0.9999) \n",
      "\t Child: but_004_Morpho_helenor_ott634337\n",
      "\t\tProto:10 but_004_Morpho_helenor_ott634337:(0.9635) \n",
      "\t\tProto:12 but_004_Morpho_helenor_ott634337:(0.9991) \n",
      "\t\tProto:13 but_004_Morpho_helenor_ott634337:(0.9985) \n",
      "\t\tProto:15 but_004_Morpho_helenor_ott634337:(0.9967) \n",
      "\t\tProto:17 but_004_Morpho_helenor_ott634337:(0.9993) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 22it [00:00, 23.01it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 005+009\n",
      "\t Child: 005+007\n",
      "\t\tProto:1 but_005_Opsiphanes_invirae_ott1056460:(0.9979) but_006_Catoblepia_berecynthia_ott3104050:(0.9996) but_007_Caligo_idomeneus_ott401861:(1.0) but_008_Caligo_eurilochus_ott572882:(0.9997) \n",
      "\t\tProto:2 but_005_Opsiphanes_invirae_ott1056460:(1.0) but_006_Catoblepia_berecynthia_ott3104050:(0.9981) but_007_Caligo_idomeneus_ott401861:(1.0) but_008_Caligo_eurilochus_ott572882:(0.9995) \n",
      "\t Child: but_009_Bia_actorion_ott451938\n",
      "\t\tProto:12 but_009_Bia_actorion_ott451938:(0.9974) \n",
      "\t\tProto:13 but_009_Bia_actorion_ott451938:(0.9983) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 76it [00:01, 38.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 029+013\n",
      "\t Child: 029+032\n",
      "\t\tProto:8 but_028_Heliconius_eleuchia_ott407976:(0.9974) but_029_Heliconius_sara_ott1034608:(0.9999) but_030_Heliconius_charithonia_ott259142:(0.9644) but_032_Heliconius_clysonymus_ott1034615:(0.977) but_033_Heliconius_telesiphe_ott984230:(1.0) \n",
      "\t\tProto:1 but_028_Heliconius_eleuchia_ott407976:(0.9999) but_029_Heliconius_sara_ott1034608:(0.9998) but_030_Heliconius_charithonia_ott259142:(0.9392) but_032_Heliconius_clysonymus_ott1034615:(0.9892) but_033_Heliconius_telesiphe_ott984230:(0.9962) \n",
      "\t\tProto:4 but_028_Heliconius_eleuchia_ott407976:(0.8496) but_029_Heliconius_sara_ott1034608:(0.9969) but_030_Heliconius_charithonia_ott259142:(0.6692) but_032_Heliconius_clysonymus_ott1034615:(0.6005) but_033_Heliconius_telesiphe_ott984230:(0.9951) \n",
      "\t Child: 013+023\n",
      "\t\tProto:17 but_013_Heliconius_atthis_ott1024619:(1.0) but_014_Heliconius_elevatus_ott1034618:(0.9921) but_015_Heliconius_ethilla_ott358146:(0.9901) but_016_Heliconius_numata_ott358148:(1.0) but_017_Heliconius_ismenius_ott984238:(1.0) but_018_Heliconius_melpomene_ott896444:(1.0) but_019_Heliconius_timareta_ott145706:(1.0) but_020_Heliconius_cydno_ott984236:(0.9988) but_021_Heliconius_pachinus_ott984241:(0.9736) but_022_Heliconius_wallacei_ott984240:(0.021) but_023_Heliconius_hierax_ott372033:(0.9996) but_024_Heliconius_xanthocles_ott1034613:(0.9976) \n",
      "\t\tProto:18 but_013_Heliconius_atthis_ott1024619:(0.9618) but_014_Heliconius_elevatus_ott1034618:(0.93) but_015_Heliconius_ethilla_ott358146:(0.9774) but_016_Heliconius_numata_ott358148:(0.9879) but_017_Heliconius_ismenius_ott984238:(0.9977) but_018_Heliconius_melpomene_ott896444:(0.9999) but_019_Heliconius_timareta_ott145706:(0.9703) but_020_Heliconius_cydno_ott984236:(0.7592) but_021_Heliconius_pachinus_ott984241:(0.7139) but_022_Heliconius_wallacei_ott984240:(0.9452) but_023_Heliconius_hierax_ott372033:(0.8694) but_024_Heliconius_xanthocles_ott1034613:(0.9997) \n",
      "\t\tProto:14 but_013_Heliconius_atthis_ott1024619:(0.9996) but_014_Heliconius_elevatus_ott1034618:(0.9976) but_015_Heliconius_ethilla_ott358146:(0.9975) but_016_Heliconius_numata_ott358148:(0.9999) but_017_Heliconius_ismenius_ott984238:(0.9881) but_018_Heliconius_melpomene_ott896444:(0.9998) but_019_Heliconius_timareta_ott145706:(0.9868) but_020_Heliconius_cydno_ott984236:(0.9774) but_021_Heliconius_pachinus_ott984241:(0.9997) but_022_Heliconius_wallacei_ott984240:(0.7115) but_023_Heliconius_hierax_ott372033:(0.9476) but_024_Heliconius_xanthocles_ott1034613:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 7/7 [00:00<00:00,  9.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 034+035\n",
      "\t Child: but_034_Eueides_isabella_ott1034607\n",
      "\t\tProto:1 but_034_Eueides_isabella_ott1034607:(0.9999) \n",
      "\t\tProto:9 but_034_Eueides_isabella_ott1034607:(0.9999) \n",
      "\t Child: but_035_Eueides_aliphera_ott1034606\n",
      "\t\tProto:16 but_035_Eueides_aliphera_ott1034606:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 15it [00:00, 18.10it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 038+039\n",
      "\t Child: but_038_Ithomia_xenos_ott265497\n",
      "\t\tProto:2 but_038_Ithomia_xenos_ott265497:(1.0) \n",
      "\t Child: 039+040\n",
      "\t\tProto:13 but_039_Greta_nero_ott3119044:(1.0) but_040_Godyris_zavaleta_ott411073:(1.0) but_041_Greta_annette_ott221639:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 8/8 [00:00<00:00, 10.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 045+046\n",
      "\t Child: but_045_Eunica_pusilla_ott3116519\n",
      "\t\tProto:0 but_045_Eunica_pusilla_ott3116519:(0.9998) \n",
      "\t\tProto:8 but_045_Eunica_pusilla_ott3116519:(1.0) \n",
      "\t\tProto:2 but_045_Eunica_pusilla_ott3116519:(1.0) \n",
      "\t\tProto:4 but_045_Eunica_pusilla_ott3116519:(0.9948) \n",
      "\t Child: but_046_Eunica_marsolia_ott3116572\n",
      "\t\tProto:13 but_046_Eunica_marsolia_ott3116572:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 16it [00:00, 18.34it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 047+049\n",
      "\t Child: 047+048\n",
      "\t\tProto:0 but_047_Temenis_laothoe_ott1079091:(1.0) but_048_Nessaea_obrinus_ott3116445:(0.9996) \n",
      "\t\tProto:7 but_047_Temenis_laothoe_ott1079091:(0.9999) but_048_Nessaea_obrinus_ott3116445:(1.0) \n",
      "\t Child: but_049_Batesia_hypochlora_ott451934\n",
      "\t\tProto:16 but_049_Batesia_hypochlora_ott451934:(1.0) \n",
      "\t\tProto:17 but_049_Batesia_hypochlora_ott451934:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 18it [00:00, 20.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 005+007\n",
      "\t Child: 005+006\n",
      "\t\tProto:9 but_005_Opsiphanes_invirae_ott1056460:(1.0) but_006_Catoblepia_berecynthia_ott3104050:(0.9999) \n",
      "\t\tProto:3 but_005_Opsiphanes_invirae_ott1056460:(1.0) but_006_Catoblepia_berecynthia_ott3104050:(1.0) \n",
      "\t Child: 007+008\n",
      "\t\tProto:13 but_007_Caligo_idomeneus_ott401861:(1.0) but_008_Caligo_eurilochus_ott572882:(0.9999) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 19it [00:00, 20.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 029+032\n",
      "\t Child: 029+030\n",
      "\t\tProto:0 but_028_Heliconius_eleuchia_ott407976:(1.0) but_029_Heliconius_sara_ott1034608:(1.0) but_030_Heliconius_charithonia_ott259142:(0.9998) \n",
      "\t\tProto:1 but_028_Heliconius_eleuchia_ott407976:(1.0) but_029_Heliconius_sara_ott1034608:(1.0) but_030_Heliconius_charithonia_ott259142:(1.0) \n",
      "\t\tProto:7 but_028_Heliconius_eleuchia_ott407976:(1.0) but_029_Heliconius_sara_ott1034608:(1.0) but_030_Heliconius_charithonia_ott259142:(0.9969) \n",
      "\t Child: 032+033\n",
      "\t\tProto:19 but_032_Heliconius_clysonymus_ott1034615:(1.0) but_033_Heliconius_telesiphe_ott984230:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 57it [00:01, 34.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 013+023\n",
      "\t Child: 013+022\n",
      "\t\tProto:3 but_013_Heliconius_atthis_ott1024619:(0.9979) but_014_Heliconius_elevatus_ott1034618:(0.6528) but_015_Heliconius_ethilla_ott358146:(0.9974) but_016_Heliconius_numata_ott358148:(0.9859) but_017_Heliconius_ismenius_ott984238:(0.9994) but_018_Heliconius_melpomene_ott896444:(0.9747) but_019_Heliconius_timareta_ott145706:(0.9357) but_020_Heliconius_cydno_ott984236:(0.9996) but_021_Heliconius_pachinus_ott984241:(0.9333) but_022_Heliconius_wallacei_ott984240:(0.9997) \n",
      "\t\tProto:4 but_013_Heliconius_atthis_ott1024619:(0.9965) but_014_Heliconius_elevatus_ott1034618:(0.6123) but_015_Heliconius_ethilla_ott358146:(0.9974) but_016_Heliconius_numata_ott358148:(0.9995) but_017_Heliconius_ismenius_ott984238:(0.9903) but_018_Heliconius_melpomene_ott896444:(0.9867) but_019_Heliconius_timareta_ott145706:(0.999) but_020_Heliconius_cydno_ott984236:(0.9978) but_021_Heliconius_pachinus_ott984241:(0.9271) but_022_Heliconius_wallacei_ott984240:(0.9464) \n",
      "\t\tProto:5 but_013_Heliconius_atthis_ott1024619:(0.9975) but_014_Heliconius_elevatus_ott1034618:(0.5769) but_015_Heliconius_ethilla_ott358146:(0.9752) but_016_Heliconius_numata_ott358148:(0.9761) but_017_Heliconius_ismenius_ott984238:(0.8314) but_018_Heliconius_melpomene_ott896444:(0.9998) but_019_Heliconius_timareta_ott145706:(0.9864) but_020_Heliconius_cydno_ott984236:(0.9985) but_021_Heliconius_pachinus_ott984241:(0.9823) but_022_Heliconius_wallacei_ott984240:(0.9964) \n",
      "\t\tProto:7 but_013_Heliconius_atthis_ott1024619:(0.9998) but_014_Heliconius_elevatus_ott1034618:(0.8066) but_015_Heliconius_ethilla_ott358146:(0.9947) but_016_Heliconius_numata_ott358148:(0.9998) but_017_Heliconius_ismenius_ott984238:(0.9998) but_018_Heliconius_melpomene_ott896444:(0.9406) but_019_Heliconius_timareta_ott145706:(0.9987) but_020_Heliconius_cydno_ott984236:(1.0) but_021_Heliconius_pachinus_ott984241:(0.8859) but_022_Heliconius_wallacei_ott984240:(0.9999) \n",
      "\t\tProto:8 but_013_Heliconius_atthis_ott1024619:(0.9996) but_014_Heliconius_elevatus_ott1034618:(0.5014) but_015_Heliconius_ethilla_ott358146:(0.9938) but_016_Heliconius_numata_ott358148:(0.9996) but_017_Heliconius_ismenius_ott984238:(0.999) but_018_Heliconius_melpomene_ott896444:(0.2843) but_019_Heliconius_timareta_ott145706:(0.9954) but_020_Heliconius_cydno_ott984236:(0.999) but_021_Heliconius_pachinus_ott984241:(0.9661) but_022_Heliconius_wallacei_ott984240:(0.999) \n",
      "\t Child: 023+024\n",
      "\t\tProto:11 but_023_Heliconius_hierax_ott372033:(0.9991) but_024_Heliconius_xanthocles_ott1034613:(0.9999) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 10it [00:00, 12.84it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 039+040\n",
      "\t Child: but_039_Greta_nero_ott3119044\n",
      "\t\tProto:1 but_039_Greta_nero_ott3119044:(1.0) \n",
      "\t Child: 040+041\n",
      "\t\tProto:19 but_040_Godyris_zavaleta_ott411073:(1.0) but_041_Greta_annette_ott221639:(0.5751) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 10/10 [00:00<00:00, 13.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 047+048\n",
      "\t Child: but_047_Temenis_laothoe_ott1079091\n",
      "\t\tProto:8 but_047_Temenis_laothoe_ott1079091:(0.9998) \n",
      "\t\tProto:5 but_047_Temenis_laothoe_ott1079091:(1.0) \n",
      "\t Child: but_048_Nessaea_obrinus_ott3116445\n",
      "\t\tProto:12 but_048_Nessaea_obrinus_ott3116445:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 9/9 [00:00<00:00, 12.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 005+006\n",
      "\t Child: but_005_Opsiphanes_invirae_ott1056460\n",
      "\t\tProto:5 but_005_Opsiphanes_invirae_ott1056460:(1.0) \n",
      "\t Child: but_006_Catoblepia_berecynthia_ott3104050\n",
      "\t\tProto:11 but_006_Catoblepia_berecynthia_ott3104050:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 9/9 [00:00<00:00, 12.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 007+008\n",
      "\t Child: but_007_Caligo_idomeneus_ott401861\n",
      "\t\tProto:0 but_007_Caligo_idomeneus_ott401861:(0.9831) \n",
      "\t\tProto:2 but_007_Caligo_idomeneus_ott401861:(0.9948) \n",
      "\t\tProto:5 but_007_Caligo_idomeneus_ott401861:(0.9996) \n",
      "\t Child: but_008_Caligo_eurilochus_ott572882\n",
      "\t\tProto:10 but_008_Caligo_eurilochus_ott572882:(0.9999) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 13it [00:00, 16.12it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 029+030\n",
      "\t Child: 029+028\n",
      "\t\tProto:0 but_028_Heliconius_eleuchia_ott407976:(0.9998) but_029_Heliconius_sara_ott1034608:(1.0) \n",
      "\t\tProto:9 but_028_Heliconius_eleuchia_ott407976:(1.0) but_029_Heliconius_sara_ott1034608:(1.0) \n",
      "\t\tProto:5 but_028_Heliconius_eleuchia_ott407976:(1.0) but_029_Heliconius_sara_ott1034608:(1.0) \n",
      "\t Child: but_030_Heliconius_charithonia_ott259142\n",
      "\t\tProto:15 but_030_Heliconius_charithonia_ott259142:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 6/6 [00:00<00:00,  8.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 032+033\n",
      "\t Child: but_032_Heliconius_clysonymus_ott1034615\n",
      "\t\tProto:5 but_032_Heliconius_clysonymus_ott1034615:(0.9996) \n",
      "\t Child: but_033_Heliconius_telesiphe_ott984230\n",
      "\t\tProto:16 but_033_Heliconius_telesiphe_ott984230:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 47it [00:01, 32.67it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 013+022\n",
      "\t Child: 013+018\n",
      "\t\tProto:9 but_013_Heliconius_atthis_ott1024619:(0.9994) but_014_Heliconius_elevatus_ott1034618:(0.9977) but_015_Heliconius_ethilla_ott358146:(0.9999) but_016_Heliconius_numata_ott358148:(1.0) but_017_Heliconius_ismenius_ott984238:(0.9995) but_018_Heliconius_melpomene_ott896444:(1.0) but_019_Heliconius_timareta_ott145706:(0.9728) but_020_Heliconius_cydno_ott984236:(0.9974) but_021_Heliconius_pachinus_ott984241:(0.9996) \n",
      "\t\tProto:4 but_013_Heliconius_atthis_ott1024619:(0.993) but_014_Heliconius_elevatus_ott1034618:(0.9989) but_015_Heliconius_ethilla_ott358146:(0.9993) but_016_Heliconius_numata_ott358148:(1.0) but_017_Heliconius_ismenius_ott984238:(0.9986) but_018_Heliconius_melpomene_ott896444:(0.9997) but_019_Heliconius_timareta_ott145706:(0.9998) but_020_Heliconius_cydno_ott984236:(0.9908) but_021_Heliconius_pachinus_ott984241:(0.9984) \n",
      "\t Child: but_022_Heliconius_wallacei_ott984240\n",
      "\t\tProto:16 but_022_Heliconius_wallacei_ott984240:(0.9987) \n",
      "\t\tProto:18 but_022_Heliconius_wallacei_ott984240:(0.9985) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 10/10 [00:00<00:00, 13.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 023+024\n",
      "\t Child: but_023_Heliconius_hierax_ott372033\n",
      "\t\tProto:9 but_023_Heliconius_hierax_ott372033:(1.0) \n",
      "\t Child: but_024_Heliconius_xanthocles_ott1034613\n",
      "\t\tProto:17 but_024_Heliconius_xanthocles_ott1034613:(1.0) \n",
      "\t\tProto:10 but_024_Heliconius_xanthocles_ott1034613:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 7/7 [00:00<00:00, 10.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 040+041\n",
      "\t Child: but_040_Godyris_zavaleta_ott411073\n",
      "\t\tProto:2 but_040_Godyris_zavaleta_ott411073:(1.0) \n",
      "\t Child: but_041_Greta_annette_ott221639\n",
      "\t\tProto:13 but_041_Greta_annette_ott221639:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 10/10 [00:00<00:00, 13.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 029+028\n",
      "\t Child: but_029_Heliconius_sara_ott1034608\n",
      "\t\tProto:9 but_029_Heliconius_sara_ott1034608:(1.0) \n",
      "\t\tProto:3 but_029_Heliconius_sara_ott1034608:(1.0) \n",
      "\t\tProto:4 but_029_Heliconius_sara_ott1034608:(1.0) \n",
      "\t\tProto:7 but_029_Heliconius_sara_ott1034608:(1.0) \n",
      "\t Child: but_028_Heliconius_eleuchia_ott407976\n",
      "\t\tProto:19 but_028_Heliconius_eleuchia_ott407976:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 42it [00:01, 30.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 013+018\n",
      "\t Child: 013+016\n",
      "\t\tProto:8 but_013_Heliconius_atthis_ott1024619:(1.0) but_014_Heliconius_elevatus_ott1034618:(0.9739) but_015_Heliconius_ethilla_ott358146:(0.9993) but_016_Heliconius_numata_ott358148:(1.0) but_017_Heliconius_ismenius_ott984238:(0.9998) \n",
      "\t\tProto:9 but_013_Heliconius_atthis_ott1024619:(0.9978) but_014_Heliconius_elevatus_ott1034618:(0.959) but_015_Heliconius_ethilla_ott358146:(0.9964) but_016_Heliconius_numata_ott358148:(0.9972) but_017_Heliconius_ismenius_ott984238:(0.9997) \n",
      "\t\tProto:3 but_013_Heliconius_atthis_ott1024619:(1.0) but_014_Heliconius_elevatus_ott1034618:(0.9509) but_015_Heliconius_ethilla_ott358146:(0.9998) but_016_Heliconius_numata_ott358148:(0.9989) but_017_Heliconius_ismenius_ott984238:(1.0) \n",
      "\t Child: 018+019\n",
      "\t\tProto:11 but_018_Heliconius_melpomene_ott896444:(0.997) but_019_Heliconius_timareta_ott145706:(0.9983) but_020_Heliconius_cydno_ott984236:(0.9983) but_021_Heliconius_pachinus_ott984241:(0.969) \n",
      "\t\tProto:12 but_018_Heliconius_melpomene_ott896444:(1.0) but_019_Heliconius_timareta_ott145706:(0.9993) but_020_Heliconius_cydno_ott984236:(1.0) but_021_Heliconius_pachinus_ott984241:(0.9952) \n",
      "\t\tProto:13 but_018_Heliconius_melpomene_ott896444:(0.9975) but_019_Heliconius_timareta_ott145706:(0.9327) but_020_Heliconius_cydno_ott984236:(0.9923) but_021_Heliconius_pachinus_ott984241:(0.9899) \n",
      "\t\tProto:15 but_018_Heliconius_melpomene_ott896444:(1.0) but_019_Heliconius_timareta_ott145706:(0.9996) but_020_Heliconius_cydno_ott984236:(0.9897) but_021_Heliconius_pachinus_ott984241:(0.9978) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 23it [00:01, 21.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 013+016\n",
      "\t Child: 013+015\n",
      "\t\tProto:4 but_013_Heliconius_atthis_ott1024619:(1.0) but_014_Heliconius_elevatus_ott1034618:(1.0) but_015_Heliconius_ethilla_ott358146:(1.0) \n",
      "\t Child: 016+017\n",
      "\t\tProto:16 but_016_Heliconius_numata_ott358148:(1.0) but_017_Heliconius_ismenius_ott984238:(0.9989) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 19it [00:00, 20.46it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 018+019\n",
      "\t Child: but_018_Heliconius_melpomene_ott896444\n",
      "\t\tProto:8 but_018_Heliconius_melpomene_ott896444:(0.9999) \n",
      "\t\tProto:1 but_018_Heliconius_melpomene_ott896444:(1.0) \n",
      "\t\tProto:6 but_018_Heliconius_melpomene_ott896444:(0.9936) \n",
      "\t Child: 019+020\n",
      "\t\tProto:19 but_019_Heliconius_timareta_ott145706:(0.9968) but_020_Heliconius_cydno_ott984236:(0.9976) but_021_Heliconius_pachinus_ott984241:(0.5849) \n",
      "\t\tProto:11 but_019_Heliconius_timareta_ott145706:(1.0) but_020_Heliconius_cydno_ott984236:(1.0) but_021_Heliconius_pachinus_ott984241:(0.9954) \n",
      "\t\tProto:13 but_019_Heliconius_timareta_ott145706:(0.9998) but_020_Heliconius_cydno_ott984236:(1.0) but_021_Heliconius_pachinus_ott984241:(1.0) \n",
      "\t\tProto:14 but_019_Heliconius_timareta_ott145706:(0.9994) but_020_Heliconius_cydno_ott984236:(0.9983) but_021_Heliconius_pachinus_ott984241:(0.9999) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 15it [00:00, 17.77it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 013+015\n",
      "\t Child: 013+014\n",
      "\t\tProto:9 but_013_Heliconius_atthis_ott1024619:(1.0) but_014_Heliconius_elevatus_ott1034618:(1.0) \n",
      "\t\tProto:3 but_013_Heliconius_atthis_ott1024619:(1.0) but_014_Heliconius_elevatus_ott1034618:(1.0) \n",
      "\t Child: but_015_Heliconius_ethilla_ott358146\n",
      "\t\tProto:18 but_015_Heliconius_ethilla_ott358146:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 8/8 [00:00<00:00, 11.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 016+017\n",
      "\t Child: but_016_Heliconius_numata_ott358148\n",
      "\t\tProto:9 but_016_Heliconius_numata_ott358148:(1.0) \n",
      "\t\tProto:3 but_016_Heliconius_numata_ott358148:(1.0) \n",
      "\t\tProto:6 but_016_Heliconius_numata_ott358148:(1.0) \n",
      "\t Child: but_017_Heliconius_ismenius_ott984238\n",
      "\t\tProto:18 but_017_Heliconius_ismenius_ott984238:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 13it [00:00, 15.87it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 019+020\n",
      "\t Child: but_019_Heliconius_timareta_ott145706\n",
      "\t\tProto:3 but_019_Heliconius_timareta_ott145706:(1.0) \n",
      "\t Child: 020+021\n",
      "\t\tProto:16 but_020_Heliconius_cydno_ott984236:(1.0) but_021_Heliconius_pachinus_ott984241:(0.9994) \n",
      "\t\tProto:11 but_020_Heliconius_cydno_ott984236:(1.0) but_021_Heliconius_pachinus_ott984241:(0.9998) \n",
      "\t\tProto:14 but_020_Heliconius_cydno_ott984236:(1.0) but_021_Heliconius_pachinus_ott984241:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 10/10 [00:00<00:00, 13.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 013+014\n",
      "\t Child: but_013_Heliconius_atthis_ott1024619\n",
      "\t\tProto:7 but_013_Heliconius_atthis_ott1024619:(1.0) \n",
      "\t Child: but_014_Heliconius_elevatus_ott1034618\n",
      "\t\tProto:11 but_014_Heliconius_elevatus_ott1034618:(1.0) \n",
      "\t\tProto:15 but_014_Heliconius_elevatus_ott1034618:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 8/8 [00:00<00:00, 10.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 020+021\n",
      "\t Child: but_020_Heliconius_cydno_ott984236\n",
      "\t\tProto:0 but_020_Heliconius_cydno_ott984236:(1.0) \n",
      "\t\tProto:2 but_020_Heliconius_cydno_ott984236:(1.0) \n",
      "\t\tProto:4 but_020_Heliconius_cydno_ott984236:(1.0) \n",
      "\t\tProto:7 but_020_Heliconius_cydno_ott984236:(1.0) \n",
      "\t\tProto:9 but_020_Heliconius_cydno_ott984236:(0.9999) \n",
      "\t Child: but_021_Heliconius_pachinus_ott984241\n",
      "\t\tProto:17 but_021_Heliconius_pachinus_ott984241:(1.0) \n",
      "Done !!!\n"
     ]
    }
   ],
   "source": [
    "# Proto activations on leaf descendents - topk images\n",
    "\n",
    "def get_heatmap(latent_activation, input_image, constant_color_scale=False):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    # image_a = (image_a - image_a.min()) / (image_a.max() - image_a.min())\n",
    "\n",
    "    # input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "    image_b = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((input_image.shape[-1], input_image.shape[-1])))\n",
    "\n",
    "    if constant_color_scale:\n",
    "        reshaped_image_a = np.concatenate((reshaped_image_a, np.zeros((reshaped_image_a.shape[1], 1)), np.ones((reshaped_image_a.shape[1], 1))*255), axis=1)\n",
    "    \n",
    "    normalized_heatmap = (reshaped_image_a - np.min(reshaped_image_a)) / (np.max(reshaped_image_a) - np.min(reshaped_image_a))\n",
    "\n",
    "    heatmap_colormap = plt.get_cmap('jet')\n",
    "    heatmap_colored = heatmap_colormap(normalized_heatmap)\n",
    "\n",
    "    if constant_color_scale:\n",
    "        heatmap_colored = heatmap_colored[:, :-2]\n",
    "    \n",
    "    heatmap_colored_uint8 = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_a_heatmap_pillow = Image.fromarray(heatmap_colored_uint8)\n",
    "    image_b_pillow = Image.fromarray((image_b * 255).astype('uint8'))\n",
    "    \n",
    "    result_image = Image.blend(image_b_pillow, image_a_heatmap_pillow, alpha=0.3)\n",
    "    \n",
    "    return np.array(result_image)\n",
    "\n",
    "def get_heatmap_uninterpolated(latent_activation, input_image):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    image_a = (image_a - image_a.min()) / (image_a.max() - image_a.min())\n",
    "\n",
    "    input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "    image_b = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((input_image.shape[-1], input_image.shape[-1]), \\\n",
    "                                                                                          resample=Image.NEAREST ))\n",
    "    normalized_heatmap = (reshaped_image_a - np.min(reshaped_image_a)) / (np.max(reshaped_image_a) - np.min(reshaped_image_a))\n",
    "    \n",
    "    heatmap_colormap = plt.get_cmap('jet')\n",
    "    heatmap_colored = heatmap_colormap(normalized_heatmap)\n",
    "    \n",
    "    heatmap_colored_uint8 = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_a_heatmap_pillow = Image.fromarray(heatmap_colored_uint8)\n",
    "    image_b_pillow = Image.fromarray((image_b * 255).astype('uint8'))\n",
    "    \n",
    "    result_image = Image.blend(image_b_pillow, image_a_heatmap_pillow, alpha=0.3)\n",
    "    \n",
    "    return np.array(result_image)\n",
    "\n",
    "from util.data import ModifiedLabelLoader\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import pdb\n",
    "from util.vis_pipnet import get_img_coordinates\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import ImageFont, Image, ImageDraw as D\n",
    "import torchvision\n",
    "from datetime import datetime\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import math\n",
    "# txt_file = open(os.path.join(run_path, \"num_proto_details_\"+datetime.now().strftime(\"%m:%d:%H:%M:%S\")+\".txt\"), \"a\")\n",
    "# txt_file.write('\\n')\n",
    "\n",
    "vizloader_name = 'testloader' # projectloader\n",
    "find_non_descendants = False # True, False # param\n",
    "topk = 2\n",
    "save_images = True # True, False\n",
    "font = ImageFont.truetype(\"arial.ttf\", 50)\n",
    "save_activation_as_npy_path = None # 'activation_as_npy'\n",
    "if (save_activation_as_npy_path is not None):\n",
    "    save_activation_as_npy_path = '_'.join(['activation_as_npy', vizloader_name])  # activation_as_npy, added for NUMPY SAVING\n",
    "if find_non_descendants and (save_activation_as_npy_path is not None):\n",
    "    save_activation_as_npy_path = save_activation_as_npy_path + '_non_desc'\n",
    "plot_overspecificity_score = True\n",
    "subtree_root = root.get_node('024+051')\n",
    "    \n",
    "from datetime import datetime\n",
    "# txt_file = open(os.path.join(run_path, \"num_proto_details_\"+datetime.now().strftime(\"%m:%d:%H:%M:%S\")+\".txt\"), \"a\")\n",
    "# txt_file.write('\\n')\n",
    "\n",
    "def write_num_proto_details(proto_mean_activations, node_name, net, threshold, txt_file, args):\n",
    "    \n",
    "    rand_input = torch.randn((1, 3, args.image_size, args.image_size))\n",
    "    with torch.no_grad():\n",
    "        *_, pooled, out = net(rand_input)\n",
    "    num_protos = pooled[node_name].shape[1]\n",
    "    used_protos = len(proto_mean_activations)\n",
    "    non_overspecific = 0\n",
    "    for p in proto_mean_activations:\n",
    "        logstr = '\\t'*2 + f'Proto:{p} '\n",
    "        protos_mean_for_all_leaf_descedants = []\n",
    "        for leaf_descendent in proto_mean_activations[p]:\n",
    "            mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "            protos_mean_for_all_leaf_descedants.append(mean_activation)\n",
    "            \n",
    "        if all([(mean_activation>0.2) for mean_activation in protos_mean_for_all_leaf_descedants]):\n",
    "            non_overspecific += 1\n",
    "            \n",
    "    txt_file.write(f\"Node:{node_name},Total:{num_protos},Used:{used_protos},Good:{non_overspecific},threshold={threshold}\\n\")\n",
    "\n",
    "\n",
    "def get_heap():\n",
    "    list_ = []\n",
    "    heapq.heapify(list_)\n",
    "    return list_\n",
    "\n",
    "patchsize, skip = get_patch_size(args)\n",
    "\n",
    "\n",
    "vizloader_dict = {'trainloader': trainloader,\n",
    "                 'projectloader': projectloader,\n",
    "                 'testloader': testloader,\n",
    "                 'test_projectloader': test_projectloader}\n",
    "vizloader_dict[vizloader_name] = unshuffle_dataloader(vizloader_dict[vizloader_name])\n",
    "\n",
    "\n",
    "if type(vizloader_dict[vizloader_name].dataset) == ImageFolder:\n",
    "    name2label = vizloader_dict[vizloader_name].dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "else:\n",
    "    name2label = vizloader_dict[vizloader_name].dataset.dataset.dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "    \n",
    "overspecificity_score_and_proto_mask = []\n",
    "\n",
    "for node in root.nodes_with_children():\n",
    "#     if node.name == 'root':\n",
    "#         continue\n",
    "#     non_leaf_children_names = [child.name for child in node.children if not child.is_leaf()]\n",
    "#     if len(non_leaf_children_names) == 0: # if all the children are leaf nodes then skip this node\n",
    "#         continue\n",
    "\n",
    "    if node.name not in subtree_root.descendents:\n",
    "        print('Skipping node', node.name)\n",
    "        continue\n",
    "\n",
    "    name2label = vizloader_dict[vizloader_name].dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "    modifiedLabelLoader = ModifiedLabelLoader(vizloader_dict[vizloader_name], node)\n",
    "    coarse_label2name = modifiedLabelLoader.modifiedlabel2name\n",
    "    node_label_to_children = {label: name for name, label in node.children_to_labels.items()}\n",
    "    \n",
    "    imgs = modifiedLabelLoader.filtered_imgs\n",
    "\n",
    "    img_iter = tqdm(enumerate(modifiedLabelLoader),\n",
    "                    total=len(modifiedLabelLoader),\n",
    "                    mininterval=50.,\n",
    "                    desc='Collecting topk',\n",
    "                    ncols=0)\n",
    "\n",
    "    classification_weights = getattr(net.module, '_'+node.name+'_classification').weight\n",
    "    \n",
    "    # maps proto_number -> grand_child_name (or descendant leaf name) -> list of top-k activations\n",
    "    proto_mean_activations = defaultdict(lambda: defaultdict(get_heap))\n",
    "\n",
    "    # maps class names to the prototypes that belong to that\n",
    "    class_and_prototypes = defaultdict(set)\n",
    "\n",
    "    for i, (xs, orig_y, ys) in img_iter:\n",
    "#         if coarse_label2name[ys.item()] not in non_leaf_children_names:\n",
    "#             continue\n",
    "\n",
    "        xs, ys = xs.to(device), ys.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output = net(xs, inference=False)\n",
    "            if len(model_output) == 3:\n",
    "                softmaxes, pooled, _ = model_output\n",
    "            elif len(model_output) == 4:\n",
    "                _, softmaxes, pooled, _ = model_output\n",
    "            pooled = pooled[node.name].squeeze(0) \n",
    "            softmaxes = softmaxes[node.name]#.squeeze(0)\n",
    "\n",
    "            for p in range(pooled.shape[0]): # pooled.shape -> [768] (== num of prototypes)\n",
    "                c_weight = torch.max(classification_weights[:,p]) # classification_weights[:,p].shape -> [200] (== num of classes)\n",
    "                relevant_proto_classes = torch.nonzero(classification_weights[:, p] > 1e-3)\n",
    "                relevant_proto_class_names = [node_label_to_children[class_idx.item()] for class_idx in relevant_proto_classes]\n",
    "                \n",
    "                # Take the max per prototype.                             \n",
    "                max_per_prototype, max_idx_per_prototype = torch.max(softmaxes, dim=0)\n",
    "                max_per_prototype_h, max_idx_per_prototype_h = torch.max(max_per_prototype, dim=1)\n",
    "                max_per_prototype_w, max_idx_per_prototype_w = torch.max(max_per_prototype_h, dim=1) #shape (num_prototypes)\n",
    "                \n",
    "                h_idx = max_idx_per_prototype_h[p, max_idx_per_prototype_w[p]]\n",
    "                w_idx = max_idx_per_prototype_w[p]\n",
    "\n",
    "                if len(relevant_proto_class_names) == 0:\n",
    "                    continue\n",
    "                \n",
    "#                 if (len(relevant_proto_class_names) == 1) and (relevant_proto_class_names[0] not in non_leaf_children_names):\n",
    "#                     continue\n",
    "                \n",
    "                h_coor_min, h_coor_max, w_coor_min, w_coor_max = get_img_coordinates(args.image_size, softmaxes.shape, patchsize, skip, h_idx, w_idx)\n",
    "                latent_activation = softmaxes[:, p, :, :]\n",
    "                \n",
    "                if not find_non_descendants:\n",
    "                    if (coarse_label2name[ys.item()] in relevant_proto_class_names):\n",
    "                        child_node = root.get_node(coarse_label2name[ys.item()])\n",
    "                        leaf_descendent = label2name[orig_y.item()]#[4:7]\n",
    "                        img_to_open = imgs[i][0] # it is a tuple of (path to image, lable)\n",
    "                        if topk and (len(proto_mean_activations[p][leaf_descendent]) >= topk):\n",
    "                            heapq.heappushpop(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                              (pooled[p].item(), img_to_open,\\\n",
    "                                               (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "                        else:\n",
    "                            heapq.heappush(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                           (pooled[p].item(), img_to_open,\\\n",
    "                                            (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "                else:\n",
    "                    if (coarse_label2name[ys.item()] not in relevant_proto_class_names):\n",
    "                        child_node = root.get_node(coarse_label2name[ys.item()])\n",
    "                        leaf_descendent = label2name[orig_y.item()]#[4:7]\n",
    "                        img_to_open = imgs[i][0] # it is a tuple of (path to image, lable)\n",
    "                        if topk and (len(proto_mean_activations[p][leaf_descendent]) >= topk):\n",
    "                            heapq.heappushpop(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                              (pooled[p].item(), img_to_open,\\\n",
    "                                               (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "                        else:\n",
    "                            heapq.heappush(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                           (pooled[p].item(), img_to_open,\\\n",
    "                                            (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "\n",
    "                class_and_prototypes[', '.join(relevant_proto_class_names)].add(p)\n",
    "\n",
    "    # write_num_proto_details(proto_mean_activations, node.name, net, threshold=0.2, txt_file=txt_file, args=args)\n",
    "\n",
    "    if plot_overspecificity_score:\n",
    "        for child_classname in class_and_prototypes:\n",
    "            for p in class_and_prototypes[child_classname]:\n",
    "                mean_activation_of_every_leaf = []\n",
    "                for leaf_descendent in proto_mean_activations[p]:\n",
    "                    mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "                    mean_activation_of_every_leaf.append(mean_activation)\n",
    "\n",
    "                overspecificity_score = 1\n",
    "                for mean_act in mean_activation_of_every_leaf:\n",
    "                    overspecificity_score *= mean_act * 1.0\n",
    "                proto_presence = getattr(net.module, '_'+node.name+'_proto_presence')\n",
    "                proto_presence = F.gumbel_softmax(proto_presence, tau=0.5, hard=True, dim=-1)\n",
    "                proto_mask = proto_presence[p, 1].item()\n",
    "                overspecificity_score_and_proto_mask.append((overspecificity_score, len(mean_activation_of_every_leaf), proto_mask))\n",
    "\n",
    "    print('Node', node.name)\n",
    "    for child_classname in class_and_prototypes:\n",
    "        \n",
    "        print('\\t'*1, 'Child:', child_classname)\n",
    "        for p in class_and_prototypes[child_classname]:\n",
    "            \n",
    "            logstr = '\\t'*2 + f'Proto:{p} '\n",
    "            mean_activation_of_every_leaf = []\n",
    "            for leaf_descendent in proto_mean_activations[p]:\n",
    "                mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "                num_images = len(proto_mean_activations[p][leaf_descendent])\n",
    "                logstr += f'{leaf_descendent}:({mean_activation}) '\n",
    "                mean_activation_of_every_leaf.append(mean_activation)\n",
    "            print(logstr)\n",
    "            \n",
    "            # # if the mean_activation is less for all leaf descendants skip the node\n",
    "            # if all([mean_act < 0.2 for mean_act in mean_activation_of_every_leaf]):\n",
    "            #     if find_non_descendants:\n",
    "            #         print('\\t'*2 + f'Not skipping proto {p} of {node.name} coz of find_non_descendants')\n",
    "            #     else:\n",
    "            #         print('\\t'*2 + f'Skipping proto {p} of {node.name}')\n",
    "            #         continue\n",
    "            \n",
    "            # have this for NON descendants\n",
    "            if len(proto_mean_activations[p]) == 0:\n",
    "                continue\n",
    "            \n",
    "            if save_images or save_activation_as_npy_path:\n",
    "                patches = []\n",
    "                right_descriptions = []\n",
    "                text_region_width = 3 # 3x the width of a patch\n",
    "\n",
    "                font_size = 40\n",
    "                fnt = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "                max_width = ImageDraw.Draw(Image.new(\"RGB\", (100, 100), (255, 0, 0))).textlength('-', font=fnt)\n",
    "                \n",
    "                for leaf_descendent in proto_mean_activations[p]:\n",
    "                    for word in leaf_descendent.split('_')[2:]:\n",
    "                        width_of_word = ImageDraw.Draw(Image.new(\"RGB\", (100, 100), (255, 0, 0))).textlength(word, font=fnt)\n",
    "                        max_width = max(max_width, width_of_word)\n",
    "\n",
    "                for leaf_descendent, heap in proto_mean_activations[p].items():\n",
    "                    species_name = ' '.join(leaf_descendent.split('_')[2:])\n",
    "                    heap = sorted(heap)[::-1]\n",
    "                    mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "                    for rank, ele in enumerate(heap):\n",
    "                        activation, img_to_open, (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation = ele\n",
    "                        image = transforms.Resize(size=(args.image_size, args.image_size))(Image.open(img_to_open))\n",
    "                        img_tensor = transforms.ToTensor()(image)#.unsqueeze_(0) #shape (1, 3, h, w)\n",
    "                        img_tensor_patch = img_tensor[:, h_coor_min:h_coor_max, w_coor_min:w_coor_max]\n",
    "                        # latent_activation[latent_activation < torch.quantile(latent_activation, 0.75).item()] = 0.\n",
    "                        # latent_activation[latent_activation < 1.5] = 0.\n",
    "                        # pdb.set_trace()\n",
    "                        overlayed_image_np = get_heatmap(latent_activation, img_tensor, constant_color_scale=True)\n",
    "                        overlayed_image = torch.tensor(overlayed_image_np).permute(2, 0, 1).float() / 255.\n",
    "                        # overlayed_image = img_tensor\n",
    "                        patches.append(overlayed_image)\n",
    "                        \n",
    "                        # added for NUMPY SAVING\n",
    "                        \n",
    "                        if save_activation_as_npy_path:\n",
    "#                             upscaled_similarity_interpolated = get_upscaled_activation_interpolated(latent_activation,\n",
    "#                                                                                        image_size=(args.image_size, args.image_size))\n",
    "                            latent_activation_npy = latent_activation.squeeze().cpu().numpy()\n",
    "                            data = {'node_name': node.name,\n",
    "                                    'proto_num': p,\n",
    "                                    'child_name': child_classname,\n",
    "                                    'leaf_desc': leaf_descendent,\n",
    "                                     'rank': rank,\n",
    "                                     'img_path': img_to_open,\n",
    "                                     'img_filename': ntpath.basename(img_to_open),\n",
    "                                     'activation': latent_activation_npy,\n",
    "                                     'max_activation': activation,\n",
    "                                     'model_type': 'NAIVE-HPIPNET'}\n",
    "                            filename = str(rank)+ '-' + ntpath.basename(img_to_open) + '.npy'\n",
    "                            save_path = os.path.join(run_path, save_activation_as_npy_path, \\\n",
    "                                                     node.name, str(p), leaf_descendent,\n",
    "                                                     filename)\n",
    "                            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                            np.save(save_path, data, allow_pickle=True)\n",
    "\n",
    "                    # # description on the right hand side\n",
    "                    # text = f'{mean_activation}, {leaf_descendent}'\n",
    "                    # txtimage = Image.new(\"RGB\", (patches[0].shape[-2]*text_region_width,patches[0].shape[-1]), (255, 255, 255))\n",
    "                    # draw = D.Draw(txtimage)\n",
    "                    # draw.text((150, patches[0].shape[1]//2), text, anchor='mm', fill=\"black\", font=font)\n",
    "                    # pdb.set_trace()\n",
    "                    # txttensor = transforms.ToTensor()(txtimage)#.unsqueeze_(0)\n",
    "                    # right_descriptions.append(txttensor)\n",
    "\n",
    "                    text = '\\n'.join(species_name.split(' '))\n",
    "                    \n",
    "                    image_size = (math.ceil(max_width) + 10, patches[0].shape[-1])\n",
    "                    txtimage = Image.new(\"RGB\", image_size, (255, 255, 255))\n",
    "                    d = ImageDraw.Draw(txtimage)\n",
    "                    d.multiline_text((image_size[0]/2, image_size[1]/2), text, font=fnt, fill=(0, 0, 0), align =\"center\", anchor=\"mm\")\n",
    "                    txttensor = transforms.ToTensor()(txtimage)#.unsqueeze_(0)\n",
    "                    right_descriptions.append(txttensor)\n",
    "                    \n",
    "\n",
    "                padding = 0\n",
    "\n",
    "                # grid = torchvision.utils.make_grid(patches, nrow=topk, padding=padding, border=0)\n",
    "                # grid_right_descriptions = torchvision.utils.make_grid(right_descriptions, nrow=1, padding=padding, border=0)\n",
    "                # grid = torch.cat([grid_right_descriptions, grid], dim=-1)\n",
    "\n",
    "                grid_rows = []\n",
    "                for k in range(len(proto_mean_activations[p])):\n",
    "                    grid_row = torchvision.utils.make_grid(patches[k*topk:(k+1)*topk], nrow=topk, padding=padding, border=0)\n",
    "                    grid_right_description = torchvision.utils.make_grid(right_descriptions[k], nrow=1, padding=padding, border=0)\n",
    "                    grid_row = torch.cat([grid_right_description, grid_row], dim=-1)\n",
    "                    grid_rows.append(grid_row)\n",
    "                # grid = torch.cat(grid_rows, dim=0)\n",
    "                grid = torchvision.utils.make_grid(grid_rows, nrow=1, padding=5, pad_value=1.)\n",
    "                    \n",
    "                # # description on the top\n",
    "                # text = f'Node:{node.name}, p{p}, Child:{child_classname}'\n",
    "                # txtimage = Image.new(\"RGB\", (grid.shape[-1], args.wshape), (0, 0, 0))\n",
    "                # draw = D.Draw(txtimage)\n",
    "                # draw.text((150, patches[0].shape[1]//2), text, anchor='mm', fill=\"white\", font=font)\n",
    "                # txttensor = transforms.ToTensor()(txtimage)#.unsqueeze_(0)\n",
    "\n",
    "                # merging top description with the grid of images\n",
    "                # grid = torch.cat([grid, txttensor], dim=1)\n",
    "                \n",
    "                if save_images:\n",
    "                    prefix = 'non_' if find_non_descendants else ''\n",
    "                    os.makedirs(os.path.join(run_path, prefix+f'descendent_specific_topk_heatmap_clean_ep={epoch}', node.name), exist_ok=True)\n",
    "                    torchvision.utils.save_image(grid, os.path.join(run_path, prefix+f'descendent_specific_topk_heatmap_clean_ep={epoch}', node.name, f'{child_classname}-p{p}.png'), border=0) # , border_color=(255, 255, 255), border=10\n",
    "\n",
    "# txt_file.write('\\n')\n",
    "# txt_file.close()\n",
    "print('Done !!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# With Color image and zoomed in image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 377it [00:05, 63.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node root\n",
      "\t Child: 026+004\n",
      "\t\tProto:1 but_001_Heliconius_cydno_alithea:(0.8326) but_002_Heliconius_cydno_chioneus:(0.9956) but_003_Heliconius_cydno_cydnides:(0.8772) but_004_Heliconius_doris_doris:(0.9743) but_018_Heliconius_melpomene_amaryllis:(0.9977) but_019_Heliconius_melpomene_bellula:(0.9891) but_020_Heliconius_melpomene_cythera:(0.9803) but_021_Heliconius_melpomene_malleti:(0.9972) but_022_Heliconius_melpomene_melpomene:(0.9953) but_023_Heliconius_melpomene_nanna:(0.5591) but_024_Heliconius_melpomene_plesseni:(0.9521) but_025_Heliconius_melpomene_rosina:(0.9981) but_026_Heliconius_melpomene_vulcanus:(0.9636) but_030_Heliconius_timareta_linaresi:(0.9293) \n",
      "\t\tProto:2 but_001_Heliconius_cydno_alithea:(0.5582) but_002_Heliconius_cydno_chioneus:(0.993) but_003_Heliconius_cydno_cydnides:(0.8837) but_004_Heliconius_doris_doris:(0.9866) but_018_Heliconius_melpomene_amaryllis:(0.9932) but_019_Heliconius_melpomene_bellula:(0.9796) but_020_Heliconius_melpomene_cythera:(0.9888) but_021_Heliconius_melpomene_malleti:(0.9998) but_022_Heliconius_melpomene_melpomene:(0.9893) but_023_Heliconius_melpomene_nanna:(0.9202) but_024_Heliconius_melpomene_plesseni:(0.9971) but_025_Heliconius_melpomene_rosina:(0.9925) but_026_Heliconius_melpomene_vulcanus:(0.9528) but_030_Heliconius_timareta_linaresi:(0.9974) \n",
      "\t\tProto:5 but_001_Heliconius_cydno_alithea:(0.4709) but_002_Heliconius_cydno_chioneus:(0.9994) but_003_Heliconius_cydno_cydnides:(0.9886) but_004_Heliconius_doris_doris:(0.9909) but_018_Heliconius_melpomene_amaryllis:(0.9974) but_019_Heliconius_melpomene_bellula:(0.9805) but_020_Heliconius_melpomene_cythera:(0.9986) but_021_Heliconius_melpomene_malleti:(0.9838) but_022_Heliconius_melpomene_melpomene:(0.909) but_023_Heliconius_melpomene_nanna:(0.9715) but_024_Heliconius_melpomene_plesseni:(0.989) but_025_Heliconius_melpomene_rosina:(0.9968) but_026_Heliconius_melpomene_vulcanus:(0.9965) but_030_Heliconius_timareta_linaresi:(0.9435) \n",
      "\t\tProto:7 but_001_Heliconius_cydno_alithea:(0.427) but_002_Heliconius_cydno_chioneus:(0.9584) but_003_Heliconius_cydno_cydnides:(0.8473) but_004_Heliconius_doris_doris:(0.4154) but_018_Heliconius_melpomene_amaryllis:(0.9773) but_019_Heliconius_melpomene_bellula:(0.6411) but_020_Heliconius_melpomene_cythera:(0.8804) but_021_Heliconius_melpomene_malleti:(0.8285) but_022_Heliconius_melpomene_melpomene:(0.7734) but_023_Heliconius_melpomene_nanna:(0.7413) but_024_Heliconius_melpomene_plesseni:(0.8225) but_025_Heliconius_melpomene_rosina:(0.9601) but_026_Heliconius_melpomene_vulcanus:(0.9298) but_030_Heliconius_timareta_linaresi:(0.7443) \n",
      "\t\tProto:8 but_001_Heliconius_cydno_alithea:(0.4229) but_002_Heliconius_cydno_chioneus:(0.8948) but_003_Heliconius_cydno_cydnides:(0.846) but_004_Heliconius_doris_doris:(0.8902) but_018_Heliconius_melpomene_amaryllis:(0.9556) but_019_Heliconius_melpomene_bellula:(0.8401) but_020_Heliconius_melpomene_cythera:(0.7741) but_021_Heliconius_melpomene_malleti:(0.9167) but_022_Heliconius_melpomene_melpomene:(0.8407) but_023_Heliconius_melpomene_nanna:(0.2834) but_024_Heliconius_melpomene_plesseni:(0.7836) but_025_Heliconius_melpomene_rosina:(0.9411) but_026_Heliconius_melpomene_vulcanus:(0.7197) but_030_Heliconius_timareta_linaresi:(0.9007) \n",
      "\t\tProto:9 but_001_Heliconius_cydno_alithea:(0.5654) but_002_Heliconius_cydno_chioneus:(0.7825) but_003_Heliconius_cydno_cydnides:(0.5752) but_004_Heliconius_doris_doris:(0.8982) but_018_Heliconius_melpomene_amaryllis:(0.8447) but_019_Heliconius_melpomene_bellula:(0.4275) but_020_Heliconius_melpomene_cythera:(0.6911) but_021_Heliconius_melpomene_malleti:(0.7754) but_022_Heliconius_melpomene_melpomene:(0.68) but_023_Heliconius_melpomene_nanna:(0.1233) but_024_Heliconius_melpomene_plesseni:(0.7871) but_025_Heliconius_melpomene_rosina:(0.8081) but_026_Heliconius_melpomene_vulcanus:(0.3768) but_030_Heliconius_timareta_linaresi:(0.4501) \n",
      "\t Child: 005+013\n",
      "\t\tProto:12 but_005_Heliconius_eleuchia_eleuchia:(0.387) but_006_Heliconius_eleuchia_primularis:(0.1006) but_007_Heliconius_erato_amalfreda:(0.9648) but_008_Heliconius_erato_chestertonii:(0.9671) but_009_Heliconius_erato_cyrbia:(0.756) but_010_Heliconius_erato_demophoon:(0.8586) but_011_Heliconius_erato_dignus:(0.1692) but_012_Heliconius_erato_erato:(0.9417) but_013_Heliconius_erato_hydara:(0.7321) but_014_Heliconius_erato_lativitta:(0.0553) but_015_Heliconius_erato_notabilis:(0.9141) but_016_Heliconius_erato_phyllis:(0.0564) but_017_Heliconius_erato_venus:(0.5887) but_027_Heliconius_sara_magdalena:(0.4939) but_028_Heliconius_sara_sara:(0.7909) but_029_Heliconius_telesiphe_sotericus:(0.1802) \n",
      "\t\tProto:13 but_005_Heliconius_eleuchia_eleuchia:(0.9342) but_006_Heliconius_eleuchia_primularis:(0.1899) but_007_Heliconius_erato_amalfreda:(0.9843) but_008_Heliconius_erato_chestertonii:(0.9684) but_009_Heliconius_erato_cyrbia:(0.9752) but_010_Heliconius_erato_demophoon:(0.9927) but_011_Heliconius_erato_dignus:(0.744) but_012_Heliconius_erato_erato:(0.9797) but_013_Heliconius_erato_hydara:(0.9789) but_014_Heliconius_erato_lativitta:(0.46) but_015_Heliconius_erato_notabilis:(0.9773) but_016_Heliconius_erato_phyllis:(0.9306) but_017_Heliconius_erato_venus:(0.9907) but_027_Heliconius_sara_magdalena:(0.5541) but_028_Heliconius_sara_sara:(0.5207) but_029_Heliconius_telesiphe_sotericus:(0.9443) \n",
      "\t\tProto:15 but_005_Heliconius_eleuchia_eleuchia:(0.9848) but_006_Heliconius_eleuchia_primularis:(0.1562) but_007_Heliconius_erato_amalfreda:(0.9706) but_008_Heliconius_erato_chestertonii:(0.9959) but_009_Heliconius_erato_cyrbia:(0.9971) but_010_Heliconius_erato_demophoon:(0.9778) but_011_Heliconius_erato_dignus:(0.9976) but_012_Heliconius_erato_erato:(0.9948) but_013_Heliconius_erato_hydara:(0.9995) but_014_Heliconius_erato_lativitta:(0.9996) but_015_Heliconius_erato_notabilis:(0.9998) but_016_Heliconius_erato_phyllis:(0.9951) but_017_Heliconius_erato_venus:(0.9997) but_027_Heliconius_sara_magdalena:(0.8314) but_028_Heliconius_sara_sara:(0.978) but_029_Heliconius_telesiphe_sotericus:(0.9986) \n",
      "\t\tProto:16 but_005_Heliconius_eleuchia_eleuchia:(0.9542) but_006_Heliconius_eleuchia_primularis:(0.7523) but_007_Heliconius_erato_amalfreda:(0.7724) but_008_Heliconius_erato_chestertonii:(0.5822) but_009_Heliconius_erato_cyrbia:(0.9545) but_010_Heliconius_erato_demophoon:(0.9997) but_011_Heliconius_erato_dignus:(0.475) but_012_Heliconius_erato_erato:(0.8725) but_013_Heliconius_erato_hydara:(0.9474) but_014_Heliconius_erato_lativitta:(0.4703) but_015_Heliconius_erato_notabilis:(0.9856) but_016_Heliconius_erato_phyllis:(0.9633) but_017_Heliconius_erato_venus:(0.8313) but_027_Heliconius_sara_magdalena:(0.8502) but_028_Heliconius_sara_sara:(0.6264) but_029_Heliconius_telesiphe_sotericus:(0.983) \n",
      "\t\tProto:18 but_005_Heliconius_eleuchia_eleuchia:(0.7608) but_006_Heliconius_eleuchia_primularis:(0.0878) but_007_Heliconius_erato_amalfreda:(0.9805) but_008_Heliconius_erato_chestertonii:(0.9817) but_009_Heliconius_erato_cyrbia:(0.9906) but_010_Heliconius_erato_demophoon:(0.9989) but_011_Heliconius_erato_dignus:(0.7476) but_012_Heliconius_erato_erato:(0.992) but_013_Heliconius_erato_hydara:(0.9797) but_014_Heliconius_erato_lativitta:(0.9996) but_015_Heliconius_erato_notabilis:(0.9986) but_016_Heliconius_erato_phyllis:(0.9945) but_017_Heliconius_erato_venus:(0.9988) but_027_Heliconius_sara_magdalena:(0.9113) but_028_Heliconius_sara_sara:(0.9841) but_029_Heliconius_telesiphe_sotericus:(0.9456) \n",
      "\t\tProto:19 but_005_Heliconius_eleuchia_eleuchia:(0.1027) but_006_Heliconius_eleuchia_primularis:(0.0584) but_007_Heliconius_erato_amalfreda:(0.0967) but_008_Heliconius_erato_chestertonii:(0.6223) but_009_Heliconius_erato_cyrbia:(0.1142) but_010_Heliconius_erato_demophoon:(0.2674) but_011_Heliconius_erato_dignus:(0.0962) but_012_Heliconius_erato_erato:(0.9483) but_013_Heliconius_erato_hydara:(0.4829) but_014_Heliconius_erato_lativitta:(0.8631) but_015_Heliconius_erato_notabilis:(0.0807) but_016_Heliconius_erato_phyllis:(0.1713) but_017_Heliconius_erato_venus:(0.1095) but_027_Heliconius_sara_magdalena:(0.8347) but_028_Heliconius_sara_sara:(0.3893) but_029_Heliconius_telesiphe_sotericus:(0.0886) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 165it [00:03, 54.04it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 026+004\n",
      "\t Child: 026+030\n",
      "\t\tProto:1 but_001_Heliconius_cydno_alithea:(0.4738) but_002_Heliconius_cydno_chioneus:(0.7147) but_003_Heliconius_cydno_cydnides:(0.9168) but_018_Heliconius_melpomene_amaryllis:(0.9965) but_019_Heliconius_melpomene_bellula:(0.8005) but_020_Heliconius_melpomene_cythera:(0.8459) but_021_Heliconius_melpomene_malleti:(0.7226) but_022_Heliconius_melpomene_melpomene:(0.8384) but_023_Heliconius_melpomene_nanna:(0.1824) but_024_Heliconius_melpomene_plesseni:(0.8585) but_025_Heliconius_melpomene_rosina:(0.9924) but_026_Heliconius_melpomene_vulcanus:(0.9195) but_030_Heliconius_timareta_linaresi:(0.9315) \n",
      "\t\tProto:3 but_001_Heliconius_cydno_alithea:(0.2052) but_002_Heliconius_cydno_chioneus:(0.8677) but_003_Heliconius_cydno_cydnides:(0.8527) but_018_Heliconius_melpomene_amaryllis:(0.7923) but_019_Heliconius_melpomene_bellula:(0.82) but_020_Heliconius_melpomene_cythera:(0.8137) but_021_Heliconius_melpomene_malleti:(0.8292) but_022_Heliconius_melpomene_melpomene:(0.9512) but_023_Heliconius_melpomene_nanna:(0.4585) but_024_Heliconius_melpomene_plesseni:(0.9749) but_025_Heliconius_melpomene_rosina:(0.9365) but_026_Heliconius_melpomene_vulcanus:(0.8565) but_030_Heliconius_timareta_linaresi:(0.4783) \n",
      "\t\tProto:4 but_001_Heliconius_cydno_alithea:(0.7335) but_002_Heliconius_cydno_chioneus:(0.9906) but_003_Heliconius_cydno_cydnides:(0.8161) but_018_Heliconius_melpomene_amaryllis:(0.9892) but_019_Heliconius_melpomene_bellula:(0.982) but_020_Heliconius_melpomene_cythera:(0.9447) but_021_Heliconius_melpomene_malleti:(0.9799) but_022_Heliconius_melpomene_melpomene:(0.9582) but_023_Heliconius_melpomene_nanna:(0.9463) but_024_Heliconius_melpomene_plesseni:(0.9917) but_025_Heliconius_melpomene_rosina:(0.9921) but_026_Heliconius_melpomene_vulcanus:(0.9776) but_030_Heliconius_timareta_linaresi:(0.863) \n",
      "\t\tProto:6 but_001_Heliconius_cydno_alithea:(0.817) but_002_Heliconius_cydno_chioneus:(0.9868) but_003_Heliconius_cydno_cydnides:(0.991) but_018_Heliconius_melpomene_amaryllis:(0.996) but_019_Heliconius_melpomene_bellula:(0.9977) but_020_Heliconius_melpomene_cythera:(0.984) but_021_Heliconius_melpomene_malleti:(0.9941) but_022_Heliconius_melpomene_melpomene:(0.9907) but_023_Heliconius_melpomene_nanna:(0.4516) but_024_Heliconius_melpomene_plesseni:(0.9797) but_025_Heliconius_melpomene_rosina:(0.987) but_026_Heliconius_melpomene_vulcanus:(0.9853) but_030_Heliconius_timareta_linaresi:(0.9858) \n",
      "\t\tProto:7 but_001_Heliconius_cydno_alithea:(0.5594) but_002_Heliconius_cydno_chioneus:(0.9722) but_003_Heliconius_cydno_cydnides:(0.9433) but_018_Heliconius_melpomene_amaryllis:(0.9923) but_019_Heliconius_melpomene_bellula:(0.9874) but_020_Heliconius_melpomene_cythera:(0.8666) but_021_Heliconius_melpomene_malleti:(0.8846) but_022_Heliconius_melpomene_melpomene:(0.9791) but_023_Heliconius_melpomene_nanna:(0.8895) but_024_Heliconius_melpomene_plesseni:(0.9947) but_025_Heliconius_melpomene_rosina:(0.9896) but_026_Heliconius_melpomene_vulcanus:(0.9851) but_030_Heliconius_timareta_linaresi:(0.9091) \n",
      "\t\tProto:9 but_001_Heliconius_cydno_alithea:(0.6728) but_002_Heliconius_cydno_chioneus:(0.9598) but_003_Heliconius_cydno_cydnides:(0.9238) but_018_Heliconius_melpomene_amaryllis:(0.9766) but_019_Heliconius_melpomene_bellula:(0.9682) but_020_Heliconius_melpomene_cythera:(0.927) but_021_Heliconius_melpomene_malleti:(0.9971) but_022_Heliconius_melpomene_melpomene:(0.9055) but_023_Heliconius_melpomene_nanna:(0.9725) but_024_Heliconius_melpomene_plesseni:(0.9592) but_025_Heliconius_melpomene_rosina:(0.9955) but_026_Heliconius_melpomene_vulcanus:(0.942) but_030_Heliconius_timareta_linaresi:(0.9727) \n",
      "\t Child: but_004_Heliconius_doris_doris\n",
      "\t\tProto:10 but_004_Heliconius_doris_doris:(0.8851) \n",
      "\t\tProto:13 but_004_Heliconius_doris_doris:(0.8648) \n",
      "\t\tProto:14 but_004_Heliconius_doris_doris:(0.9558) \n",
      "\t\tProto:15 but_004_Heliconius_doris_doris:(0.9966) \n",
      "\t\tProto:18 but_004_Heliconius_doris_doris:(0.7336) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 212it [00:03, 59.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 005+013\n",
      "\t Child: 005+028\n",
      "\t\tProto:1 but_005_Heliconius_eleuchia_eleuchia:(0.705) but_006_Heliconius_eleuchia_primularis:(0.7031) but_027_Heliconius_sara_magdalena:(0.9776) but_028_Heliconius_sara_sara:(0.7027) \n",
      "\t\tProto:2 but_005_Heliconius_eleuchia_eleuchia:(0.9971) but_006_Heliconius_eleuchia_primularis:(0.9994) but_027_Heliconius_sara_magdalena:(0.9995) but_028_Heliconius_sara_sara:(0.9975) \n",
      "\t\tProto:6 but_005_Heliconius_eleuchia_eleuchia:(0.9889) but_006_Heliconius_eleuchia_primularis:(0.9897) but_027_Heliconius_sara_magdalena:(0.9911) but_028_Heliconius_sara_sara:(0.9941) \n",
      "\t\tProto:7 but_005_Heliconius_eleuchia_eleuchia:(0.9741) but_006_Heliconius_eleuchia_primularis:(0.988) but_027_Heliconius_sara_magdalena:(0.992) but_028_Heliconius_sara_sara:(0.9759) \n",
      "\t Child: 013+029\n",
      "\t\tProto:10 but_007_Heliconius_erato_amalfreda:(0.9992) but_008_Heliconius_erato_chestertonii:(0.9846) but_009_Heliconius_erato_cyrbia:(0.9842) but_010_Heliconius_erato_demophoon:(0.9993) but_011_Heliconius_erato_dignus:(0.9613) but_012_Heliconius_erato_erato:(0.9878) but_013_Heliconius_erato_hydara:(0.995) but_014_Heliconius_erato_lativitta:(0.9993) but_015_Heliconius_erato_notabilis:(0.9993) but_016_Heliconius_erato_phyllis:(0.9953) but_017_Heliconius_erato_venus:(0.9995) but_029_Heliconius_telesiphe_sotericus:(0.994) \n",
      "\t\tProto:11 but_007_Heliconius_erato_amalfreda:(0.987) but_008_Heliconius_erato_chestertonii:(0.9871) but_009_Heliconius_erato_cyrbia:(0.997) but_010_Heliconius_erato_demophoon:(0.9988) but_011_Heliconius_erato_dignus:(0.9851) but_012_Heliconius_erato_erato:(0.9572) but_013_Heliconius_erato_hydara:(0.9648) but_014_Heliconius_erato_lativitta:(0.9508) but_015_Heliconius_erato_notabilis:(0.991) but_016_Heliconius_erato_phyllis:(0.9325) but_017_Heliconius_erato_venus:(0.9811) but_029_Heliconius_telesiphe_sotericus:(0.9849) \n",
      "\t\tProto:12 but_007_Heliconius_erato_amalfreda:(0.8609) but_008_Heliconius_erato_chestertonii:(0.9767) but_009_Heliconius_erato_cyrbia:(0.9536) but_010_Heliconius_erato_demophoon:(0.9987) but_011_Heliconius_erato_dignus:(0.9773) but_012_Heliconius_erato_erato:(0.8672) but_013_Heliconius_erato_hydara:(0.9718) but_014_Heliconius_erato_lativitta:(0.9712) but_015_Heliconius_erato_notabilis:(0.989) but_016_Heliconius_erato_phyllis:(0.9672) but_017_Heliconius_erato_venus:(0.9892) but_029_Heliconius_telesiphe_sotericus:(0.9791) \n",
      "\t\tProto:15 but_007_Heliconius_erato_amalfreda:(0.9985) but_008_Heliconius_erato_chestertonii:(0.9982) but_009_Heliconius_erato_cyrbia:(0.9954) but_010_Heliconius_erato_demophoon:(0.9985) but_011_Heliconius_erato_dignus:(0.9984) but_012_Heliconius_erato_erato:(0.9658) but_013_Heliconius_erato_hydara:(0.9998) but_014_Heliconius_erato_lativitta:(0.9936) but_015_Heliconius_erato_notabilis:(0.9994) but_016_Heliconius_erato_phyllis:(0.9974) but_017_Heliconius_erato_venus:(0.9997) but_029_Heliconius_telesiphe_sotericus:(0.9854) \n",
      "\t\tProto:17 but_007_Heliconius_erato_amalfreda:(0.9369) but_008_Heliconius_erato_chestertonii:(0.9941) but_009_Heliconius_erato_cyrbia:(0.9737) but_010_Heliconius_erato_demophoon:(0.9971) but_011_Heliconius_erato_dignus:(0.9827) but_012_Heliconius_erato_erato:(0.986) but_013_Heliconius_erato_hydara:(0.9898) but_014_Heliconius_erato_lativitta:(0.9939) but_015_Heliconius_erato_notabilis:(0.9786) but_016_Heliconius_erato_phyllis:(0.9922) but_017_Heliconius_erato_venus:(0.9873) but_029_Heliconius_telesiphe_sotericus:(0.9957) \n",
      "\t\tProto:19 but_007_Heliconius_erato_amalfreda:(0.9498) but_008_Heliconius_erato_chestertonii:(0.5507) but_009_Heliconius_erato_cyrbia:(0.9658) but_010_Heliconius_erato_demophoon:(0.9824) but_011_Heliconius_erato_dignus:(0.9304) but_012_Heliconius_erato_erato:(0.9157) but_013_Heliconius_erato_hydara:(0.987) but_014_Heliconius_erato_lativitta:(0.881) but_015_Heliconius_erato_notabilis:(0.9669) but_016_Heliconius_erato_phyllis:(0.7314) but_017_Heliconius_erato_venus:(0.9759) but_029_Heliconius_telesiphe_sotericus:(0.9027) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 158it [00:02, 56.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 026+030\n",
      "\t Child: 026+020+023+021+018+024+019+025\n",
      "\t\tProto:8 but_018_Heliconius_melpomene_amaryllis:(0.9988) but_019_Heliconius_melpomene_bellula:(0.9994) but_020_Heliconius_melpomene_cythera:(0.9429) but_021_Heliconius_melpomene_malleti:(0.9997) but_022_Heliconius_melpomene_melpomene:(0.9832) but_023_Heliconius_melpomene_nanna:(0.9927) but_024_Heliconius_melpomene_plesseni:(0.9811) but_025_Heliconius_melpomene_rosina:(0.9999) but_026_Heliconius_melpomene_vulcanus:(0.9889) \n",
      "\t\tProto:1 but_018_Heliconius_melpomene_amaryllis:(0.9473) but_019_Heliconius_melpomene_bellula:(0.9889) but_020_Heliconius_melpomene_cythera:(0.9683) but_021_Heliconius_melpomene_malleti:(0.9868) but_022_Heliconius_melpomene_melpomene:(0.9939) but_023_Heliconius_melpomene_nanna:(0.943) but_024_Heliconius_melpomene_plesseni:(0.9971) but_025_Heliconius_melpomene_rosina:(0.9923) but_026_Heliconius_melpomene_vulcanus:(0.9939) \n",
      "\t\tProto:5 but_018_Heliconius_melpomene_amaryllis:(0.9745) but_019_Heliconius_melpomene_bellula:(0.9836) but_020_Heliconius_melpomene_cythera:(0.6908) but_021_Heliconius_melpomene_malleti:(0.9833) but_022_Heliconius_melpomene_melpomene:(0.9924) but_023_Heliconius_melpomene_nanna:(0.9965) but_024_Heliconius_melpomene_plesseni:(0.9995) but_025_Heliconius_melpomene_rosina:(0.9924) but_026_Heliconius_melpomene_vulcanus:(0.9992) \n",
      "\t\tProto:7 but_018_Heliconius_melpomene_amaryllis:(0.964) but_019_Heliconius_melpomene_bellula:(0.9945) but_020_Heliconius_melpomene_cythera:(0.9652) but_021_Heliconius_melpomene_malleti:(0.9932) but_022_Heliconius_melpomene_melpomene:(0.998) but_023_Heliconius_melpomene_nanna:(0.9612) but_024_Heliconius_melpomene_plesseni:(0.9831) but_025_Heliconius_melpomene_rosina:(0.9854) but_026_Heliconius_melpomene_vulcanus:(0.9923) \n",
      "\t Child: 030+001\n",
      "\t\tProto:10 but_001_Heliconius_cydno_alithea:(0.6853) but_002_Heliconius_cydno_chioneus:(0.9981) but_003_Heliconius_cydno_cydnides:(0.9901) but_030_Heliconius_timareta_linaresi:(0.9888) \n",
      "\t\tProto:15 but_001_Heliconius_cydno_alithea:(0.7531) but_002_Heliconius_cydno_chioneus:(0.9993) but_003_Heliconius_cydno_cydnides:(0.8437) but_030_Heliconius_timareta_linaresi:(0.9876) \n",
      "\t\tProto:17 but_001_Heliconius_cydno_alithea:(0.1558) but_002_Heliconius_cydno_chioneus:(0.9948) but_003_Heliconius_cydno_cydnides:(0.7197) but_030_Heliconius_timareta_linaresi:(0.4809) \n",
      "\t\tProto:18 but_001_Heliconius_cydno_alithea:(0.0513) but_002_Heliconius_cydno_chioneus:(0.1332) but_003_Heliconius_cydno_cydnides:(0.0757) but_030_Heliconius_timareta_linaresi:(0.2394) \n",
      "\t\tProto:19 but_001_Heliconius_cydno_alithea:(0.9801) but_002_Heliconius_cydno_chioneus:(0.9979) but_003_Heliconius_cydno_cydnides:(0.9982) but_030_Heliconius_timareta_linaresi:(0.9956) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 33it [00:01, 27.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 005+028\n",
      "\t Child: 005+006\n",
      "\t\tProto:0 but_005_Heliconius_eleuchia_eleuchia:(0.9998) but_006_Heliconius_eleuchia_primularis:(0.9988) \n",
      "\t\tProto:9 but_005_Heliconius_eleuchia_eleuchia:(0.9997) but_006_Heliconius_eleuchia_primularis:(0.9995) \n",
      "\t\tProto:2 but_005_Heliconius_eleuchia_eleuchia:(0.9417) but_006_Heliconius_eleuchia_primularis:(0.9433) \n",
      "\t\tProto:6 but_005_Heliconius_eleuchia_eleuchia:(0.9999) but_006_Heliconius_eleuchia_primularis:(0.9999) \n",
      "\t Child: 028+027\n",
      "\t\tProto:16 but_027_Heliconius_sara_magdalena:(1.0) but_028_Heliconius_sara_sara:(1.0) \n",
      "\t\tProto:19 but_027_Heliconius_sara_magdalena:(0.9998) but_028_Heliconius_sara_sara:(0.9985) \n",
      "\t\tProto:13 but_027_Heliconius_sara_magdalena:(0.9981) but_028_Heliconius_sara_sara:(1.0) \n",
      "\t\tProto:14 but_027_Heliconius_sara_magdalena:(1.0) but_028_Heliconius_sara_sara:(0.9999) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 179it [00:03, 55.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 013+029\n",
      "\t Child: 013+015+014+017+011+016+008+009+010+007+012\n",
      "\t\tProto:0 but_007_Heliconius_erato_amalfreda:(0.7249) but_008_Heliconius_erato_chestertonii:(0.9728) but_009_Heliconius_erato_cyrbia:(0.937) but_010_Heliconius_erato_demophoon:(0.9995) but_011_Heliconius_erato_dignus:(0.9048) but_012_Heliconius_erato_erato:(0.9689) but_013_Heliconius_erato_hydara:(0.9653) but_014_Heliconius_erato_lativitta:(0.9972) but_015_Heliconius_erato_notabilis:(0.97) but_016_Heliconius_erato_phyllis:(0.92) but_017_Heliconius_erato_venus:(0.9702) \n",
      "\t\tProto:1 but_007_Heliconius_erato_amalfreda:(0.7944) but_008_Heliconius_erato_chestertonii:(0.6842) but_009_Heliconius_erato_cyrbia:(0.7212) but_010_Heliconius_erato_demophoon:(0.9427) but_011_Heliconius_erato_dignus:(0.4279) but_012_Heliconius_erato_erato:(0.9458) but_013_Heliconius_erato_hydara:(0.5448) but_014_Heliconius_erato_lativitta:(0.7915) but_015_Heliconius_erato_notabilis:(0.8012) but_016_Heliconius_erato_phyllis:(0.8203) but_017_Heliconius_erato_venus:(0.8172) \n",
      "\t\tProto:2 but_007_Heliconius_erato_amalfreda:(0.9342) but_008_Heliconius_erato_chestertonii:(0.9334) but_009_Heliconius_erato_cyrbia:(0.9508) but_010_Heliconius_erato_demophoon:(0.9054) but_011_Heliconius_erato_dignus:(0.2352) but_012_Heliconius_erato_erato:(0.9889) but_013_Heliconius_erato_hydara:(0.9951) but_014_Heliconius_erato_lativitta:(0.995) but_015_Heliconius_erato_notabilis:(0.9947) but_016_Heliconius_erato_phyllis:(0.9691) but_017_Heliconius_erato_venus:(0.9977) \n",
      "\t\tProto:3 but_007_Heliconius_erato_amalfreda:(0.4121) but_008_Heliconius_erato_chestertonii:(0.393) but_009_Heliconius_erato_cyrbia:(0.5264) but_010_Heliconius_erato_demophoon:(0.3759) but_011_Heliconius_erato_dignus:(0.3373) but_012_Heliconius_erato_erato:(0.4289) but_013_Heliconius_erato_hydara:(0.3814) but_014_Heliconius_erato_lativitta:(0.3793) but_015_Heliconius_erato_notabilis:(0.5331) but_016_Heliconius_erato_phyllis:(0.3606) but_017_Heliconius_erato_venus:(0.3606) \n",
      "\t\tProto:5 but_007_Heliconius_erato_amalfreda:(0.9346) but_008_Heliconius_erato_chestertonii:(0.8402) but_009_Heliconius_erato_cyrbia:(0.7677) but_010_Heliconius_erato_demophoon:(0.9187) but_011_Heliconius_erato_dignus:(0.4141) but_012_Heliconius_erato_erato:(0.9264) but_013_Heliconius_erato_hydara:(0.8879) but_014_Heliconius_erato_lativitta:(0.9199) but_015_Heliconius_erato_notabilis:(0.8562) but_016_Heliconius_erato_phyllis:(0.6538) but_017_Heliconius_erato_venus:(0.8591) \n",
      "\t\tProto:6 but_007_Heliconius_erato_amalfreda:(0.9914) but_008_Heliconius_erato_chestertonii:(0.9326) but_009_Heliconius_erato_cyrbia:(0.995) but_010_Heliconius_erato_demophoon:(0.3693) but_011_Heliconius_erato_dignus:(0.9856) but_012_Heliconius_erato_erato:(0.9964) but_013_Heliconius_erato_hydara:(0.9943) but_014_Heliconius_erato_lativitta:(0.9506) but_015_Heliconius_erato_notabilis:(0.9976) but_016_Heliconius_erato_phyllis:(0.9917) but_017_Heliconius_erato_venus:(0.9922) \n",
      "\t\tProto:8 but_007_Heliconius_erato_amalfreda:(0.9935) but_008_Heliconius_erato_chestertonii:(0.964) but_009_Heliconius_erato_cyrbia:(0.8873) but_010_Heliconius_erato_demophoon:(0.9794) but_011_Heliconius_erato_dignus:(0.9035) but_012_Heliconius_erato_erato:(0.9781) but_013_Heliconius_erato_hydara:(0.8793) but_014_Heliconius_erato_lativitta:(0.9548) but_015_Heliconius_erato_notabilis:(0.9649) but_016_Heliconius_erato_phyllis:(0.8079) but_017_Heliconius_erato_venus:(0.9693) \n",
      "\t Child: but_029_Heliconius_telesiphe_sotericus\n",
      "\t\tProto:10 but_029_Heliconius_telesiphe_sotericus:(0.9685) \n",
      "\t\tProto:11 but_029_Heliconius_telesiphe_sotericus:(0.999) \n",
      "\t\tProto:13 but_029_Heliconius_telesiphe_sotericus:(1.0) \n",
      "\t\tProto:16 but_029_Heliconius_telesiphe_sotericus:(0.9897) \n",
      "\t\tProto:18 but_029_Heliconius_telesiphe_sotericus:(0.9736) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 123it [00:03, 31.73it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 026+020+023+021+018+024+019+025\n",
      "\t Child: but_026_Heliconius_melpomene_vulcanus\n",
      "\t\tProto:0 but_026_Heliconius_melpomene_vulcanus:(0.9982) \n",
      "\t\tProto:1 but_026_Heliconius_melpomene_vulcanus:(0.9964) \n",
      "\t\tProto:3 but_026_Heliconius_melpomene_vulcanus:(0.9966) \n",
      "\t\tProto:6 but_026_Heliconius_melpomene_vulcanus:(0.98) \n",
      "\t\tProto:9 but_026_Heliconius_melpomene_vulcanus:(0.9841) \n",
      "\t Child: but_020_Heliconius_melpomene_cythera\n",
      "\t\tProto:12 but_020_Heliconius_melpomene_cythera:(0.9986) \n",
      "\t\tProto:13 but_020_Heliconius_melpomene_cythera:(1.0) \n",
      "\t\tProto:14 but_020_Heliconius_melpomene_cythera:(0.9996) \n",
      "\t\tProto:17 but_020_Heliconius_melpomene_cythera:(0.9936) \n",
      "\t\tProto:18 but_020_Heliconius_melpomene_cythera:(1.0) \n",
      "\t Child: but_023_Heliconius_melpomene_nanna\n",
      "\t\tProto:29 but_023_Heliconius_melpomene_nanna:(0.9999) \n",
      "\t\tProto:20 but_023_Heliconius_melpomene_nanna:(0.9986) \n",
      "\t\tProto:28 but_023_Heliconius_melpomene_nanna:(0.9655) \n",
      "\t Child: but_021_Heliconius_melpomene_malleti\n",
      "\t\tProto:32 but_021_Heliconius_melpomene_malleti:(0.9999) \n",
      "\t\tProto:34 but_021_Heliconius_melpomene_malleti:(0.9998) \n",
      "\t\tProto:35 but_021_Heliconius_melpomene_malleti:(0.9999) \n",
      "\t\tProto:39 but_021_Heliconius_melpomene_malleti:(0.9994) \n",
      "\t Child: but_018_Heliconius_melpomene_amaryllis\n",
      "\t\tProto:40 but_018_Heliconius_melpomene_amaryllis:(1.0) \n",
      "\t\tProto:43 but_018_Heliconius_melpomene_amaryllis:(1.0) \n",
      "\t\tProto:44 but_018_Heliconius_melpomene_amaryllis:(1.0) \n",
      "\t\tProto:46 but_018_Heliconius_melpomene_amaryllis:(0.9994) \n",
      "\t Child: but_024_Heliconius_melpomene_plesseni\n",
      "\t\tProto:52 but_024_Heliconius_melpomene_plesseni:(0.997) \n",
      "\t\tProto:55 but_024_Heliconius_melpomene_plesseni:(0.9997) \n",
      "\t\tProto:56 but_024_Heliconius_melpomene_plesseni:(0.8354) \n",
      "\t\tProto:57 but_024_Heliconius_melpomene_plesseni:(1.0) \n",
      "\t\tProto:58 but_024_Heliconius_melpomene_plesseni:(1.0) \n",
      "\t Child: but_019_Heliconius_melpomene_bellula\n",
      "\t\tProto:65 but_019_Heliconius_melpomene_bellula:(0.998) \n",
      "\t\tProto:68 but_019_Heliconius_melpomene_bellula:(0.9983) \n",
      "\t\tProto:69 but_019_Heliconius_melpomene_bellula:(0.9999) \n",
      "\t\tProto:63 but_019_Heliconius_melpomene_bellula:(0.9949) \n",
      "\t Child: 025+022\n",
      "\t\tProto:70 but_022_Heliconius_melpomene_melpomene:(0.9949) but_025_Heliconius_melpomene_rosina:(0.9894) \n",
      "\t\tProto:72 but_022_Heliconius_melpomene_melpomene:(0.9781) but_025_Heliconius_melpomene_rosina:(0.9477) \n",
      "\t\tProto:73 but_022_Heliconius_melpomene_melpomene:(0.3656) but_025_Heliconius_melpomene_rosina:(0.311) \n",
      "\t\tProto:75 but_022_Heliconius_melpomene_melpomene:(0.9999) but_025_Heliconius_melpomene_rosina:(0.9946) \n",
      "\t\tProto:77 but_022_Heliconius_melpomene_melpomene:(0.9987) but_025_Heliconius_melpomene_rosina:(0.9929) \n",
      "\t\tProto:79 but_022_Heliconius_melpomene_melpomene:(0.9814) but_025_Heliconius_melpomene_rosina:(0.9989) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 35it [00:01, 28.96it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 030+001\n",
      "\t Child: but_030_Heliconius_timareta_linaresi\n",
      "\t\tProto:1 but_030_Heliconius_timareta_linaresi:(0.992) \n",
      "\t\tProto:2 but_030_Heliconius_timareta_linaresi:(0.9943) \n",
      "\t\tProto:4 but_030_Heliconius_timareta_linaresi:(0.9557) \n",
      "\t\tProto:5 but_030_Heliconius_timareta_linaresi:(1.0) \n",
      "\t Child: 001+003+002\n",
      "\t\tProto:11 but_001_Heliconius_cydno_alithea:(0.6343) but_002_Heliconius_cydno_chioneus:(0.9871) but_003_Heliconius_cydno_cydnides:(0.1742) \n",
      "\t\tProto:13 but_001_Heliconius_cydno_alithea:(0.9572) but_002_Heliconius_cydno_chioneus:(0.9997) but_003_Heliconius_cydno_cydnides:(0.9774) \n",
      "\t\tProto:14 but_001_Heliconius_cydno_alithea:(0.6781) but_002_Heliconius_cydno_chioneus:(0.9992) but_003_Heliconius_cydno_cydnides:(0.23) \n",
      "\t\tProto:16 but_001_Heliconius_cydno_alithea:(0.789) but_002_Heliconius_cydno_chioneus:(0.9986) but_003_Heliconius_cydno_cydnides:(0.4284) \n",
      "\t\tProto:17 but_001_Heliconius_cydno_alithea:(0.8639) but_002_Heliconius_cydno_chioneus:(1.0) but_003_Heliconius_cydno_cydnides:(0.1071) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 16/16 [00:00<00:00, 17.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 005+006\n",
      "\t Child: but_005_Heliconius_eleuchia_eleuchia\n",
      "\t\tProto:0 but_005_Heliconius_eleuchia_eleuchia:(0.9998) \n",
      "\t\tProto:4 but_005_Heliconius_eleuchia_eleuchia:(0.99) \n",
      "\t\tProto:5 but_005_Heliconius_eleuchia_eleuchia:(0.9999) \n",
      "\t\tProto:7 but_005_Heliconius_eleuchia_eleuchia:(1.0) \n",
      "\t Child: but_006_Heliconius_eleuchia_primularis\n",
      "\t\tProto:18 but_006_Heliconius_eleuchia_primularis:(0.9996) \n",
      "\t\tProto:15 but_006_Heliconius_eleuchia_primularis:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 17/17 [00:00<00:00, 17.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 028+027\n",
      "\t Child: but_028_Heliconius_sara_sara\n",
      "\t\tProto:0 but_028_Heliconius_sara_sara:(0.9999) \n",
      "\t\tProto:1 but_028_Heliconius_sara_sara:(0.9998) \n",
      "\t\tProto:9 but_028_Heliconius_sara_sara:(0.9997) \n",
      "\t Child: but_027_Heliconius_sara_magdalena\n",
      "\t\tProto:11 but_027_Heliconius_sara_magdalena:(0.9986) \n",
      "\t\tProto:13 but_027_Heliconius_sara_magdalena:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 172/172 [00:06<00:00, 27.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 013+015+014+017+011+016+008+009+010+007+012\n",
      "\t Child: but_013_Heliconius_erato_hydara\n",
      "\t\tProto:1 but_013_Heliconius_erato_hydara:(1.0) \n",
      "\t\tProto:3 but_013_Heliconius_erato_hydara:(0.9995) \n",
      "\t Child: but_015_Heliconius_erato_notabilis\n",
      "\t\tProto:10 but_015_Heliconius_erato_notabilis:(1.0) \n",
      "\t\tProto:19 but_015_Heliconius_erato_notabilis:(0.9998) \n",
      "\t\tProto:12 but_015_Heliconius_erato_notabilis:(1.0) \n",
      "\t\tProto:13 but_015_Heliconius_erato_notabilis:(0.9414) \n",
      "\t Child: but_014_Heliconius_erato_lativitta\n",
      "\t\tProto:20 but_014_Heliconius_erato_lativitta:(0.994) \n",
      "\t\tProto:22 but_014_Heliconius_erato_lativitta:(1.0) \n",
      "\t\tProto:24 but_014_Heliconius_erato_lativitta:(0.9857) \n",
      "\t\tProto:26 but_014_Heliconius_erato_lativitta:(1.0) \n",
      "\t\tProto:27 but_014_Heliconius_erato_lativitta:(0.9999) \n",
      "\t Child: but_017_Heliconius_erato_venus\n",
      "\t\tProto:34 but_017_Heliconius_erato_venus:(1.0) \n",
      "\t\tProto:35 but_017_Heliconius_erato_venus:(1.0) \n",
      "\t\tProto:30 but_017_Heliconius_erato_venus:(0.9999) \n",
      "\t\tProto:39 but_017_Heliconius_erato_venus:(0.9998) \n",
      "\t Child: but_011_Heliconius_erato_dignus\n",
      "\t\tProto:46 but_011_Heliconius_erato_dignus:(0.9354) \n",
      "\t\tProto:47 but_011_Heliconius_erato_dignus:(0.9996) \n",
      "\t Child: but_016_Heliconius_erato_phyllis\n",
      "\t\tProto:50 but_016_Heliconius_erato_phyllis:(1.0) \n",
      "\t\tProto:54 but_016_Heliconius_erato_phyllis:(0.999) \n",
      "\t\tProto:55 but_016_Heliconius_erato_phyllis:(0.9994) \n",
      "\t\tProto:56 but_016_Heliconius_erato_phyllis:(0.9954) \n",
      "\t\tProto:58 but_016_Heliconius_erato_phyllis:(0.9966) \n",
      "\t Child: but_008_Heliconius_erato_chestertonii\n",
      "\t\tProto:67 but_008_Heliconius_erato_chestertonii:(0.9975) \n",
      "\t\tProto:60 but_008_Heliconius_erato_chestertonii:(0.9999) \n",
      "\t\tProto:61 but_008_Heliconius_erato_chestertonii:(1.0) \n",
      "\t\tProto:62 but_008_Heliconius_erato_chestertonii:(0.9994) \n",
      "\t Child: but_009_Heliconius_erato_cyrbia\n",
      "\t\tProto:72 but_009_Heliconius_erato_cyrbia:(0.9997) \n",
      "\t\tProto:73 but_009_Heliconius_erato_cyrbia:(1.0) \n",
      "\t\tProto:74 but_009_Heliconius_erato_cyrbia:(0.9997) \n",
      "\t\tProto:79 but_009_Heliconius_erato_cyrbia:(0.9999) \n",
      "\t Child: but_010_Heliconius_erato_demophoon\n",
      "\t\tProto:81 but_010_Heliconius_erato_demophoon:(1.0) \n",
      "\t\tProto:82 but_010_Heliconius_erato_demophoon:(0.9937) \n",
      "\t\tProto:84 but_010_Heliconius_erato_demophoon:(0.9898) \n",
      "\t\tProto:85 but_010_Heliconius_erato_demophoon:(0.9999) \n",
      "\t\tProto:86 but_010_Heliconius_erato_demophoon:(0.9999) \n",
      "\t\tProto:87 but_010_Heliconius_erato_demophoon:(0.9998) \n",
      "\t Child: but_007_Heliconius_erato_amalfreda\n",
      "\t\tProto:96 but_007_Heliconius_erato_amalfreda:(0.9997) \n",
      "\t\tProto:92 but_007_Heliconius_erato_amalfreda:(1.0) \n",
      "\t\tProto:94 but_007_Heliconius_erato_amalfreda:(0.9992) \n",
      "\t Child: but_012_Heliconius_erato_erato\n",
      "\t\tProto:104 but_012_Heliconius_erato_erato:(1.0) \n",
      "\t\tProto:103 but_012_Heliconius_erato_erato:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 33/33 [00:01<00:00, 28.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 025+022\n",
      "\t Child: but_025_Heliconius_melpomene_rosina\n",
      "\t\tProto:1 but_025_Heliconius_melpomene_rosina:(1.0) \n",
      "\t\tProto:9 but_025_Heliconius_melpomene_rosina:(1.0) \n",
      "\t Child: but_022_Heliconius_melpomene_melpomene\n",
      "\t\tProto:17 but_022_Heliconius_melpomene_melpomene:(0.9999) \n",
      "\t\tProto:18 but_022_Heliconius_melpomene_melpomene:(1.0) \n",
      "\t\tProto:11 but_022_Heliconius_melpomene_melpomene:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 30/30 [00:01<00:00, 25.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 001+003+002\n",
      "\t Child: but_001_Heliconius_cydno_alithea\n",
      "\t\tProto:0 but_001_Heliconius_cydno_alithea:(0.9998) \n",
      "\t\tProto:5 but_001_Heliconius_cydno_alithea:(0.9906) \n",
      "\t\tProto:6 but_001_Heliconius_cydno_alithea:(0.9942) \n",
      "\t Child: but_003_Heliconius_cydno_cydnides\n",
      "\t\tProto:11 but_003_Heliconius_cydno_cydnides:(0.9997) \n",
      "\t\tProto:12 but_003_Heliconius_cydno_cydnides:(0.9999) \n",
      "\t\tProto:13 but_003_Heliconius_cydno_cydnides:(1.0) \n",
      "\t\tProto:15 but_003_Heliconius_cydno_cydnides:(0.9967) \n",
      "\t Child: but_002_Heliconius_cydno_chioneus\n",
      "\t\tProto:20 but_002_Heliconius_cydno_chioneus:(0.9966) \n",
      "\t\tProto:21 but_002_Heliconius_cydno_chioneus:(0.9998) \n",
      "\t\tProto:22 but_002_Heliconius_cydno_chioneus:(0.9999) \n",
      "\t\tProto:23 but_002_Heliconius_cydno_chioneus:(1.0) \n",
      "\t\tProto:25 but_002_Heliconius_cydno_chioneus:(1.0) \n",
      "\t\tProto:26 but_002_Heliconius_cydno_chioneus:(0.9999) \n",
      "\t\tProto:27 but_002_Heliconius_cydno_chioneus:(0.9998) \n",
      "\t\tProto:28 but_002_Heliconius_cydno_chioneus:(0.9999) \n",
      "\t\tProto:29 but_002_Heliconius_cydno_chioneus:(0.9206) \n",
      "Done !!!\n"
     ]
    }
   ],
   "source": [
    "# Proto activations on leaf descendents - topk images\n",
    "\n",
    "def get_heatmap(latent_activation, input_image, constant_color_scale=False):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    # image_a = (image_a - image_a.min()) / (image_a.max() - image_a.min())\n",
    "\n",
    "    # input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "    image_b = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((input_image.shape[-1], input_image.shape[-1])))\n",
    "\n",
    "    if constant_color_scale:\n",
    "        reshaped_image_a = np.concatenate((reshaped_image_a, np.zeros((reshaped_image_a.shape[1], 1)), np.ones((reshaped_image_a.shape[1], 1))*255), axis=1)\n",
    "    \n",
    "    normalized_heatmap = (reshaped_image_a - np.min(reshaped_image_a)) / (np.max(reshaped_image_a) - np.min(reshaped_image_a))\n",
    "\n",
    "    heatmap_colormap = plt.get_cmap('jet')\n",
    "    # heatmap_colormap = plt.get_cmap('rainbow')\n",
    "    heatmap_colored = heatmap_colormap(normalized_heatmap)\n",
    "\n",
    "    if constant_color_scale:\n",
    "        heatmap_colored = heatmap_colored[:, :-2]\n",
    "    \n",
    "    heatmap_colored_uint8 = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_a_heatmap_pillow = Image.fromarray(heatmap_colored_uint8)\n",
    "    image_b_pillow = Image.fromarray((image_b * 255).astype('uint8'))\n",
    "    \n",
    "    result_image = Image.blend(image_b_pillow, image_a_heatmap_pillow, alpha=0.3)\n",
    "    \n",
    "    return np.array(result_image)\n",
    "\n",
    "def get_heatmap_uninterpolated(latent_activation, input_image):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    image_a = (image_a - image_a.min()) / (image_a.max() - image_a.min())\n",
    "\n",
    "    input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "    image_b = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((input_image.shape[-1], input_image.shape[-1]), \\\n",
    "                                                                                          resample=Image.NEAREST ))\n",
    "    normalized_heatmap = (reshaped_image_a - np.min(reshaped_image_a)) / (np.max(reshaped_image_a) - np.min(reshaped_image_a))\n",
    "    \n",
    "    heatmap_colormap = plt.get_cmap('jet')\n",
    "    heatmap_colored = heatmap_colormap(normalized_heatmap)\n",
    "    \n",
    "    heatmap_colored_uint8 = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_a_heatmap_pillow = Image.fromarray(heatmap_colored_uint8)\n",
    "    image_b_pillow = Image.fromarray((image_b * 255).astype('uint8'))\n",
    "    \n",
    "    result_image = Image.blend(image_b_pillow, image_a_heatmap_pillow, alpha=0.3)\n",
    "    \n",
    "    return np.array(result_image)\n",
    "\n",
    "from util.data import ModifiedLabelLoader\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import pdb\n",
    "from util.vis_pipnet import get_img_coordinates\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import ImageFont, Image, ImageDraw as D\n",
    "import torchvision\n",
    "from datetime import datetime\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import math\n",
    "# txt_file = open(os.path.join(run_path, \"num_proto_details_\"+datetime.now().strftime(\"%m:%d:%H:%M:%S\")+\".txt\"), \"a\")\n",
    "# txt_file.write('\\n')\n",
    "\n",
    "vizloader_name = 'testloader' # projectloader\n",
    "find_non_descendants = False # True, False # param\n",
    "topk = 3\n",
    "save_images = True # True, False\n",
    "font = ImageFont.truetype(\"arial.ttf\", 50)\n",
    "save_activation_as_npy_path = None # 'activation_as_npy'\n",
    "if (save_activation_as_npy_path is not None):\n",
    "    save_activation_as_npy_path = '_'.join(['activation_as_npy', vizloader_name])  # activation_as_npy, added for NUMPY SAVING\n",
    "if find_non_descendants and (save_activation_as_npy_path is not None):\n",
    "    save_activation_as_npy_path = save_activation_as_npy_path + '_non_desc'\n",
    "plot_overspecificity_score = True\n",
    "subtree_root = root#.get_node('024+051')\n",
    "    \n",
    "from datetime import datetime\n",
    "# txt_file = open(os.path.join(run_path, \"num_proto_details_\"+datetime.now().strftime(\"%m:%d:%H:%M:%S\")+\".txt\"), \"a\")\n",
    "# txt_file.write('\\n')\n",
    "\n",
    "def write_num_proto_details(proto_mean_activations, node_name, net, threshold, txt_file, args):\n",
    "    \n",
    "    rand_input = torch.randn((1, 3, args.image_size, args.image_size))\n",
    "    with torch.no_grad():\n",
    "        *_, pooled, out = net(rand_input)\n",
    "    num_protos = pooled[node_name].shape[1]\n",
    "    used_protos = len(proto_mean_activations)\n",
    "    non_overspecific = 0\n",
    "    for p in proto_mean_activations:\n",
    "        logstr = '\\t'*2 + f'Proto:{p} '\n",
    "        protos_mean_for_all_leaf_descedants = []\n",
    "        for leaf_descendent in proto_mean_activations[p]:\n",
    "            mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "            protos_mean_for_all_leaf_descedants.append(mean_activation)\n",
    "            \n",
    "        if all([(mean_activation>0.2) for mean_activation in protos_mean_for_all_leaf_descedants]):\n",
    "            non_overspecific += 1\n",
    "            \n",
    "    txt_file.write(f\"Node:{node_name},Total:{num_protos},Used:{used_protos},Good:{non_overspecific},threshold={threshold}\\n\")\n",
    "\n",
    "\n",
    "def get_heap():\n",
    "    list_ = []\n",
    "    heapq.heapify(list_)\n",
    "    return list_\n",
    "\n",
    "patchsize, skip = get_patch_size(args)\n",
    "\n",
    "\n",
    "vizloader_dict = {'trainloader': trainloader,\n",
    "                 'projectloader': projectloader,\n",
    "                 'testloader': testloader,\n",
    "                 'test_projectloader': test_projectloader}\n",
    "vizloader_dict[vizloader_name] = unshuffle_dataloader(vizloader_dict[vizloader_name])\n",
    "\n",
    "\n",
    "if type(vizloader_dict[vizloader_name].dataset) == ImageFolder:\n",
    "    name2label = vizloader_dict[vizloader_name].dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "else:\n",
    "    name2label = vizloader_dict[vizloader_name].dataset.dataset.dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "    \n",
    "overspecificity_score_and_proto_mask = []\n",
    "\n",
    "for node in root.nodes_with_children():\n",
    "#     if node.name == 'root':\n",
    "#         continue\n",
    "#     non_leaf_children_names = [child.name for child in node.children if not child.is_leaf()]\n",
    "#     if len(non_leaf_children_names) == 0: # if all the children are leaf nodes then skip this node\n",
    "#         continue\n",
    "\n",
    "    if (node.name not in subtree_root.descendents) and (node.name != subtree_root.name):\n",
    "        print('Skipping node', node.name)\n",
    "        continue\n",
    "\n",
    "    name2label = vizloader_dict[vizloader_name].dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "    modifiedLabelLoader = ModifiedLabelLoader(vizloader_dict[vizloader_name], node)\n",
    "    coarse_label2name = modifiedLabelLoader.modifiedlabel2name\n",
    "    node_label_to_children = {label: name for name, label in node.children_to_labels.items()}\n",
    "    \n",
    "    imgs = modifiedLabelLoader.filtered_imgs\n",
    "\n",
    "    img_iter = tqdm(enumerate(modifiedLabelLoader),\n",
    "                    total=len(modifiedLabelLoader),\n",
    "                    mininterval=50.,\n",
    "                    desc='Collecting topk',\n",
    "                    ncols=0)\n",
    "\n",
    "    classification_weights = getattr(net.module, '_'+node.name+'_classification').weight\n",
    "    \n",
    "    # maps proto_number -> grand_child_name (or descendant leaf name) -> list of top-k activations\n",
    "    proto_mean_activations = defaultdict(lambda: defaultdict(get_heap))\n",
    "\n",
    "    # maps class names to the prototypes that belong to that\n",
    "    class_and_prototypes = defaultdict(set)\n",
    "\n",
    "    for i, (xs, orig_y, ys) in img_iter:\n",
    "#         if coarse_label2name[ys.item()] not in non_leaf_children_names:\n",
    "#             continue\n",
    "\n",
    "        xs, ys = xs.to(device), ys.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output = net(xs, inference=False)\n",
    "            if len(model_output) == 3:\n",
    "                softmaxes, pooled, _ = model_output\n",
    "            elif len(model_output) == 4:\n",
    "                _, softmaxes, pooled, _ = model_output\n",
    "            pooled = pooled[node.name].squeeze(0) \n",
    "            softmaxes = softmaxes[node.name]#.squeeze(0)\n",
    "\n",
    "            for p in range(pooled.shape[0]): # pooled.shape -> [768] (== num of prototypes)\n",
    "                c_weight = torch.max(classification_weights[:,p]) # classification_weights[:,p].shape -> [200] (== num of classes)\n",
    "                relevant_proto_classes = torch.nonzero(classification_weights[:, p] > 1e-3)\n",
    "                relevant_proto_class_names = [node_label_to_children[class_idx.item()] for class_idx in relevant_proto_classes]\n",
    "                \n",
    "                # Take the max per prototype.                             \n",
    "                max_per_prototype, max_idx_per_prototype = torch.max(softmaxes, dim=0)\n",
    "                max_per_prototype_h, max_idx_per_prototype_h = torch.max(max_per_prototype, dim=1)\n",
    "                max_per_prototype_w, max_idx_per_prototype_w = torch.max(max_per_prototype_h, dim=1) #shape (num_prototypes)\n",
    "                \n",
    "                h_idx = max_idx_per_prototype_h[p, max_idx_per_prototype_w[p]]\n",
    "                w_idx = max_idx_per_prototype_w[p]\n",
    "\n",
    "                if len(relevant_proto_class_names) == 0:\n",
    "                    continue\n",
    "                \n",
    "#                 if (len(relevant_proto_class_names) == 1) and (relevant_proto_class_names[0] not in non_leaf_children_names):\n",
    "#                     continue\n",
    "                \n",
    "                h_coor_min, h_coor_max, w_coor_min, w_coor_max = get_img_coordinates(args.image_size, softmaxes.shape, patchsize, skip, h_idx, w_idx)\n",
    "                latent_activation = softmaxes[:, p, :, :]\n",
    "                \n",
    "                if not find_non_descendants:\n",
    "                    if (coarse_label2name[ys.item()] in relevant_proto_class_names):\n",
    "                        child_node = root.get_node(coarse_label2name[ys.item()])\n",
    "                        leaf_descendent = label2name[orig_y.item()]#[4:7]\n",
    "                        img_to_open = imgs[i][0] # it is a tuple of (path to image, lable)\n",
    "                        if topk and (len(proto_mean_activations[p][leaf_descendent]) >= topk):\n",
    "                            heapq.heappushpop(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                              (pooled[p].item(), img_to_open,\\\n",
    "                                               (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "                        else:\n",
    "                            heapq.heappush(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                           (pooled[p].item(), img_to_open,\\\n",
    "                                            (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "                else:\n",
    "                    if (coarse_label2name[ys.item()] not in relevant_proto_class_names):\n",
    "                        child_node = root.get_node(coarse_label2name[ys.item()])\n",
    "                        leaf_descendent = label2name[orig_y.item()]#[4:7]\n",
    "                        img_to_open = imgs[i][0] # it is a tuple of (path to image, lable)\n",
    "                        if topk and (len(proto_mean_activations[p][leaf_descendent]) >= topk):\n",
    "                            heapq.heappushpop(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                              (pooled[p].item(), img_to_open,\\\n",
    "                                               (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "                        else:\n",
    "                            heapq.heappush(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                           (pooled[p].item(), img_to_open,\\\n",
    "                                            (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "\n",
    "                class_and_prototypes[', '.join(relevant_proto_class_names)].add(p)\n",
    "\n",
    "    # write_num_proto_details(proto_mean_activations, node.name, net, threshold=0.2, txt_file=txt_file, args=args)\n",
    "\n",
    "    if plot_overspecificity_score:\n",
    "        for child_classname in class_and_prototypes:\n",
    "            for p in class_and_prototypes[child_classname]:\n",
    "                mean_activation_of_every_leaf = []\n",
    "                for leaf_descendent in proto_mean_activations[p]:\n",
    "                    mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "                    mean_activation_of_every_leaf.append(mean_activation)\n",
    "\n",
    "                overspecificity_score = 1\n",
    "                for mean_act in mean_activation_of_every_leaf:\n",
    "                    overspecificity_score *= mean_act * 1.0\n",
    "                proto_presence = getattr(net.module, '_'+node.name+'_proto_presence')\n",
    "                proto_presence = F.gumbel_softmax(proto_presence, tau=0.5, hard=True, dim=-1)\n",
    "                proto_mask = proto_presence[p, 1].item()\n",
    "                overspecificity_score_and_proto_mask.append((overspecificity_score, len(mean_activation_of_every_leaf), proto_mask))\n",
    "\n",
    "    print('Node', node.name)\n",
    "    for child_classname in class_and_prototypes:\n",
    "        \n",
    "        print('\\t'*1, 'Child:', child_classname)\n",
    "        for p in class_and_prototypes[child_classname]:\n",
    "            \n",
    "            logstr = '\\t'*2 + f'Proto:{p} '\n",
    "            mean_activation_of_every_leaf = []\n",
    "            for leaf_descendent in proto_mean_activations[p]:\n",
    "                mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "                num_images = len(proto_mean_activations[p][leaf_descendent])\n",
    "                logstr += f'{leaf_descendent}:({mean_activation}) '\n",
    "                mean_activation_of_every_leaf.append(mean_activation)\n",
    "            print(logstr)\n",
    "            \n",
    "            # # if the mean_activation is less for all leaf descendants skip the node\n",
    "            # if all([mean_act < 0.2 for mean_act in mean_activation_of_every_leaf]):\n",
    "            #     if find_non_descendants:\n",
    "            #         print('\\t'*2 + f'Not skipping proto {p} of {node.name} coz of find_non_descendants')\n",
    "            #     else:\n",
    "            #         print('\\t'*2 + f'Skipping proto {p} of {node.name}')\n",
    "            #         continue\n",
    "            \n",
    "            # have this for NON descendants\n",
    "            if len(proto_mean_activations[p]) == 0:\n",
    "                continue\n",
    "            \n",
    "            if save_images or save_activation_as_npy_path:\n",
    "                patches = []\n",
    "                right_descriptions = []\n",
    "                text_region_width = 3 # 3x the width of a patch\n",
    "\n",
    "                font_size = 40\n",
    "                fnt = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "                max_width = ImageDraw.Draw(Image.new(\"RGB\", (100, 100), (255, 0, 0))).textlength('-', font=fnt)\n",
    "                \n",
    "                for leaf_descendent in proto_mean_activations[p]:\n",
    "                    for word in leaf_descendent.split('_')[2:]:\n",
    "                        width_of_word = ImageDraw.Draw(Image.new(\"RGB\", (100, 100), (255, 0, 0))).textlength(word, font=fnt)\n",
    "                        max_width = max(max_width, width_of_word)\n",
    "\n",
    "                for leaf_descendent, heap in proto_mean_activations[p].items():\n",
    "                    # if 'BUT' in args.dataset:\n",
    "                    #     species_name = ' '.join(leaf_descendent.split('_')[2:4])\n",
    "                    # else:\n",
    "                    species_name = ' '.join(leaf_descendent.split('_')[2:])\n",
    "                    heap = sorted(heap)[::-1]\n",
    "                    mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "                    for rank, ele in enumerate(heap):\n",
    "                        activation, img_to_open, (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation = ele\n",
    "                        image = transforms.Resize(size=(args.image_size, args.image_size))(Image.open(img_to_open))\n",
    "                        img_tensor = transforms.ToTensor()(image)#.unsqueeze_(0) #shape (1, 3, h, w)\n",
    "\n",
    "                        overlayed_image_np = get_heatmap(latent_activation, img_tensor, constant_color_scale=True)\n",
    "                        overlayed_image = torch.tensor(overlayed_image_np).permute(2, 0, 1).float() / 255.\n",
    "                        \n",
    "                        reshaped_latent_activation = np.array(Image.fromarray((latent_activation.cpu().numpy()[0] * 255).astype('uint8')).resize((img_tensor.shape[-1], img_tensor.shape[-1])))\n",
    "                        center = np.unravel_index(np.argmax(reshaped_latent_activation), reshaped_latent_activation.shape)\n",
    "                        # center = ((h_coor_min + h_coor_max) / 2., (w_coor_min + w_coor_max) / 2.)\n",
    "                        patch_size = 64\n",
    "                        h_coor_min = int(max(0, center[0] - (patch_size/2.)))\n",
    "                        h_coor_max = int(min(img_tensor.shape[1], center[0] + (patch_size/2.)))\n",
    "                        w_coor_min = int(max(0, center[1] - (patch_size/2.)))\n",
    "                        w_coor_max = int(min(img_tensor.shape[2], center[1] + (patch_size/2.)))\n",
    "                        img_tensor_patch = img_tensor[:, h_coor_min:h_coor_max, w_coor_min:w_coor_max]\n",
    "\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        # overlayed_image = img_tensor\n",
    "\n",
    "                        \n",
    "\n",
    "                        scale_factor = 1.7  # 70% increase\n",
    "\n",
    "                        heatmap_patch = overlayed_image[:, h_coor_min:h_coor_max, w_coor_min:w_coor_max]\n",
    "                        resized_heatmap_patch = F.interpolate(heatmap_patch.unsqueeze(0), scale_factor=scale_factor, \\\n",
    "                                                      mode='bilinear', align_corners=False).squeeze(0)\n",
    "                        resized_heatmap_patch = torchvision.utils.draw_bounding_boxes((resized_heatmap_patch * 255).to(torch.uint8), \\\n",
    "                                                                                torch.tensor([[0, 0, resized_heatmap_patch.shape[2], resized_heatmap_patch.shape[1]]]), \\\n",
    "                                                                                width=4, colors=(255, 0, 0))\n",
    "                        resized_heatmap_patch = resized_heatmap_patch.float() / 255.\n",
    "                        \n",
    "                        resized_img_patch = F.interpolate(img_tensor_patch.unsqueeze(0), scale_factor=scale_factor, \\\n",
    "                                                      mode='bilinear', align_corners=False).squeeze(0)\n",
    "                        resized_img_patch = torchvision.utils.draw_bounding_boxes((resized_img_patch * 255).to(torch.uint8), \\\n",
    "                                                                                torch.tensor([[0, 0, resized_img_patch.shape[2], resized_img_patch.shape[1]]]), \\\n",
    "                                                                                width=4, colors=(255, 255, 0))\n",
    "                        resized_img_patch = resized_img_patch.float() / 255.\n",
    "                        \n",
    "                        resized_patch = torchvision.utils.make_grid([resized_img_patch, resized_heatmap_patch], nrow=1, padding=1, pad_value=1., border=1)\n",
    "                        white_image = torch.ones(3, img_tensor.shape[1], img_tensor.shape[2])\n",
    "                        patch_height = resized_patch.shape[1]\n",
    "                        y_start = (white_image.shape[1] - patch_height) // 2                        \n",
    "                        x_start = 10  # 10 pixels from the left\n",
    "                        white_image[:, y_start:y_start+patch_height, x_start:x_start+resized_patch.shape[2]] = resized_patch\n",
    "\n",
    "                        # Bounding box on original image\n",
    "                        img_tensor = torchvision.utils.draw_bounding_boxes((img_tensor * 255).to(torch.uint8), \\\n",
    "                                                                                torch.tensor([[w_coor_min, h_coor_min, w_coor_max, h_coor_max]]), \\\n",
    "                                                                                width=2, colors=(255, 255, 0))\n",
    "                        img_tensor = img_tensor.float() / 255.\n",
    "\n",
    "                        # Bounding box on overlayed image\n",
    "                        overlayed_image = torchvision.utils.draw_bounding_boxes((overlayed_image * 255).to(torch.uint8), \\\n",
    "                                                                                torch.tensor([[w_coor_min, h_coor_min, w_coor_max, h_coor_max]]), \\\n",
    "                                                                                width=2, colors=(255, 0, 0))\n",
    "                        overlayed_image = overlayed_image.float() / 255.\n",
    "\n",
    "                        grid_cell = torchvision.utils.make_grid([overlayed_image, img_tensor, white_image], nrow=3, padding=5, pad_value=1., border=1)\n",
    "\n",
    "                        patches.append(grid_cell)\n",
    "                        \n",
    "                        # added for NUMPY SAVING\n",
    "                        \n",
    "                        if save_activation_as_npy_path:\n",
    "#                             upscaled_similarity_interpolated = get_upscaled_activation_interpolated(latent_activation,\n",
    "#                                                                                        image_size=(args.image_size, args.image_size))\n",
    "                            latent_activation_npy = latent_activation.squeeze().cpu().numpy()\n",
    "                            data = {'node_name': node.name,\n",
    "                                    'proto_num': p,\n",
    "                                    'child_name': child_classname,\n",
    "                                    'leaf_desc': leaf_descendent,\n",
    "                                     'rank': rank,\n",
    "                                     'img_path': img_to_open,\n",
    "                                     'img_filename': ntpath.basename(img_to_open),\n",
    "                                     'activation': latent_activation_npy,\n",
    "                                     'max_activation': activation,\n",
    "                                     'model_type': 'NAIVE-HPIPNET'}\n",
    "                            filename = str(rank)+ '-' + ntpath.basename(img_to_open) + '.npy'\n",
    "                            save_path = os.path.join(run_path, save_activation_as_npy_path, \\\n",
    "                                                     node.name, str(p), leaf_descendent,\n",
    "                                                     filename)\n",
    "                            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                            np.save(save_path, data, allow_pickle=True)\n",
    "\n",
    "                    # # description on the right hand side\n",
    "                    # text = f'{mean_activation}, {leaf_descendent}'\n",
    "                    # txtimage = Image.new(\"RGB\", (patches[0].shape[-2]*text_region_width,patches[0].shape[-1]), (255, 255, 255))\n",
    "                    # draw = D.Draw(txtimage)\n",
    "                    # draw.text((150, patches[0].shape[1]//2), text, anchor='mm', fill=\"black\", font=font)\n",
    "                    # pdb.set_trace()\n",
    "                    # txttensor = transforms.ToTensor()(txtimage)#.unsqueeze_(0)\n",
    "                    # right_descriptions.append(txttensor)\n",
    "\n",
    "                    text = '\\n'.join(species_name.split(' '))\n",
    "                    image_size = (math.ceil(max_width) + 10, patches[0].shape[1])\n",
    "                    txtimage = Image.new(\"RGB\", image_size, (255, 255, 255))\n",
    "                    d = ImageDraw.Draw(txtimage)\n",
    "                    d.multiline_text((image_size[0]/2, image_size[1]/2), text, font=fnt, fill=(0, 0, 0), align =\"center\", anchor=\"mm\")\n",
    "                    txttensor = transforms.ToTensor()(txtimage)#.unsqueeze_(0)\n",
    "                    right_descriptions.append(txttensor)\n",
    "                    \n",
    "\n",
    "                padding = 0\n",
    "\n",
    "                # grid = torchvision.utils.make_grid(patches, nrow=topk, padding=padding, border=0)\n",
    "                # grid_right_descriptions = torchvision.utils.make_grid(right_descriptions, nrow=1, padding=padding, border=0)\n",
    "                # grid = torch.cat([grid_right_descriptions, grid], dim=-1)\n",
    "\n",
    "                grid_rows = []\n",
    "                for k in range(len(proto_mean_activations[p])):\n",
    "                    grid_row = torchvision.utils.make_grid(patches[k*topk:(k+1)*topk], nrow=topk, padding=padding, border=0)\n",
    "                    grid_right_description = torchvision.utils.make_grid(right_descriptions[k], nrow=1, padding=padding, border=0)\n",
    "                    # pdb.set_trace()\n",
    "                    grid_row = torch.cat([grid_right_description, grid_row], dim=-1)\n",
    "                    grid_rows.append(grid_row)\n",
    "                # grid = torch.cat(grid_rows, dim=0)\n",
    "                grid = torchvision.utils.make_grid(grid_rows, nrow=1, padding=5, pad_value=1.)\n",
    "                    \n",
    "                # # description on the top\n",
    "                # text = f'Node:{node.name}, p{p}, Child:{child_classname}'\n",
    "                # txtimage = Image.new(\"RGB\", (grid.shape[-1], args.wshape), (0, 0, 0))\n",
    "                # draw = D.Draw(txtimage)\n",
    "                # draw.text((150, patches[0].shape[1]//2), text, anchor='mm', fill=\"white\", font=font)\n",
    "                # txttensor = transforms.ToTensor()(txtimage)#.unsqueeze_(0)\n",
    "\n",
    "                # merging top description with the grid of images\n",
    "                # grid = torch.cat([grid, txttensor], dim=1)\n",
    "                \n",
    "                if save_images:\n",
    "                    prefix = 'non_' if find_non_descendants else ''\n",
    "                    os.makedirs(os.path.join(run_path, prefix+f'descendent_specific_topk_heatmap_withbb_ep={epoch}_{subtree_root.name}', node.name), exist_ok=True)\n",
    "                    torchvision.utils.save_image(grid, os.path.join(run_path, prefix+f'descendent_specific_topk_heatmap_withbb_ep={epoch}_{subtree_root.name}', node.name, f'{child_classname}-p{p}.png'), border=0) # , border_color=(255, 255, 255), border=10\n",
    "\n",
    "# txt_file.write('\\n')\n",
    "# txt_file.close()\n",
    "print('Done !!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With only heatmpa, Color zoom in and heatmpa zoom in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping node root\n",
      "Skipping node 129+024+067\n",
      "Skipping node 089+046\n",
      "Skipping node 129+065\n",
      "Skipping node 024+051\n",
      "Skipping node 067+070\n",
      "Skipping node 089+090\n",
      "Skipping node 046+087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 445it [7:32:42, 61.04s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 171\u001b[0m\n\u001b[1;32m    168\u001b[0m xs, ys \u001b[38;5;241m=\u001b[39m xs\u001b[38;5;241m.\u001b[39mto(device), ys\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 171\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model_output) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    173\u001b[0m         softmaxes, pooled, _ \u001b[38;5;241m=\u001b[39m model_output\n",
      "File \u001b[0;32m~/.conda/envs/hpnet4/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/hpnet4/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:169\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m ({},)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[1;32m    171\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, kwargs)\n",
      "File \u001b[0;32m~/.conda/envs/hpnet4/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/PIPNet_wandb/runs/178-PruningNaiveHPIPNetMaskL1=0.5MaskTrainExtra=15epsEps=60TanhDesc=0.05MinCont=0.1_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=20/source_clone/pipnet/pipnet.py:139\u001b[0m, in \u001b[0;36mPIPNet.forward\u001b[0;34m(self, xs, inference, apply_overspecificity_mask)\u001b[0m\n\u001b[1;32m    136\u001b[0m     cosine_similarity \u001b[38;5;241m=\u001b[39m functional_UnitConv2D(proto_layer_input_features, prototypes\u001b[38;5;241m.\u001b[39mweight, prototypes\u001b[38;5;241m.\u001b[39mbias)\n\u001b[1;32m    137\u001b[0m     proto_features[node\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m cosine_similarity \u001b[38;5;241m*\u001b[39m proto_features_softmaxed[node\u001b[38;5;241m.\u001b[39mname]\n\u001b[0;32m--> 139\u001b[0m pooled[node\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproto_features\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mfocal \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    142\u001b[0m     pooled[node\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m pooled[node\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_avg_pool(proto_features[node\u001b[38;5;241m.\u001b[39mname])\n",
      "File \u001b[0;32m~/.conda/envs/hpnet4/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/hpnet4/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/hpnet4/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/hpnet4/lib/python3.9/site-packages/torch/nn/modules/flatten.py:46\u001b[0m, in \u001b[0;36mFlatten.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Proto activations on leaf descendents - topk images\n",
    "\n",
    "def get_heatmap(latent_activation, input_image, constant_color_scale=False):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    # image_a = (image_a - image_a.min()) / (image_a.max() - image_a.min())\n",
    "\n",
    "    # input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "    image_b = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((input_image.shape[-1], input_image.shape[-1])))\n",
    "\n",
    "    if constant_color_scale:\n",
    "        reshaped_image_a = np.concatenate((reshaped_image_a, np.zeros((reshaped_image_a.shape[1], 1)), np.ones((reshaped_image_a.shape[1], 1))*255), axis=1)\n",
    "    \n",
    "    normalized_heatmap = (reshaped_image_a - np.min(reshaped_image_a)) / (np.max(reshaped_image_a) - np.min(reshaped_image_a))\n",
    "\n",
    "    heatmap_colormap = plt.get_cmap('jet')\n",
    "    # heatmap_colormap = plt.get_cmap('rainbow')\n",
    "    heatmap_colored = heatmap_colormap(normalized_heatmap)\n",
    "\n",
    "    if constant_color_scale:\n",
    "        heatmap_colored = heatmap_colored[:, :-2]\n",
    "    \n",
    "    heatmap_colored_uint8 = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_a_heatmap_pillow = Image.fromarray(heatmap_colored_uint8)\n",
    "    image_b_pillow = Image.fromarray((image_b * 255).astype('uint8'))\n",
    "    \n",
    "    result_image = Image.blend(image_b_pillow, image_a_heatmap_pillow, alpha=0.3)\n",
    "    \n",
    "    return np.array(result_image)\n",
    "\n",
    "def get_heatmap_uninterpolated(latent_activation, input_image):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    image_a = (image_a - image_a.min()) / (image_a.max() - image_a.min())\n",
    "\n",
    "    input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "    image_b = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((input_image.shape[-1], input_image.shape[-1]), \\\n",
    "                                                                                          resample=Image.NEAREST ))\n",
    "    normalized_heatmap = (reshaped_image_a - np.min(reshaped_image_a)) / (np.max(reshaped_image_a) - np.min(reshaped_image_a))\n",
    "    \n",
    "    heatmap_colormap = plt.get_cmap('jet')\n",
    "    heatmap_colored = heatmap_colormap(normalized_heatmap)\n",
    "    \n",
    "    heatmap_colored_uint8 = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_a_heatmap_pillow = Image.fromarray(heatmap_colored_uint8)\n",
    "    image_b_pillow = Image.fromarray((image_b * 255).astype('uint8'))\n",
    "    \n",
    "    result_image = Image.blend(image_b_pillow, image_a_heatmap_pillow, alpha=0.3)\n",
    "    \n",
    "    return np.array(result_image)\n",
    "\n",
    "from util.data import ModifiedLabelLoader\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import pdb\n",
    "from util.vis_pipnet import get_img_coordinates\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import ImageFont, Image, ImageDraw as D\n",
    "import torchvision\n",
    "from datetime import datetime\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import math\n",
    "# txt_file = open(os.path.join(run_path, \"num_proto_details_\"+datetime.now().strftime(\"%m:%d:%H:%M:%S\")+\".txt\"), \"a\")\n",
    "# txt_file.write('\\n')\n",
    "\n",
    "vizloader_name = 'testloader' # projectloader\n",
    "find_non_descendants = False # True, False # param\n",
    "topk = 3\n",
    "save_images = True # True, False\n",
    "font = ImageFont.truetype(\"arial.ttf\", 50)\n",
    "save_activation_as_npy_path = None # 'activation_as_npy'\n",
    "if (save_activation_as_npy_path is not None):\n",
    "    save_activation_as_npy_path = '_'.join(['activation_as_npy', vizloader_name])  # activation_as_npy, added for NUMPY SAVING\n",
    "if find_non_descendants and (save_activation_as_npy_path is not None):\n",
    "    save_activation_as_npy_path = save_activation_as_npy_path + '_non_desc'\n",
    "plot_overspecificity_score = True\n",
    "# subtree_root = root.get_node('024+051')\n",
    "subtree_root = root.get_node('129+192')\n",
    "    \n",
    "from datetime import datetime\n",
    "# txt_file = open(os.path.join(run_path, \"num_proto_details_\"+datetime.now().strftime(\"%m:%d:%H:%M:%S\")+\".txt\"), \"a\")\n",
    "# txt_file.write('\\n')\n",
    "\n",
    "def write_num_proto_details(proto_mean_activations, node_name, net, threshold, txt_file, args):\n",
    "    \n",
    "    rand_input = torch.randn((1, 3, args.image_size, args.image_size))\n",
    "    with torch.no_grad():\n",
    "        *_, pooled, out = net(rand_input)\n",
    "    num_protos = pooled[node_name].shape[1]\n",
    "    used_protos = len(proto_mean_activations)\n",
    "    non_overspecific = 0\n",
    "    for p in proto_mean_activations:\n",
    "        logstr = '\\t'*2 + f'Proto:{p} '\n",
    "        protos_mean_for_all_leaf_descedants = []\n",
    "        for leaf_descendent in proto_mean_activations[p]:\n",
    "            mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "            protos_mean_for_all_leaf_descedants.append(mean_activation)\n",
    "            \n",
    "        if all([(mean_activation>0.2) for mean_activation in protos_mean_for_all_leaf_descedants]):\n",
    "            non_overspecific += 1\n",
    "            \n",
    "    txt_file.write(f\"Node:{node_name},Total:{num_protos},Used:{used_protos},Good:{non_overspecific},threshold={threshold}\\n\")\n",
    "\n",
    "\n",
    "def get_heap():\n",
    "    list_ = []\n",
    "    heapq.heapify(list_)\n",
    "    return list_\n",
    "\n",
    "patchsize, skip = get_patch_size(args)\n",
    "\n",
    "\n",
    "vizloader_dict = {'trainloader': trainloader,\n",
    "                 'projectloader': projectloader,\n",
    "                 'testloader': testloader,\n",
    "                 'test_projectloader': test_projectloader}\n",
    "vizloader_dict[vizloader_name] = unshuffle_dataloader(vizloader_dict[vizloader_name])\n",
    "\n",
    "\n",
    "if type(vizloader_dict[vizloader_name].dataset) == ImageFolder:\n",
    "    name2label = vizloader_dict[vizloader_name].dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "else:\n",
    "    name2label = vizloader_dict[vizloader_name].dataset.dataset.dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "    \n",
    "overspecificity_score_and_proto_mask = []\n",
    "\n",
    "for node in root.nodes_with_children():\n",
    "#     if node.name == 'root':\n",
    "#         continue\n",
    "#     non_leaf_children_names = [child.name for child in node.children if not child.is_leaf()]\n",
    "#     if len(non_leaf_children_names) == 0: # if all the children are leaf nodes then skip this node\n",
    "#         continue\n",
    "\n",
    "    if (node.name not in subtree_root.descendents) and (node.name != subtree_root.name):\n",
    "        print('Skipping node', node.name)\n",
    "        continue\n",
    "\n",
    "    name2label = vizloader_dict[vizloader_name].dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "    modifiedLabelLoader = ModifiedLabelLoader(vizloader_dict[vizloader_name], node)\n",
    "    coarse_label2name = modifiedLabelLoader.modifiedlabel2name\n",
    "    node_label_to_children = {label: name for name, label in node.children_to_labels.items()}\n",
    "    \n",
    "    imgs = modifiedLabelLoader.filtered_imgs\n",
    "\n",
    "    img_iter = tqdm(enumerate(modifiedLabelLoader),\n",
    "                    total=len(modifiedLabelLoader),\n",
    "                    mininterval=50.,\n",
    "                    desc='Collecting topk',\n",
    "                    ncols=0)\n",
    "\n",
    "    classification_weights = getattr(net.module, '_'+node.name+'_classification').weight\n",
    "    \n",
    "    # maps proto_number -> grand_child_name (or descendant leaf name) -> list of top-k activations\n",
    "    proto_mean_activations = defaultdict(lambda: defaultdict(get_heap))\n",
    "\n",
    "    # maps class names to the prototypes that belong to that\n",
    "    class_and_prototypes = defaultdict(set)\n",
    "\n",
    "    for i, (xs, orig_y, ys) in img_iter:\n",
    "#         if coarse_label2name[ys.item()] not in non_leaf_children_names:\n",
    "#             continue\n",
    "\n",
    "        xs, ys = xs.to(device), ys.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output = net(xs, inference=False)\n",
    "            if len(model_output) == 3:\n",
    "                softmaxes, pooled, _ = model_output\n",
    "            elif len(model_output) == 4:\n",
    "                _, softmaxes, pooled, _ = model_output\n",
    "            pooled = pooled[node.name].squeeze(0) \n",
    "            softmaxes = softmaxes[node.name]#.squeeze(0)\n",
    "\n",
    "            for p in range(pooled.shape[0]): # pooled.shape -> [768] (== num of prototypes)\n",
    "                c_weight = torch.max(classification_weights[:,p]) # classification_weights[:,p].shape -> [200] (== num of classes)\n",
    "                relevant_proto_classes = torch.nonzero(classification_weights[:, p] > 1e-3)\n",
    "                relevant_proto_class_names = [node_label_to_children[class_idx.item()] for class_idx in relevant_proto_classes]\n",
    "                \n",
    "                # Take the max per prototype.                             \n",
    "                max_per_prototype, max_idx_per_prototype = torch.max(softmaxes, dim=0)\n",
    "                max_per_prototype_h, max_idx_per_prototype_h = torch.max(max_per_prototype, dim=1)\n",
    "                max_per_prototype_w, max_idx_per_prototype_w = torch.max(max_per_prototype_h, dim=1) #shape (num_prototypes)\n",
    "                \n",
    "                h_idx = max_idx_per_prototype_h[p, max_idx_per_prototype_w[p]]\n",
    "                w_idx = max_idx_per_prototype_w[p]\n",
    "\n",
    "                if len(relevant_proto_class_names) == 0:\n",
    "                    continue\n",
    "                \n",
    "#                 if (len(relevant_proto_class_names) == 1) and (relevant_proto_class_names[0] not in non_leaf_children_names):\n",
    "#                     continue\n",
    "                \n",
    "                h_coor_min, h_coor_max, w_coor_min, w_coor_max = get_img_coordinates(args.image_size, softmaxes.shape, patchsize, skip, h_idx, w_idx)\n",
    "                latent_activation = softmaxes[:, p, :, :]\n",
    "                \n",
    "                if not find_non_descendants:\n",
    "                    if (coarse_label2name[ys.item()] in relevant_proto_class_names):\n",
    "                        child_node = root.get_node(coarse_label2name[ys.item()])\n",
    "                        leaf_descendent = label2name[orig_y.item()]#[4:7]\n",
    "                        img_to_open = imgs[i][0] # it is a tuple of (path to image, lable)\n",
    "                        if topk and (len(proto_mean_activations[p][leaf_descendent]) >= topk):\n",
    "                            heapq.heappushpop(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                              (pooled[p].item(), img_to_open,\\\n",
    "                                               (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "                        else:\n",
    "                            heapq.heappush(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                           (pooled[p].item(), img_to_open,\\\n",
    "                                            (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "                else:\n",
    "                    if (coarse_label2name[ys.item()] not in relevant_proto_class_names):\n",
    "                        child_node = root.get_node(coarse_label2name[ys.item()])\n",
    "                        leaf_descendent = label2name[orig_y.item()]#[4:7]\n",
    "                        img_to_open = imgs[i][0] # it is a tuple of (path to image, lable)\n",
    "                        if topk and (len(proto_mean_activations[p][leaf_descendent]) >= topk):\n",
    "                            heapq.heappushpop(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                              (pooled[p].item(), img_to_open,\\\n",
    "                                               (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "                        else:\n",
    "                            heapq.heappush(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                           (pooled[p].item(), img_to_open,\\\n",
    "                                            (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "\n",
    "                class_and_prototypes[', '.join(relevant_proto_class_names)].add(p)\n",
    "\n",
    "    # write_num_proto_details(proto_mean_activations, node.name, net, threshold=0.2, txt_file=txt_file, args=args)\n",
    "\n",
    "    if plot_overspecificity_score:\n",
    "        for child_classname in class_and_prototypes:\n",
    "            for p in class_and_prototypes[child_classname]:\n",
    "                mean_activation_of_every_leaf = []\n",
    "                for leaf_descendent in proto_mean_activations[p]:\n",
    "                    mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "                    mean_activation_of_every_leaf.append(mean_activation)\n",
    "\n",
    "                overspecificity_score = 1\n",
    "                for mean_act in mean_activation_of_every_leaf:\n",
    "                    overspecificity_score *= mean_act * 1.0\n",
    "                proto_presence = getattr(net.module, '_'+node.name+'_proto_presence')\n",
    "                proto_presence = F.gumbel_softmax(proto_presence, tau=0.5, hard=True, dim=-1)\n",
    "                proto_mask = proto_presence[p, 1].item()\n",
    "                overspecificity_score_and_proto_mask.append((overspecificity_score, len(mean_activation_of_every_leaf), proto_mask))\n",
    "\n",
    "    print('Node', node.name)\n",
    "    for child_classname in class_and_prototypes:\n",
    "        \n",
    "        print('\\t'*1, 'Child:', child_classname)\n",
    "        for p in class_and_prototypes[child_classname]:\n",
    "            \n",
    "            logstr = '\\t'*2 + f'Proto:{p} '\n",
    "            mean_activation_of_every_leaf = []\n",
    "            for leaf_descendent in proto_mean_activations[p]:\n",
    "                mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "                num_images = len(proto_mean_activations[p][leaf_descendent])\n",
    "                logstr += f'{leaf_descendent}:({mean_activation}) '\n",
    "                mean_activation_of_every_leaf.append(mean_activation)\n",
    "            print(logstr)\n",
    "            \n",
    "            # # if the mean_activation is less for all leaf descendants skip the node\n",
    "            # if all([mean_act < 0.2 for mean_act in mean_activation_of_every_leaf]):\n",
    "            #     if find_non_descendants:\n",
    "            #         print('\\t'*2 + f'Not skipping proto {p} of {node.name} coz of find_non_descendants')\n",
    "            #     else:\n",
    "            #         print('\\t'*2 + f'Skipping proto {p} of {node.name}')\n",
    "            #         continue\n",
    "            \n",
    "            # have this for NON descendants\n",
    "            if len(proto_mean_activations[p]) == 0:\n",
    "                continue\n",
    "            \n",
    "            if save_images or save_activation_as_npy_path:\n",
    "                patches = []\n",
    "                right_descriptions = []\n",
    "                text_region_width = 3 # 3x the width of a patch\n",
    "\n",
    "                font_size = 40\n",
    "                fnt = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "                max_width = ImageDraw.Draw(Image.new(\"RGB\", (100, 100), (255, 0, 0))).textlength('-', font=fnt)\n",
    "                \n",
    "                for leaf_descendent in proto_mean_activations[p]:\n",
    "                    for word in leaf_descendent.split('_')[2:]:\n",
    "                        width_of_word = ImageDraw.Draw(Image.new(\"RGB\", (100, 100), (255, 0, 0))).textlength(word, font=fnt)\n",
    "                        max_width = max(max_width, width_of_word)\n",
    "\n",
    "                for leaf_descendent, heap in proto_mean_activations[p].items():\n",
    "                    # if 'BUT' in args.dataset:\n",
    "                    #     species_name = ' '.join(leaf_descendent.split('_')[2:4])\n",
    "                    # else:\n",
    "                    species_name = ' '.join(leaf_descendent.split('_')[2:])\n",
    "                    heap = sorted(heap)[::-1]\n",
    "                    mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "                    for rank, ele in enumerate(heap):\n",
    "                        activation, img_to_open, (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation = ele\n",
    "                        image = transforms.Resize(size=(args.image_size, args.image_size))(Image.open(img_to_open))\n",
    "                        img_tensor = transforms.ToTensor()(image)#.unsqueeze_(0) #shape (1, 3, h, w)\n",
    "\n",
    "                        overlayed_image_np = get_heatmap(latent_activation, img_tensor, constant_color_scale=True)\n",
    "                        overlayed_image = torch.tensor(overlayed_image_np).permute(2, 0, 1).float() / 255.\n",
    "                        \n",
    "                        reshaped_latent_activation = np.array(Image.fromarray((latent_activation.cpu().numpy()[0] * 255).astype('uint8')).resize((img_tensor.shape[-1], img_tensor.shape[-1])))\n",
    "                        center = np.unravel_index(np.argmax(reshaped_latent_activation), reshaped_latent_activation.shape)\n",
    "                        # center = ((h_coor_min + h_coor_max) / 2., (w_coor_min + w_coor_max) / 2.)\n",
    "                        patch_size = 64\n",
    "                        h_coor_min = int(max(0, center[0] - (patch_size/2.)))\n",
    "                        h_coor_max = int(min(img_tensor.shape[1], center[0] + (patch_size/2.)))\n",
    "                        w_coor_min = int(max(0, center[1] - (patch_size/2.)))\n",
    "                        w_coor_max = int(min(img_tensor.shape[2], center[1] + (patch_size/2.)))\n",
    "                        img_tensor_patch = img_tensor[:, h_coor_min:h_coor_max, w_coor_min:w_coor_max]\n",
    "\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        # overlayed_image = img_tensor\n",
    "\n",
    "                        \n",
    "\n",
    "                        scale_factor = 1.7  # 70% increase\n",
    "\n",
    "                        heatmap_patch = overlayed_image[:, h_coor_min:h_coor_max, w_coor_min:w_coor_max]\n",
    "                        resized_heatmap_patch = F.interpolate(heatmap_patch.unsqueeze(0), scale_factor=scale_factor, \\\n",
    "                                                      mode='bilinear', align_corners=False).squeeze(0)\n",
    "                        resized_heatmap_patch = torchvision.utils.draw_bounding_boxes((resized_heatmap_patch * 255).to(torch.uint8), \\\n",
    "                                                                                torch.tensor([[0, 0, resized_heatmap_patch.shape[2], resized_heatmap_patch.shape[1]]]), \\\n",
    "                                                                                width=4, colors=(255, 0, 0))\n",
    "                        resized_heatmap_patch = resized_heatmap_patch.float() / 255.\n",
    "                        \n",
    "                        resized_img_patch = F.interpolate(img_tensor_patch.unsqueeze(0), scale_factor=scale_factor, \\\n",
    "                                                      mode='bilinear', align_corners=False).squeeze(0)\n",
    "                        resized_img_patch = torchvision.utils.draw_bounding_boxes((resized_img_patch * 255).to(torch.uint8), \\\n",
    "                                                                                torch.tensor([[0, 0, resized_img_patch.shape[2], resized_img_patch.shape[1]]]), \\\n",
    "                                                                                width=4, colors=(255, 255, 0))\n",
    "                        resized_img_patch = resized_img_patch.float() / 255.\n",
    "                        \n",
    "                        resized_patch = torchvision.utils.make_grid([resized_img_patch, resized_heatmap_patch], nrow=1, padding=1, pad_value=1., border=1)\n",
    "                        white_image = torch.ones(3, img_tensor.shape[1], img_tensor.shape[2])\n",
    "                        patch_height = resized_patch.shape[1]\n",
    "                        y_start = (white_image.shape[1] - patch_height) // 2                        \n",
    "                        x_start = 10  # 10 pixels from the left\n",
    "                        white_image[:, y_start:y_start+patch_height, x_start:x_start+resized_patch.shape[2]] = resized_patch\n",
    "\n",
    "                        # Bounding box on original image\n",
    "                        img_tensor = torchvision.utils.draw_bounding_boxes((img_tensor * 255).to(torch.uint8), \\\n",
    "                                                                                torch.tensor([[w_coor_min, h_coor_min, w_coor_max, h_coor_max]]), \\\n",
    "                                                                                width=2, colors=(255, 255, 0))\n",
    "                        img_tensor = img_tensor.float() / 255.\n",
    "\n",
    "                        # Bounding box on overlayed image\n",
    "                        overlayed_image = torchvision.utils.draw_bounding_boxes((overlayed_image * 255).to(torch.uint8), \\\n",
    "                                                                                torch.tensor([[w_coor_min, h_coor_min, w_coor_max, h_coor_max]]), \\\n",
    "                                                                                width=2, colors=(255, 0, 0))\n",
    "                        overlayed_image = overlayed_image.float() / 255.\n",
    "\n",
    "                        grid_cell = torchvision.utils.make_grid([overlayed_image, white_image], nrow=2, padding=5, pad_value=1., border=1)\n",
    "\n",
    "                        patches.append(grid_cell)\n",
    "                        \n",
    "                        # added for NUMPY SAVING\n",
    "                        \n",
    "                        if save_activation_as_npy_path:\n",
    "#                             upscaled_similarity_interpolated = get_upscaled_activation_interpolated(latent_activation,\n",
    "#                                                                                        image_size=(args.image_size, args.image_size))\n",
    "                            latent_activation_npy = latent_activation.squeeze().cpu().numpy()\n",
    "                            data = {'node_name': node.name,\n",
    "                                    'proto_num': p,\n",
    "                                    'child_name': child_classname,\n",
    "                                    'leaf_desc': leaf_descendent,\n",
    "                                     'rank': rank,\n",
    "                                     'img_path': img_to_open,\n",
    "                                     'img_filename': ntpath.basename(img_to_open),\n",
    "                                     'activation': latent_activation_npy,\n",
    "                                     'max_activation': activation,\n",
    "                                     'model_type': 'NAIVE-HPIPNET'}\n",
    "                            filename = str(rank)+ '-' + ntpath.basename(img_to_open) + '.npy'\n",
    "                            save_path = os.path.join(run_path, save_activation_as_npy_path, \\\n",
    "                                                     node.name, str(p), leaf_descendent,\n",
    "                                                     filename)\n",
    "                            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                            np.save(save_path, data, allow_pickle=True)\n",
    "\n",
    "                    # # description on the right hand side\n",
    "                    # text = f'{mean_activation}, {leaf_descendent}'\n",
    "                    # txtimage = Image.new(\"RGB\", (patches[0].shape[-2]*text_region_width,patches[0].shape[-1]), (255, 255, 255))\n",
    "                    # draw = D.Draw(txtimage)\n",
    "                    # draw.text((150, patches[0].shape[1]//2), text, anchor='mm', fill=\"black\", font=font)\n",
    "                    # pdb.set_trace()\n",
    "                    # txttensor = transforms.ToTensor()(txtimage)#.unsqueeze_(0)\n",
    "                    # right_descriptions.append(txttensor)\n",
    "\n",
    "                    text = '\\n'.join(species_name.split(' '))\n",
    "                    image_size = (math.ceil(max_width) + 10, patches[0].shape[1])\n",
    "                    txtimage = Image.new(\"RGB\", image_size, (255, 255, 255))\n",
    "                    d = ImageDraw.Draw(txtimage)\n",
    "                    d.multiline_text((image_size[0]/2, image_size[1]/2), text, font=fnt, fill=(0, 0, 0), align =\"center\", anchor=\"mm\")\n",
    "                    txttensor = transforms.ToTensor()(txtimage)#.unsqueeze_(0)\n",
    "                    right_descriptions.append(txttensor)\n",
    "                    \n",
    "\n",
    "                padding = 0\n",
    "\n",
    "                # grid = torchvision.utils.make_grid(patches, nrow=topk, padding=padding, border=0)\n",
    "                # grid_right_descriptions = torchvision.utils.make_grid(right_descriptions, nrow=1, padding=padding, border=0)\n",
    "                # grid = torch.cat([grid_right_descriptions, grid], dim=-1)\n",
    "\n",
    "                grid_rows = []\n",
    "                for k in range(len(proto_mean_activations[p])):\n",
    "                    grid_row = torchvision.utils.make_grid(patches[k*topk:(k+1)*topk], nrow=topk, padding=padding, border=0)\n",
    "                    grid_right_description = torchvision.utils.make_grid(right_descriptions[k], nrow=1, padding=padding, border=0)\n",
    "                    # pdb.set_trace()\n",
    "                    grid_row = torch.cat([grid_right_description, grid_row], dim=-1)\n",
    "                    grid_rows.append(grid_row)\n",
    "                # grid = torch.cat(grid_rows, dim=0)\n",
    "                grid = torchvision.utils.make_grid(grid_rows, nrow=1, padding=5, pad_value=1.)\n",
    "                    \n",
    "                # # description on the top\n",
    "                # text = f'Node:{node.name}, p{p}, Child:{child_classname}'\n",
    "                # txtimage = Image.new(\"RGB\", (grid.shape[-1], args.wshape), (0, 0, 0))\n",
    "                # draw = D.Draw(txtimage)\n",
    "                # draw.text((150, patches[0].shape[1]//2), text, anchor='mm', fill=\"white\", font=font)\n",
    "                # txttensor = transforms.ToTensor()(txtimage)#.unsqueeze_(0)\n",
    "\n",
    "                # merging top description with the grid of images\n",
    "                # grid = torch.cat([grid, txttensor], dim=1)\n",
    "                \n",
    "                if save_images:\n",
    "                    prefix = 'non_' if find_non_descendants else ''\n",
    "                    os.makedirs(os.path.join(run_path, prefix+f'descendent_specific_topk_heatmap_with_heatmap_colorzoomin_heatmapzoomin_ep={epoch}_{subtree_root.name}', node.name), exist_ok=True)\n",
    "                    torchvision.utils.save_image(grid, os.path.join(run_path, prefix+f'descendent_specific_topk_heatmap_with_heatmap_colorzoomin_heatmapzoomin_ep={epoch}_{subtree_root.name}', node.name, f'{child_classname}-p{p}.png'), border=0) # , border_color=(255, 255, 255), border=10\n",
    "\n",
    "# txt_file.write('\\n')\n",
    "# txt_file.close()\n",
    "print('Done !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example list of tuples\n",
    "data = overspecificity_score_and_proto_mask\n",
    "\n",
    "# Separate the data based on the second value\n",
    "x_0 = [x for x, y, z in data if z == 0]  # First values where the second value is 0\n",
    "x_1 = [x for x, y, z in data if z == 1]  # First values where the second value is 1\n",
    "\n",
    "# Create a dummy y-axis value since this is a one-dimensional scatter plot\n",
    "# y_0 = [y for x, y, z in data if z == 0]  # Dummy y values for blue points\n",
    "# y_1 = [y for x, y, z in data if z == 1]  # Dummy y values for red points\n",
    "\n",
    "# y_0 = [1]*len(x_0)\n",
    "# y_1 = [1.5]*len(x_1)\n",
    "\n",
    "y_0 = [1]*len(x_0)\n",
    "y_1 = [1]*len(x_1)\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(x_1, y_1, color='red', label='Value 1', s=4)   # Plot points with second value 1 in red\n",
    "plt.scatter(x_0, y_0, color='blue', label='Value 0', s=4)  # Plot points with second value 0 in blue\n",
    "\n",
    "\n",
    "# Additional plot formatting\n",
    "plt.xlabel('Overspecificity score')\n",
    "# plt.ylabel('Num descendants')  # Hide y-axis ticks since it's a one-dimensional plot\n",
    "plt.yticks([])\n",
    "# plt.legend(loc='best')\n",
    "# plt.title('One-dimensional Scatter Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Package(s) not found: cairosvg\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip show cairosvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.nn.functional' from '/home/harishbabu/.conda/envs/hpnet4/lib/python3.9/site-packages/torch/nn/functional.py'>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
